\chapter{Describing data} \label{projections}
%Some commonly boldfaced vectors.
\def\xv{{\bf x} } \def\yv{{\bf y} } \def\pv{{\bf p} }
\def\Iv{{\bf I}}\def\Sv{{\bf S}}\def\yv{{\bf y}}\def\Zv{{\bf Z}} \def\cv{{\bf c}} \def\uv{{\bf u}} \def\Yv{{\bf Y}} \def\Xv{{\bf X}} \def\Qv{{\bf Q}}
\def\betav{{\mbox{\boldmath$\beta$}}}
\def\vector#1{\left[\matrix{#1}\right]}
%@-     so lgrind will play nice with makeindex

This chapter covers some methods of describing a data set, via a
number of strategies of increasing complexity. 
The first approach, in Section \ref{basicstats}, consists of simply looking
at summary statistics for a series of observations about a single
variable, like its mean and variance. It imposes no structure on the
data of any sort. The next level of structure is to
assume that the data is drawn from a distribution; instead of finding
the mean or variance, we would instead estimate the parameters that
describe the distribution, using the data. These are primarily
one-dimensional methods.

The remainder of the chapter goes very multi-dimensional, asking how
to describe data when there are more dimensions than we humans could
even visualize. The first approach, taken in in Section \ref{pca} and
known as factor analysis,
is projection: find a two- or three-dimensional subspace that best
describes the fifty-dimensional data. 

The second approach, in Section \ref{cat}, provides still more
structure. The model labels one variable as the dependent variable, and claims
that it is a linear combination of the other, independent, variables.
This is the ordinary least squares model, which has dozens of variants.

One way to characterize these two approaches is that both aim to project
$N$-di\-men\-sion\-al data onto the best subspace of significantly less than
$N$ di\-men\-sions---but they have different definitions of `best'.
The standard OLS regression consists of
finding the one-dimensional line which minimizes the sum of squared
distances between the data and that line. Factor analysis consists
of finding the few dimensions where the data's variance is maximized, after being
projected onto the subspace.



\section{Characteristics of data and distributions}\label{basicstats}

\paragraph{Expected value:} 
First, let us say that a value of $x$ has probability $P(x)$. Then
if $f(x)$ is an arbitrary function, 
$$E(f(x))=\int_{\forall x} f(x)p(x) dx.$$

If we have a sample of data points, then we take each single observation
to be equally likely: $p(x_i)={1\over n} \forall i$.
The expected value for a sample then becomes a familar calculation
$E(X)=(\sum x)/n$, and is \comment{(I should check this) }the BLUE of
the true mean $\mu$. Section \ref{CLT} will discuss the magical properties of the
sample expected value that make statistical analysis possible.

\paragraph{Covariance:} The population \ind{covariance}, given a data set,
is $\sigma_{xy} = E[(x-\overline x)(y-\overline y)]$, which is equivalent
to $E[xy]-E[x]E[y]$. The \ind{variance} is a special case where $\xv=\yv$.

The sample covariance is $s_{xy}=\sigma_{xy}\cdot {n\over n-1}$. This
is
the expectation if you  divided the sum of $(x-\overline
x)(y-\overline y)$ by $n-1$ instead of $n$. This
can be proven to be unbiased.

\paragraph{More moments:}\index{skew} \index{kurtosis} \index{moments} 
If so inclined, one could also write out the
expectation in the variance equation as an integral:
$$\var(f(x))=\int_{\forall x} (f(x) - \overline{f(x}))^2 p(x) dx.$$
But we can calculate a similar integral for higher powers as well:
$${\rm skew}(f(x))=\int_{\forall x} (f(x) - \overline{f(x}))^3 p(x) dx\\
{\rm kurtosis}(f(x))=\int_{\forall x} (f(x) - \overline{f(x}))^4 p(x) dx.$$
These three integrals are the \vocab{central moments} of $f(x)$. They
are central because we subtracted the mean from the function before
taking the second, third, or fourth power.\footnote{The
central first moment is always zero; the non-central second, third,
\dots, moments are difficult to interpret and basically ignored. Thus,
some authors refer to all of these as \airq{the $n$th moment} and leave
it as understood whether the moment is central or non-central.}

The skew indicates that a distribution is upward leaning, a negative
skew indicates a downward lean. Kurtosis is typically put in plain
English as \airq{fat tails}: how much density is in the tails of the
distribution? For example, the kurtosis of a ${\cal N}(0,1)$ is three,
while the kurtosis of a \ind{student's $t$ distribution} with $n$ degrees of
freedom is greater than three, and decreases as $n$ grows, converging to
three.

The same caveat about unbiased estimates of the sample and population
variance holds for skew and kurtosis: $\sum (x-\overline x)^m/(n-1)$
is an unbiased estimate of the sample central moment for $m \in\{2, 3,
4, \dots\}$, and $\sum (x-\overline x)^m/n$ is an unbiased estimate of
the population central moment. The functions
\cind{gsl\_\-vec\-tor\_\-var},
\cind{apop\_\-vec\-tor\_\-var},
\cind{apop\_\-vec\-tor\_\-skew}, and
\cind{apop\_\-vec\-tor\_\-kurt\-o\-sis} all return the sample central
moment, presuming that your data is much more likely to be a sample than
a population. Multiply the output by $(n-1)/n$ to get the population
central moment.

\paragraph{Correlation/Cauchy-Schwarz:} $\rho_{xy}={s_{xy}\over s_x
s_y}$. This is just another fact for your memorization, but it makes a
surprise appearance in section \ref{ts}. This also allowed Cauchy \&
Schwarz to state the \ind{Cauchy-Schwarz inequality}: $0\leq \rho^2 \leq 1$.
\label{correlation}

\paragraph{Coding it} Given a vector, Apophenia provides functions to
calculate most of the above, e.g.:

\begin{lstlisting}
apop_data *set = gather_data();
gsl_vector v1, v2;
v1 = gsl_matrix_col(set->data, 0).vector;
v2 = gsl_matrix_col(set->data, 1).vector;
double mean1 = apop_vector_mean(&v1);
double var1 = apop_vector_var(&v1);
double covar = apop_vector_covar(&v1, &v2);
double corr = apop_vector_correlation(&v1, &v2);
apop_data_show(apop_matrix_summarize(set));
\end{lstlisting}
The last item in the code, \ttind{apop\_data\_show} produces a table of
some summary statistics for every column of the data set.

\section{Sample distributions}\index{sample distributions}
\label{distlist}

These are some distributions that our sample may take on. They
and are worth memorizing
before any major statistics test. Common distributions of statistical
parameters are discussed in section \ref{dist2}.

\paragraph{Bernoulli\index{Bernoulli distribution}}

The number of times that an event which has likelihood $p$ happens in
one trial. I.e., $x$ is zero or one here.

\long\def\eqnbox#1{
\begin{center}
\fbox{
\parbox{10cm}{
\begin{eqnarray}
#1
\end{eqnarray}
}
}
\end{center}
}

\long\def\wideeqnbox#1#2{
\begin{center}
\fbox{
\parbox{#1}{
\begin{eqnarray}
#2
\end{eqnarray}
}
}
\end{center}
}

\eqnbox{
P(x|p)&=&p^x (1-p)^{(1-x)}				\nonumber\\
E(x)&=&p						\nonumber\\
\var(x)&=&p(1-p)					\nonumber
}

\paragraph{Binomial\index{Binomial distribution}}

The number of times that an event which has likelihood $p$ happens over the
course of $n$ trials.\footnote{The notation $\left({n\atop x}\right)$ indicates
\airq{$n$ choose $x$}, the number of sets of $x$ elements that can be
pulled from $n$ objects. The equation is $$\left({n\atop x}\right) =
{n!\over x!(n-x)!},$$ and the function is \cind{gsl\_\-sf\_\-choose(n,x)}
(in the GSL's Special Functions section).\index{choose}}

\eqnbox{
P(x|p)&=&\left(\matrix{n\cr x}\right) p^x (1-p)^{(n-x)}		\nonumber\\
E(x)&=&np							\nonumber\\
\var(x)&=&np(1-p)						\nonumber
}

\paragraph{Poisson\index{Poisson distribution}}

This is for things that happen over and over again. Given
$\lambda\geq0$, 
the number of people who have been unemployed for
up to $\lambda$ weeks,
or the number of people on a ship who were flogged up to
$\lambda$ times. 

\eqnbox{
P(x|\lambda)&=&{e^{-\lambda}\lambda^x\over x!}			\nonumber\\
E(x)&=&	\lambda							\nonumber\\
\var(x)&=&\lambda						\nonumber
}

\paragraph{Normal\index{Normal distribution}}

You know and love the bell curve, aka the Gaussian distribution.\footnote{Typography note: this book will use \airq{Normal}, in an attempt to avoid arbitrary names, and with a capital to indicate that this is a proper name and not an adjective.} See the CLT (section \ref{CLT}) for
its application.

\eqnbox{ \label{normal}
P(x|\mu,\sigma)&=&{1\over \sigma\sqrt{2\pi}} e^{-(1/2)[(x-\mu)/\sigma]^2}\\ 
E(x)&=&\mu							\nonumber\\
\var(x)&=&\sigma^2						\nonumber
}

\paragraph{Multivariate Normal}\index{multivariate normal distribution}
All of the above distributions were for a single variable---one
dimension. Let us say that we have a data set $\Xv$, which includes a
thousand observations and seven variables (so $\Xv$ is a 1000$\times$7
matrix). We know its mean is
$\mu$ (a vector of length seven) and that the covariance among the
variables is $\Sigma$ (a seven by seven matrix). Then the Multivariate
Normal distribution that one would fit to this data is:

\eqnbox{
p(\Xv|\mu, \Sigma) &=& {\exp(-{1\over 2} (\Xv-\mu)' \Sigma^{-1} (\Xv-\mu)) 
\over
   \sqrt{(2 \pi)^n \det(\Sigma)}}            \nonumber\\
   E(\Xv) &=& \mu                           \nonumber\\
   \var(\Xv) &=& \Sigma            \nonumber
}					

Apophenia provides the function \ttind{apop\_multivariate\_normal\_prob} to evaluate this likelihood for a given data point.

\paragraph{Beta distribution} The \ind{Beta distribution} is a flexible way to
describe data inside the range $[0, 1]$.  

\eqnbox{
p(x) = {\Gamma(a+b) \over \Gamma(a) \Gamma(b)} x^{a-1}
(1-x)^{b-1}\nonumber
}




\section{Factor analysis} \label{pca} 
This is also known as principal component
analysis, singular value decomposition, or spectral decomposition, depending upon your field. 

It is a purely descriptive method.  The idea is that we want a few
dimensions that will capture the most variance possible---usually two,
because we can plot two dimensions. That is, we will project the data
onto the best plane, where `best' means that it captures as much
variance in the data as possible.

After plotting the data, perhaps with markers for certain observations,
we may find intuitive descriptions for the dimensions that we had just plotted the
data on. My favorite example of this is the work of Poole \& Rosenthal,
who did a principal component analysis\footnote{They actually did
the analysis using an intriguing maximum likelihood method, rather
than the eigenvector method here. Nonetheless, the end result and its
interpretation is the same.} on all of the U.S. Congresses. They found
that 90\% of the variance in vote patterns could be explained by two dimensions.
Studying the data points, they determined that one of these dimensions could be
described as `fiscal issues' and the other as `social issues'. This method stands
out because Poole \& Rosenthal did not have to look at bills and place them on
either scale---the data placed itself, and they just had to name the scales.


It can be shown that the best $n$ axes, in the sense above, are the
$n$ eigenvectors of the data's covariance matrix with the $n$ largest
associated eigenvalues.

\subsection{Coding it}
This section will show you the computation of a singular value
decomposition on three levels. The first goes through the steps of
calculating eigenvectors yourself; the second uses the 
GSL's built-in singular value decomposition function; the third is a
single call in Apophenia.

The only hard part is finding the eigenvalues of
$(X'X)$; the GSL saw us coming, and gives us the \cinline{gsl\_eigen\_symm} functions
to calculate the eigenvectors of a symmetric matrix.

The GSL is too polite to allocate large vectors behind our backs, so
it asks that we pass in pre-allocated workspaces when it needs such
things. The eigenvalue-finding function requires such a workspace, so
here is a function that allocates the workspace, calls the eigenfunction,
then frees the workspace:
\begin{lstlisting}
void find_eigens(gsl_matrix *subject, gsl_vector *eigenvals, gsl_matrix *eigenvecs){
   gsl_eigen_symmv_workspace * w = gsl_eigen_symmv_alloc(subject->size1);
   gsl_eigen_symmv(subject, eigenvals, eigenvecs, w);
   gsl_eigen_symmv_free (w);
   gsl_matrix_free(subject);
}
\end{lstlisting}

Notice that I free the matrix whose eigenvalues are being calculated at the end.
This is because the matrix is destroyed in the
calculations, and shouldn't be referred to again. 

Here's how this function is used. In the tradition of C, the code is
mostly declarations, and in the last two lines, it will calculate the
covariance matrix $X'X$ for the data set, and then find its eigenvalues
and eigenvectors.

I will assume that you have already got a data matrix ready, named
\cinline{data}, as per the last chapter. Now we need to remove the means
from the data, and find $X'X$:\footnote{\ttind{apop\_matrix\_normalize}
is actually inappropriate here; this footnote is a reminder to self to
fix this.}

\begin{lstlisting}
int ds=data->size2;
gsl_matrix *xpx  = gsl_matrix_calloc(ds, ds);
gsl_vector *eigenvals   = gsl_vector_alloc(ds);
gsl_matrix *eigenvecs   = gsl_matrix_alloc(ds, ds);

apop_matrix_normalize(data);
gsl_blas_dgemm(CblasTrans,CblasNoTrans, x, x, xpx);
find_eigens(xpx, eigenvals, eigenvecs);
\end{lstlisting}

Now we have the eigenvectors and their associated eigenvalues; we need only find
the largest eigenvalues, and project the data onto their associated eigenvectors.
The GSL helps us by giving us functions for
finding the indices of the largest elements of a vector.
\begin{lstlisting}
const int dimensions_we_want = 2;
gsl_matrix *pc_space = gsl_matrix_alloc(ds,dimensions_we_want);
gsl_vector *temp_vector = gsl_vector_alloc(ds);
int indexes[dimensions_we_want];
int i;

gsl_sort_vector_largest_index(indexes, dimensions_we_want, eigenvals);

for (i=0;i<dimensions_we_want; i++){
   gsl_matrix_get_col(temp_vector, eigenvecs, indexes[i]);
   gsl_matrix_set_col(pc_space, i, temp_vector);
}
\end{lstlisting}

All that's left to do is the projection. Notice the convention I used:
the \cinline{pc\_space} has eigenvectors on its columns, and as many columns as the
dimensionality we want in the end. Below, I transpose that before premultiplying
the data set by the principal component matrix.

\begin{lstlisting}
gsl_matrix *projected 
                 = gsl_matrix_alloc(data->size1, dimensions_we_want);
gsl_blas_dgemm(CblasTrans,CblasNoTrans, pc_space, data, projected);
\end{lstlisting}

You will probably want to plot the \cinline{projected} matrix; the means
of doing so are discussed in Chapter \ref{gnuplot}

\paragraph{The easy way}

Figure \ref{svdecomposition} presents a function which takes some
shortcuts from the above, which we would use in practice. It is a
simplification of \ttind{apop\_\-sv\_de\-comp\-osi\-tion}. The function simply makes
a copy of the data and feeds that in to the GSL's built-in singular-value
decomposition function (which eats the copy, and must also be fed an
additional dummy vector for its work). The function then returns the first
\cinline{dimensions\_we\_want} eigenvectors and all of the eigenvalues. I
assume that all of the input arguments have had space allocated before
calling the function.  \codefig{svdecomposition}{A function to find the
singluar-value decomposition of a data set}




\section{Linear models}
\label{cat}

Assume that our variable of interest, $\Yv$, is
described by a linear combination of the explanatory variables, $\Xv$,
plus maybe a Normally-distributed error term, $\uv$. In short,
$\Yv=\Xv\betav + \uv$, where we will estimate the values of $\betav$. 

There is a catalog of methods to estimate the values of $\betav$,
depending on the assumptions behind how the $\Xv$s are related. If they
are basically unrelated, then we can use the model known as {\em ordinary
least squares} (fully specified below). When we believe that some parts
of the $\Xv$ matrix are correlated to other rows, such as rows $t$ and
$t+1$ of 
time series data, then we need to write down a matrix
$\Sigma$ to describe the covariances.

Thus, we can write down a catalog of models. The baseline is OLS, but by
providing different values of $\Sigma$ or other minor modifications, we
can write down other models. The computational process is very similar
for all.

[A coding note: if $\Xv$ is ten by a million, then $\Sigma$ is a million
by a million, which is still orders of magnitude beyond our computing
power. There are two approaches one could take: one would be to use
regularities in the $\Sigma$ matrix to do pencil-and-paper math before
doing the computations, so that something nearer to a closed-form
solution can be calculated without writing down $\Sigma$. The other
option would be to use a \index{sparse matrices}sparse matrix, in which
we write down only those elements of the matrix that are not zero. 

Apophenia intends to use the sparse matrix method where possible, but this is not yet
implemented. \ttind{apop\_GLS.estimate}() currently takes a full matrix
for $\Sigma$, making it wholly unusable for data sets larger than maybe a
thousand observations. The sparse-matrix implementation of GLS is
left as an exercise to the reader.]

Let $\betav$ be the true parameters of the process that produced the
data. There is no way to truly know $\beta$, but we can estimate it
using techniques derived elsewhere and described below; let 
$\hat\betav$ be the estimate of $\betav$.  There are many ways to
describve when $\hat\betav$ does a better or worse job at estimating 
$\betav$; the key feature we will be demanding is that the estimator be
unbiased, meaning that in the limit, $E(\hat\betav) = \betav$.

The catalog describes the mean and variance of the $\hat\betav$s that
each model derives, because the Normal distribution---the basis for all
hypothesis tests---requires those two inputs,
and without them we can do no post-estimation testing.  
In section \ref{tstat} we get rid of $\sigma$, but we
need to know $\mu$, and unbiasedness guarantees us that if all is going
well, $\mu =\hat\betav$.

\subsection{OLS}
\index{Ordinary Least Squares|(}\index{OLS|see{Ordinary Least Squares}}
The model: 
\begin{itemize}
\item $\Yv=\Xv\betav + \uv$
\item $N$=the number of observations. $\Yv$ and $\uv$ are $N \times 1$
matrices.
\item $K$=the number of parameters to be estimated. $\Xv$ is $N \times K$;
$\betav$ is $K$ by 1
\item It is customary that the first column of $\betav$ is a column of
ones. If you don't do that, then you need to replace every column $\Xv_i$ below
with $\Xv_i-\overline X_i$, the equivalence of which is left as an exercise
for the reader.\footnote{If you would like to take the route of
normalizing each column to have mean zero, see
\cind{apop\_\-ma\-trix\_\-norm\-al\-ize}.}
\end{itemize}

Assumptions:

\begin{itemize}
\item $\uv$ is normally distributed.
\item $E(\uv) = 0$
\item $\var(u_i) = \sigma^2$, a constant, $\forall\ i$
\item $\cov(u_i, u_j) = 0$, $\forall\ i\neq j$. That is, $\Sigma =
\sigma^2{\bf I}$.
\item $\var(\Xv_k)>0$, and as $n\to\infty$, it remains finite
\item The columns of $\Xv$ aren't colinear
\item $N>K$
\end{itemize}

When all of that holds, then:

\eqnbox{
\hat\betav_{\rm OLS}  &=& (\Xv'\Xv)^{-1}(\Xv'\Yv)			\nonumber\\
E(\hat\betav_{\rm OLS} ) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm OLS} ) &=& \sigma^2(\Xv'\Xv)^{-1}		\nonumber
}
	\index{Ordinary Least Squares|)}
\subsection{GLS} \label{GLS}\index{GLS|see{Generalized Least Squares}}
Generalized Least Squares generalizes OLS by allowing $\uv'\uv$ to be a
known matrix $\Sigma$, with no additional restrictions. For OLS,
$\Sigma=\sigma^2{\bf I}$. Note how neatly plugging $\sigma^2{\bf I}$ in to the
estimator and variance below reduces them to the OLS variance.

\eqnbox{
\hat\betav_{\rm GLS} &=& (\Xv'\Sigma^{-1}\Xv)^{-1}(\Xv'\Sigma^{-1}\Yv)			\nonumber\\
E(\hat\betav_{\rm GLS}) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm GLS}) &=& (\Xv'\Sigma^{-1}\Xv)^{-1}		\nonumber
%var(\hat\betav_{GLS}) &=& 
% 			(\Xv'\Xv)^{-1}\Xv'\Sigma\Xv(\Xv'\Xv)^{-1}\nonumber
}

\subsection{\ind{IV}}			\label{IV}

If $\Xv$ is measured with error, then $\hat\betav_{OLS}$ is inconsistent and
asymptotically biased toward zero (in most cases, see e.g. \cite{kmenta},
p 349). Bias is a big deal, so we need an alternate method. If there exists
another instrumental variable $\Zv$ such that $\cov([Z_i-\overline
Z][u_i-\overline u])=0$, then we can do the following:

\eqnbox{
\hat\betav_{\rm IV} &=& (\Zv'\Xv)^{-1}(\Zv'\Yv)			\nonumber\\
E(\hat\betav_{\rm IV}) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm IV}) &=& 
	\sigma^2(\Zv'\Xv)^{-1}\Zv'\Zv(\Xv'\Zv)^{-1}	\nonumber
}

Note how the variance shrinks as $\Zv'\Xv$ increases, and if
$\Zv'\Xv=0$, you can not use this method. 

\subsection{Constrained LS \index{least squares!constrained}}

Add the constraint that $\Qv'\betav=\cv$. Then:

%\wideeqnbox{6.2in}{
\wideeqnbox{\textwidth}{
\hat\betav_{CLS} &=& \hat\beta_{OLS}-
(\Xv'\Xv)^{-1}\Qv(\Qv'(\Xv'\Xv)^{-1}\Qv)^{-1}
		(\Qv'\hat\beta-\cv)			\nonumber\\
%E(\hat\betav) &=& \beta					\nonumber\\
\var(\hat\betav_{CLS}) &=& \sigma^2[(\Xv'\Xv)^{-1} - 		
(\Xv'\Xv)^{-1}\Qv(\Qv'(\Xv'\Xv)^{-1}\Qv)^{-1}\Qv'(\Xv'\Xv)^{-1}]
\nonumber
}
(See, e.g., \cite{amemiya:ez}.)
If the constraint is satisfied by $\hat\beta$, then
$(\Qv'\hat\beta-\cv)$ is zero and $\hat\betav_{CLS}=\hat\betav_{OLS}$.
Though you have to ask yourself why you're imposing a constraint that
you haven't tested; perhaps you should first have a look at the F-test
based on this form, in Section \ref{ftestsec}.

\subsection{\ind{FGLS}}

If we do not know $\Sigma$, which is universally the case except on
test questions, we need some way to make the GLS estimation Feasible. So we
assume that $\Sigma$ is a function of one parameter, $\Sigma(\rho)$.

\label{ts}\index{time series|(}
\paragraph{\ind{AR(1)} time series model}

This is where errors are autoregressed by one period\footnote{AR(4) is
also well studied, because of seasonal data.}. I.e.,

$$y_t=\Xv_t\beta + \rho\epsilon_{t-1} + u_t$$

where $u_t$ is truly distributed ${\cal N}(0,1)$, and $\epsilon_t=
\rho\epsilon_{t-1} + u_t$. This can come about from  a form like
$y_t=\Xv_t\beta + y_{t-1}\gamma + \epsilon_t.$


Since $\epsilon_{t-1}$ includes $\rho\epsilon_{t-2}$, $\rho^n u_{t-n}$
appears in the regression for period $t$. The covariance matrix should
be clear in your head now: it's $\sigma^2$ times one on the diagonals,
$\rho$ on the off-diagonal, $\rho^2$ on the off-off-diagonals, \&c. In
this form, the $\rho$ really is the correlation coefficient---$\rho$
from section \ref{correlation}.

So all we need to apply GLS is $\hat\rho$. It so happens that OLS is
consistent for the AR(1) case, so we can find $\hat\epsilon$, and then
just find $r=(\sum \hat\epsilon_t \hat\epsilon_{t-1})/(\sum
\hat\epsilon_t^2)$. From there, proceed as with GLS.


\index{time series|)}

\subsection{Fitting it} 
To summarize,
the coefficients of  the line of best fit are the value of $\beta$ which solves:
$ \beta = (X'X)^{-1}X'Y$.

I will assume that we have already read a data matrix in to \cinline{apop\_data *set}. First, here is the easy way:
\begin{lstlisting}
apop_estimate *est = apop_OLS.estimate(set, NULL);
apop_estimate_print(est);
\end{lstlisting}
But if the assumptions of OLS do not fit, then you will need to know how
the \cinline{a\-pop\_\-OLS.\-est\-i\-mate} function works, so the remainder of this
section goes over the steps one would take to estimate $\beta$.  \ttindex{apop\_OLS}

 Typically, the $Y$
values are the first column of data, so we would like to extract that to a separate variable:
\begin{lstlisting}
gsl_vector      *y_data         = gsl_vector_alloc(set->data->size1);
gsl_matrix_get_col(y_data, set->data, 0);
\end{lstlisting}

We now have the $Y$-values safely stored.  When doing an OLS projection,
there needs to be a column of ones in the data set (because OLS is an
{\sl affine} linear projection), so we can overwrite the first column
with ones. 
\begin{lstlisting}
gsl_vector_view v         = gsl_matrix_column(set->data, 0);
gsl_vector_set_all(&(v.vector), 1);  
\end{lstlisting}


Now we have both the $Y$ and the $X$ in the equation, so we can find $(X'X)$ and $X'Y$.
Both parts are a
simple application of the BLAS function from the last chapter---a matrix $\cdot$ a matrix, and matrix $\cdot$ a vector:
\begin{lstlisting}
gsl_vector      *xpy            = gsl_vector_calloc(set->data->size2);
gsl_matrix      *xpx            = gsl_matrix_calloc(set->data->size2, set->data->size2);
gsl_blas_dgemm(CblasTrans,CblasNoTrans, 1, set->data, set->data, 0, xpx);
gsl_blas_dgemv(CblasTrans, 1, set->data, y_data, 0, xpy);
\end{lstlisting}

The GSL has a function to solve equations of the type ${\bf A} \beta =
{\bf C}$; which is exactly the form we have here---$(X'X)\beta = (X'Y)$.  \label{ols}
%\begin{verbatim}
\begin{lstlisting}
        gsl_linalg_HH_solve (xpx, xpy, *beta);
\end{lstlisting}
%\end{verbatim}

The vector \cinline{beta} now lives up to its name, holding the coefficients
$\beta$. Notice that we never had to take an inverse.  

An alternative approach actually takes the inverse of $(X'X)$. In practice, this is
almost necesary, because the variance is $\sigma^2 (\Xv'\Xv)^{-1}$.
Also, the matrix which projects data onto the space defined by $X$,
often called the `hat matrix', is $H = X(X'X)^{-1}X'$. Although it may
make the computer sweat for large matrices, the inverse function
from the last chapter is already paying off, allowing us to find
$(\Xv'\Xv)^{-1}$ with one function call:
\begin{lstlisting}
gsl_matrix *xpxinv;
apop_det_and_inv(xpx, &xpxinv, 0, 1);
\end{lstlisting}
Two more lines of algebra will calculate the hat matrix for us: 
\lstset{texcl=true}
\begin{lstlisting}
gsl_matrix * first_part = gsl_matrix_calloc(set->data->size1,set->data->size1);
gsl_matrix * hat_matrix = gsl_matrix_calloc(set->data->size1,set->data->size1);
//Find $X(X'X)^{-1}$:
gsl_blas_dgemm(CblasNoTrans,CblasNoTrans, set->data, xpx_inv, first_part);	
//Find $X(X'X)^{-1}X$:
gsl_blas_dgemm(CblasNoTrans,CblasTrans, first_part, set->data, hat_matrix);	
gsl_matrix_free(first_part);
\end{lstlisting}
\lstset{texcl=false} %for now.

If we have a second data set \cinline{set2} of the appropriate number of
rows (meaning that \cinline{(set->data-$>$size1 == set2->data-$>$size1)}, then we can use the hat matrix to do the projection:\\
\begin{lstlisting}
gsl_matrix * projected_data = gsl_matrix_calloc(set->data->size1,set2->data->size2);
gsl_blas_dgemm(CblasNoTrans,CblasNoTrans, hat_matrix, set2->data, projected_data);
\end{lstlisting}





