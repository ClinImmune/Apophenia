\chapter{Describing data} \label{projections}

This chapter covers some methods of describing a data set, via a
number of strategies of increasing complexity. 
The first approach, in Section \ref{basicstats}, consists of simply looking
at summary statistics for a series of observations about a single
variable, like its mean and variance. It imposes no structure on the
data of any sort. The next level of structure is to
assume that the data is drawn from a distribution; instead of finding
the mean or variance, we would instead estimate the parameters that
describe the distribution, using the data. These are primarily
one-dimensional methods.

The remainder of the chapter goes very multi-dimensional, asking how
to describe data when there are more dimensions than we humans could
even visualize. The first approach, taken in in Section \ref{pca} and
known as factor analysis,
is projection: find a two- or three-dimensional subspace that best
describes the fifty-dimensional data. 

The second approach, in Section \ref{cat}, provides still more
structure. The model labels one variable as the dependent variable, and claims
that it is a linear combination of the other, independent, variables.
This is the ordinary least squares (OLS) model, which has dozens of variants.

One way to characterize the two projection approaches is that both
aim to project $N$-di\-men\-sion\-al data onto the best subspace of
significantly less than $N$ di\-men\-sions---but they have different
definitions of \airq{best}.  The standard OLS \index{Ordinary Least
Squares} regression consists of finding the one-dimensional line
that minimizes the sum of squared distances between the data and that
line. Factor analysis consists of finding the few dimensions where the
data's variance is maximized, after being projected onto the subspace.


\section{Characteristics of data and distributions}\label{basicstats}
The first step in analyzing a data set is always to get a quick lay of
the land: where do the variables generally lie? How far do they wander?
As variable $A$ goes up, does variable $B$ generally go up as well?

\paragraph{Expected value:} \blindvocab{expected value}
First, let us say that a value of $x$ has probability $P(x)$. Then
if $f(x)$ is an arbitrary function, 
$$E\left(f(x)\right)=\int_{\forall x} f(x)p(x) dx.$$

If we have a vector of data points, $\xv$, consisting of $n$ elements
$x_i$, then we take each single observation to be equally likely:
$p(x_i)={1\over n}\ \forall i$.  The expected value for a sample then
becomes the familiar calculation $$E(\xv)={\sum_i x_i\over n},$$ and is
\comment{(I should check this) }the BLUE of the true mean $\mu$. Section
\ref{CLT} will discuss the magical properties of the sample expected
value that make statistical analysis possible.

\paragraph{More moments:}\blindvocab{skew} \blindvocab{kurtosis} \index{moments} 
The variance for discrete data is the familiar formula of the mean
squared distance to the average. Let $\overline x$ indicate the mean of
$x$, then the variance of the sample is:
\begin{equation}
\var(\xv) = {1\over n-1} \sum_i\left(x_i - \overline x\right).
\label{vareqn}
\end{equation}
The sum is divided by $n-1$ instead of $n$ because, given the mean
$\overline x$ and $n-1$ elements, the $n^{\rm th}$ element is not a
random draw, but can be deterministically solved. That is, given the
mean, there are only $n-1$ \vocab{degrees of freedom}. Approximately
every statistics textbook ever written provides a proof that Equation
\ref{vareqn} is an unbiased estimator of the population's true variance.

Given a continuous probability distribution from which the data was
taken, one write out the
expectation in the variance equation as an integral,
$$\var(f(x))=\int_{\forall x} \left(f(x) - \overline{f(x})\right)^2 p(x) dx.$$
and similarly for higher powers as well:
\begin{eqnarray*}
{\rm skew}\left(f(x)\right)&=&\int_{\forall x} \left(f(x) - \overline{f(x})\right)^3 p(x) dx\\
{\rm kurtosis}\left(f(x)\right)&=&\int_{\forall x} \left(f(x) - \overline{f(x})\right)^4 p(x) dx.
\end{eqnarray*} \blindvocab{noncentral moment}
These three integrals are the \vocab{central moments} of $f(x)$. They
are central because we subtracted the mean from the function before
taking the second, third, or fourth power.\footnote{The
central first moment is always zero; the non-central second, third,
\dots, moments are difficult to interpret and basically ignored. Thus,
some authors refer to all of these as \airq{the $n$th moment}, $n \in
\{1, 2, 3, 4\}$, and leave
it as understood when the moment is central or non-central.}

Positive skew indicates that a distribution is upward leaning, and a negative
skew indicates a downward lean. Kurtosis is typically put in plain
English as \airq{fat tails}: how much density is in the tails of the
distribution? For example, the kurtosis of a ${\cal N}(0,1)$ is three,
while the kurtosis of a \ind{Student's $t$ distribution} with $n$ degrees of
freedom is greater than three, and decreases as $n$ grows, converging to
three (see page \pageref{tkurt} for a full analysis).

\cindex{gsl\_stats\_variance} \cindex{apop\_vector\_var}
\cindex{apop\_vector\_skew} \cindex{apop\_vector\_kurtosis}
\label{kurtskew}
The same caveat about unbiased estimates of the sample versus population
variance holds for skew and kurtosis: $\sum (x-\overline x)^m/(n-1)$
is an unbiased estimate of the sample central moment for $m \in\{2, 3,
4, \dots\}$, and $\sum (x-\overline x)^m/n$ is an unbiased estimate of
the population central moment. All of the moment-finding functions in
the GSL and Apophenia (e.g.,
\cinline{gsl\_stats\_var\-i\-ance},
\cinline{apop\_vec\-tor\_var}, and
\cinline{apop\_vec\-tor\_skew}) all return the sample central
moment, presuming that your data is much more likely to be a sample than
a population. Multiply the output by $(n-1)/n$ to get the population
central moment.\footnote{The underlying GSL functions 
\cind{gsl\_stats\_skew} and \cind{gsl\_stats\_kurtosis} are a bit
eccentric.   
First, they are 
normalized by dividing by the square root of the sample variance, which
is a digression from the standard equations in this text. Second, although
\cind{gsl\_stats\_variance} is a sum divided by $n-1$, the higher moments
are a sum divided by $n$. That is, after they are normalized by a standard
deviation assuming a sample, the calculation proceeds as if the data are a
population. Of course, this detail is only relevant for for small $n$.}


What information can we get from the higher moments?  Below, we will
see the powerful Central Limit Theorem, which says that if a variable
represents the mean of a set of independent and identical draws, then it
will have a ${\cal N}(\mu,\sigma)$ distribution, where $\mu$ and $\sigma$
are unknowns that can be estimated from the data. These two parameters
completely define the distribution; the skew of a Normal is always zero,
and the kurtosis is always $3\sigma^4$. If the kurtosis is larger,
then this typically means that the assumption of independent draws is
false---the observations are interconnected.\label{kurt1} One often sees
this among social networks or other systems where independent entities
observe and imitate each other.

\paragraph{Covariance:} The population \vocab{covariance}, given a data set,
is $\sigma^2_{\xv\yv} = {1\over n}\sum_i(x_i-\overline \xv)(y_i-\overline \yv)$, which is equivalent
to $E[\xv\yv]-E[\xv]E[\yv]$. The \ind{variance} is a special case where $\xv=\yv$.

As with the variance, the unbiased estimate of the 
sample covariance is $s^2_{xy}=\sigma^2_{xy}\cdot {n\over n-1}$---the
expectation if you  divided the sum of $(x-\overline x)(y-\overline y)$ by
$n-1$ instead of $n$.

\paragraph{Correlation/Cauchy-Schwarz:} The \vocab{correlation coefficient} is
defined as $$\rho_{\xv\yv}\equiv{\sigma_{\xv\yv}\over \sigma_\xv
\sigma_\xv},$$ and is usually estimated using ${s_{\xv\yv}\over
s_\xv s_\xv}$.  This is mostly just another statistic useful for
describing two columns of data, but it makes a surprise appearance
in Section \ref{ts}. It also allowed Cauchy \& Schwarz to state
the \vocab{Cauchy-Schwarz inequality}: $0\leq \rho^2 \leq 1$.
\label{correlation}

\paragraph{Coding it} Given a vector, Apophenia provides functions to
calculate most of the above, e.g.:

\cindex{apop\_vector\_mean} \cindex{apop\_vector\_var}
\cindex{apop\_data\_show} \cindex{apop\_vector\_skew}
\cindex{apop\_vector\_correlation}
\cindex{apop\_matrix\_summarize}
\begin{lstlisting}
apop_data *set = gather_data();
gsl_vector v1, v2;
v1 = gsl_matrix_col(set->matrix, 0).vector;
v2 = gsl_matrix_col(set->matrix, 1).vector;
double mean1 = apop_vector_mean(&v1);
double var1 = apop_vector_var(&v1);
double skew1 = apop_vector_skew(&v1);
double kurt1 = apop_vector_kurtosis(&v1);
double covar = apop_vector_covar(&v1, &v2);
double corr = apop_vector_correlation(&v1, &v2);
apop_data_show(apop_matrix_summarize(set));
\end{lstlisting}
The last item in the code, \cind{apop\_matrix\_summarize} produces a table of
some summary statistics for every column of the data set.

\paragraph{Percentiles} \index{percentiles} The mean and variance can be misleading for
skewed data. The first option for describing a data set whose
distribution is likely ab-Normal is to plot a histogram of the data, as
per page \pageref{histosec}.

A numeric option is to print the \ind{quartiles} or \ind{quintiles} of the
data. For quartiles, one would sort the data, and then display the 
values of the data points 0\%, 25\%, 50\%, 75\%, and 100\% of the
way through the set. Notice that the 0\% value is the minimum of the data
set, the 100\% value is the maximum, and the 50\% value is probably the
median \index{median|(}(see below). For quintiles, one would print the
values of data points  0\%, 20\%, 40\%, 60\%, 80\% and 100\% of the way
through the set.

Sorting \index{sorting!of \cinlinetwo{gsl\_vector}s} a \ci{gsl\_vector} is 
simple: if you have a vector \ci{my\_data}, then call
\begin{lstlisting}
gsl_vector_sort(my_data);
\end{lstlisting}
and the vector will be sorted in place, so
\ci{gsl\_vector\_get(my\_data, 0)} is the minimum of the data,
\ci{gsl\_vector\_get(my\_data, my\_data->size)} is the maximum, and 
\ci{gsl\_vector\_get(my\_data, my\_data->size/2)} is about the median.

The function \cind{apop\_vector\_percentiles} takes in a
\ci{gsl\_vector} and returns the percentiles---the value of the
data point 0\%, 1\%, \dots, 99\%, 100\% of the way through the data.
It takes in two arguments: the data vector, and a character describing
the rounding method---\ci{'u'} for rounding up, \ci{'d'} for rounding
down, and \ci{'a'} for averaging. Since the number of elements in a
data set are rarely divisible by a hundred and one, the position of most
percentile points likely falls between two data points. For example, if
the data set has 107 points, then the tenth data point is 9.47\% through
the data set, and the eleventh data point is 10.38\% through the set, so
which is the tenth percentile? If you specify \ci{'u'}, then it is the
eleventh data point; if you specify \ci{'d'} then it is the tenth data
point, and if you specify \ci{'a'}, then it is the simple mean of the two.

The standard definition of the median is that it is the middle
value of the data point if the data set has an odd number of elements,
and it is the average of the two data points closest to the middle if
the data set has an even number of elements. 
Thus, here is a function to find the median of a data set. It find the
percentiles using the averaging rule for interpolation, marks down the
$50^{\rm th}$ percentile, then cleans up and returns that value.
\begin{lstlisting}
double find_median(gsl_vector *v){
    double *pctiles = apop_vector_percentiles(v, 'a');
    double out = pctiles[50];
    free(pctiles);
    return out;
}
\end{lstlisting}

\exercise{On page \pageref{gdppercap} you tabulated GDP per capita for
the countries of the world.  What is the median country's GDP
per capita? How much is it affected by the rounding method?}

\exercise{
Write a function with
header \ci{double show\_quartiles(gsl\_vector *v, char rounding\_method,
int divisions)} that passes \ci{v} and \ci{rounding\_method} to
\ci{apop\_vector\_percentiles}, and then prints a table of selected
percentiles to the screen. If \ci{divisions==4}, print quartiles, if
\ci{divisions==5}, print quintiles, if \ci{divisions==10}, print
deciles, et cetera.

Use your function to print the deciles for country incomes from
\ci{data-wb}.
}
\index{median|)}

\paragraph{\treesymbol{} Weighted data} Your data may be aggregated so
that one line of data represents multiple observations. For example,
sampling efficiencly can be improved by sampling subpopulations
differently depending upon their expected variance \citep{sarndal:mass}.
For this and other reasons, data from statistical agencies often
includes weightings.

This is not the place to go into details about statistically sound means
of weighting data, but if your data includes weightings, then you will
want means to find the weighted moments of the data. Assuming you have
data in vectors \ci{v}, \ci{v1}, and \ci{v2} and corresponding weights in a vector
\ci{w}, then you can use the following functions. Notice that the
covaiance function takes only one weighting, because it does not make
sense to try to find the covariance of differently-weighted data, so the
\ci{w} vector is assumed to apply to both vectors.
\cindex{apop\_vector\_weighted\_mean} \cindex{apop\_vector\_weighted\_var}
\cindex{apop\_data\_weighted\_show} \cindex{apop\_vector\_weighted\_skew}
\cindex{apop\_vector\_weighted\_cov}
\begin{lstlisting}
double mean_w = apop_vector_weighted_mean(v, w);
double var_w = apop_vector_weighted_var(v, w);
double skew_w = apop_vector_weighted_skew(v, w);
double kurt_w = apop_vector_weighted_kurtosis(v, w);
double corr = apop_vector_cov(v1, v2, w);
\end{lstlisting}

\summary{
\item The most basic means of describing data is via its moments. The
basic moments should be produced and skimmed for any data set; in simple cases, there is
no need to go further.
\item The mean and variance are well known, but there is also
information in higher moments---the skew and kurtosis.
\item It is also important to know how variables interrelate, which can
be summarized using the variance-covariance matrix.
\item There is a one-line function to produce each of these pieces of information.
Notably, \ci{apop\_matrix\_summarize} produces a summary of each column
of a data matrix.
\item You can get a quick-and-rough numerical description of a data set's
distribution using quartiles or quintiles; to do so, use
\ci{apop\_vector\_percentiles}.
}

\section{Sample distributions}\index{sample distributions}
\label{distlist}


Here are some distributions that an observed variable may take on. They are
worth memorizing before any major statistics test. Common distributions
of statistical parameters are discussed in Section \ref{dist2}.

Each has a story attached, which is directly useful for
modeling. For example, if you think that a variable is the outcome of $n$
independent, binary events, then the variable should be modeled as a
Binomial distribution, and once you estimate the parameter to the
distribution, you will have a full model of that variable, that you can
even test if so inclined.

The catalog below includes the three most typical items one would want
from a distribution: a random number generator (RNG) that would produce
data with the given distribution, a probability density function (PDF),
and a cumulative density function (CDF). The full details of RNG use
will be discussed below, but the functions are cataloged here for easy
reference, and each requires a pointer to a \ci{gsl\_rng}, which will be
named \ci{r}.


\paragraph{Bernoulli\index{Bernoulli distribution|textbf}}\label{bernie}

The number of times that an event which has likelihood $p$ happens in
one trial. I.e., $x$ is zero or one here.

\indwcsub{Bernoulli distribution}{gsl\_ran\_bernoulli\_pdf}
\indwcsub{Bernoulli distribution}{gsl\_ran\_bernoulli}
%\indwcsub{Bernoulli distribution}{gsl\_ran\_bernoulli}
\cindex{gsl\_ran\_bernoulli} \cindex{gsl\_ran\_bernoulli\_pdf}
\eqnbox{
P(x|p)&=&p^x (1-p)^{(1-x)}				\nonumber\\
        &=&\ci{gsl\_ran\_bernoulli\_pdf(x, p)}\nonumber\\
E(x)&=&p						\nonumber\\
\var(x)&=&p(1-p)					\nonumber\\
RNG&:&\ci{gsl\_ran\_bernoulli(r, p)}					\nonumber
}

\paragraph{Binomial\index{Binomial distribution|textbf}}

The number of times that an event which has likelihood $p$ happens over the
course of $n$ trials.

The notation $\left({n\atop x}\right)$ indicates
\airq{$n$ choose $x$}, the number of sets of $x$ elements that can be
pulled from $n$ objects. The equation is $$\left(\begin{matrix}n\cr x\end{matrix}\right) =
{n!\over x!(n-x)!},$$ and the function is \cinline{gsl\_sf\_choose(n,x)} 
(in the GSL's Special Functions section).
\index{choose!gsl\_sf\_choose@\cinlinetwo{gsl\_sf\_choose}} \cindex{gsl\_sf\_choose}

\indwcsub{Binomial distribution}{gsl\_ran\_binomial}
\indwcsub{Binomial distribution}{gsl\_ran\_binomial\_pdf}
\cindex{gsl\_ran\_binomial} \cindex{gsl\_ran\_binomial\_pdf}
\eqnbox{
%P(x|p)&=&\left(\matrix{n\cr x}\right) p^x (1-p)^{(n-x)}		\nonumber\\
P(x|p,n)&=&\left(\begin{matrix}n\cr x\end{matrix}\right) p^x (1-p)^{(n-x)}		\nonumber\\
        &=&\ci{gsl\_ran\_binomial\_pdf(x, p, n)}\nonumber\\
E(x)&=&np							\label{binome}\\
\var(x)&=&np(1-p)						\label{binomv}\\
RNG&:&\ci{gsl\_ran\_binomial(r, p, n)}					\nonumber
}

\begin{itemize}
\item If $X \sim {\rm Bernoulli}(p)$, then for the sum of $n$ independent draws,
$\sum_{i=1}^n X_i \sim {\rm Binomial}(n,p)$.
\item As $n\to\infty$, Binomial$(n,p)$ $\to$ Poisson$(np)$ or ${\cal
N}(np, np(1-p))$.
\end{itemize}

Since $n$ is known and $E(x)$ and $\var(x)$ are values calculated from
the data, Equations \ref{binome} and \ref{binomv} are an oversolved
system of two variables for one unknown, $p$. Thus, you can test for
\vocab{excess variance}, which indicates that there are interactions that
falsify that the observations were \vocab{iid} Bernoulli events.

\paragraph{Normal\index{Normal distribution|textbf}}

You know and love the bell curve, aka the Gaussian
distribution.\footnote{Typography note: this book will use \airq{Normal},
in an attempt to avoid un-mnemonic proper names, and with a capital to indicate
that this is a proper name and not an adjective.} See the CLT (Section
\ref{CLT}) for its application.


\indwcsub{Normal distribution}{gsl\_ran\_gaussian}
\indwcsub{Normal distribution}{gsl\_ran\_gaussian\_pdf}
\indwcsub{Normal distribution}{gsl\_cdf\_gaussian\_(P,Q)}
\cindex{gsl\_ran\_gaussian} \cindex{gsl\_ran\_gaussian\_pdf}
\cindex{gsl\_cdf\_gaussian\_Q}\cindex{gsl\_cdf\_gaussian\_P}
\eqnbox{ \label{normal}
P(x|\mu,\sigma)&=&{1\over \sigma\sqrt{2\pi}} e^{-{1\over 2}[{(x-\mu)\over\sigma}]^2}\\ 
    &=& \ci{gsl\_ran\_gaussian\_pdf(x, sigma) + mu}\nonumber\\
E(x)&=&\mu							\nonumber\\
\var(x)&=&\sigma^2						\nonumber\\
\int_{-\infty}^x {\cal N}(\mu,\sigma)dy &=&
        \ci{gsl\_cdf\_gaussian\_P(x-mu, sigma)}\nonumber\\
\int^{\infty}_x {\cal N}(\mu,\sigma)dy &=&
        \ci{gsl\_cdf\_gaussian\_Q(x-mu, sigma)}\nonumber\\
RNG&:&\ci{gsl\_ran\_gaussian(r, sigma)+mu}			\nonumber
}

\begin{itemize}
\item If $X \sim {\cal N} (a,b)$ and $Y \sim {\cal N} (c,d)$ then $X+Y \sim {\cal N}
(a+c,b+d)$.
\end{itemize}

\paragraph{Multivariate Normal}\index{multivariate normal distribution}
All of the other distributions are for a single variable---one
dimension. Let us say that we have a data set $\Xv$, which includes a
thousand observations and seven variables (so $\Xv$ is a 1000$\times$7
matrix). We know its mean is
$\mu$ (a vector of length seven) and that the covariance among the
variables is $\Sigma$ (a seven by seven matrix). Then the Multivariate
Normal distribution that one would fit to this data is:

\eqnbox{
p(\Xv|\mu, \Sigma) &=& {\exp\left(-{1\over 2} (\Xv-\mu)' \Sigma^{-1} (\Xv-\mu)\right) 
\over
   \sqrt{(2 \pi)^n \det(\Sigma)}}            \nonumber\\
&=&\cind{apop\_multivariate\_normal\_prob(...)} \nonumber\\
   E(\Xv) &=& \mu                           \nonumber\\
   \var(\Xv) &=& \Sigma            \nonumber
}					

\begin{itemize}
\item When $\Xv$ has only one column and $\Sigma = \sigma^2\Iv$,
this reduces to the univariate Normal distribution.
\end{itemize}

\indwcsub{Poisson distribution}{gsl\_ran\_poisson}
\indwcsub{Poisson distribution}{gsl\_ran\_poisson\_pdf}
\cindex{gsl\_ran\_poisson}\cindex{gsl\_ran\_poisson\_pdf}
\paragraph{Poisson\index{Poisson distribution|textbf}}

Say that independent events occur at the mean rate of $\lambda$ events per 
period. What is the probaility that there will be $x$ events in a single 
period? Or, say that  objects are independently scattered
with mean density $\lambda$ per square km. What is the probability that
there are $x$ objects in a randomly-selected square km?

\eqnbox{
P(x|\lambda)&=&{e^{-\lambda}\lambda^x\over x!}			\nonumber\\
        &=&\ci{gsl\_ran\_poisson\_pdf(x, lambda)}\nonumber\\
E(x)&=&	\lambda							\nonumber\\
\var(x)&=&\lambda						\nonumber\\
RNG&:&\ci{gsl\_ran\_poisson(r, lambda)}					\nonumber
}
\begin{itemize}
\item If $X \sim {\rm Poisson}(\lambda_1)$ and $Y \sim {\rm
Poisson}(\lambda_2)$ (and $X$ and $Y$ are independent), then $(X + Y) \sim {\rm Poisson}(\lambda_1 +
\lambda_2)$.  \item As $\lambda\to\infty$,  ${\rm Poisson}(\lambda)
\to {\cal N}(\lambda, \lambda)$.  \end{itemize}


\indwcsub{Exponential distribution}{gsl\_ran\_exponential}
\indwcsub{Exponential distribution}{gsl\_ran\_exponential\_pdf}
\indwcsub{Exponential distribution}{gsl\_cdf\_exponential\_(P,Q)}
\cindex{gsl\_ran\_exponential} \cindex{gsl\_ran\_exponential\_pdf}
\cindex{gsl\_cdf\_exponential\_(P,Q)}
\paragraph{Exponential distribution\index{Exponential distribution|\textbf}}

The story of exponential decay is that in each period, $\lambda$ percent
of the current population drops out of the population.

To give an example from my own research, say that we believe that in
each period, $\lambda$ crew members of a ship are flogged. If the flogging
is purely random, then $\lambda$ members are flogged once, $\lambda^2$
members are flogged twice, et cetera.

This is sometimes called the \vocab{Negative exponential distribution}.

\eqnbox{
p(x|\lambda) &=& \frac{1}{\lambda}e^{\frac{-x}{\lambda}}		\nonumber\\
    &=& \ci{gsl\_ran\_exponential\_pdf(x, lambda)}\nonumber\\
E(x)&=&	\lambda							\nonumber\\
\var(x)&=&\lambda^2						\nonumber\\
\int_{-\infty}^x {\rm Exp}(\lambda) dy &=&
        \ci{gsl\_cdf\_exponential\_P(x, lambda)}\nonumber\\
\int^{\infty}_x {\rm Exp}(\lambda) dy &=&
        \ci{gsl\_cdf\_exponential\_Q(x, lambda)}\nonumber\\
RNG&:&\ci{gsl\_ran\_exponential(r, lambda)}			\nonumber
}






\indwcsub{Beta distribution}{gsl\_ran\_beta}
\indwcsub{Beta distribution}{gsl\_ran\_beta\_pdf}
\indwcsub{Beta distribution}{gsl\_cdf\_beta\_(P,Q)}
\cindex{gsl\_ran\_beta} \cindex{gsl\_ran\_beta\_pdf}
\cindex{gsl\_cdf\_beta\_(P,Q)}
\paragraph{Beta distribution} The \ind{Beta distribution} is a flexible
way to describe data inside the range $[0, 1]$.\footnote{The \ind{Gamma
function} is defined as $\Gamma(x) = \int_0^\infty  t^{x-1} \exp(-t)
dt$. One may think of it as a continuous version of the \ind{factorial}
function, because $\Gamma(x) = (x-1)!$. It is not to be confused with
the Gamma distribution, which is so named because it is defined using
the Gamma function.}
See page \pageref{beta} for more on its uses, including the use of the
\ci{apop\_random\_beta} RNG.

\eqnbox{
P(x|\alpha,\beta) &=& {\Gamma(\alpha + \beta) \over \Gamma(\alpha) \Gamma(\beta)} x^{\alpha - 1}
(1-x)^{\beta - 1}\nonumber\\
    &=& \ci{gsl\_ran\_beta\_pdf(x, alpha, beta)}\nonumber\\
E(x)&=&{\alpha\over\alpha+\beta}							\nonumber\\
\var(x)&=&{\alpha\beta\over(\alpha+\beta)^2(\alpha + \beta + 1)}             \nonumber\\
\int_{-\infty}^x \betav(\alpha, \beta) dy &=&
        \ci{gsl\_cdf\_beta\_P(x, alpha, beta)}\nonumber\\
\int^{\infty}_x \betav(\alpha, \beta) dy &=&
        \ci{gsl\_cdf\_beta\_Q(x, alpha, beta)}\nonumber\\
RNG_1&:&\ci{gsl\_ran\_exponential(r, alpha, beta)}			\nonumber\\
RNG_2&:&\ci{apop\_random\_beta(r, mu, sigma)} \nonumber
}

\begin{itemize}
\item If $\alpha = \beta = 1$, then this is the ${\rm Uniform}[0,1]$ distribution.\index{Uniform distribution}
\end{itemize}


\index{Flat distribution|see{Uniform distribution}}
\indwcsub{Uniform distribution}{gsl\_ran\_flat}
\indwcsub{Uniform distribution}{gsl\_ran\_flat\_pdf}
\indwcsub{Uniform distribution}{gsl\_cdf\_flat\_(P,Q)}
\cindex{gsl\_rng\_uniform}\cindex{gsl\_ran\_flat} \cindex{gsl\_ran\_flat\_pdf}
\cindex{gsl\_cdf\_flat\_(P,Q)}
\paragraph{Uniform distribution\index{Uniform distribution|textbf}} What discussion of distributions
would be complete without mention of the Uniform? It represents a neutral
prior belief that any value within $[\alpha, \beta]$ is equally possible.

\eqnbox{
P(x) &=& \frac{1}{\beta - \alpha}       \nonumber\\
    &=& \ci{gsl\_ran\_flat\_pdf(x, alpha, beta)}\nonumber\\
E(x) &=& \frac{\beta - \alpha}{2}       \nonumber\\
\var(x) &=& \frac{(\beta - \alpha)^2}{12}   \nonumber\\
\int_{-\infty}^x {\cal U}(\alpha, \beta) dy &=&
        \ci{gsl\_cdf\_flat\_P(x, alpha, beta)}\nonumber\\
\int^{\infty}_x {\cal U}(\alpha, \beta) dy &=&
        \ci{gsl\_cdf\_flat\_Q(x, alpha, beta)}\nonumber\\
RNG, {\rm general}&:&\ci{gsl\_ran\_flat(r, alpha, beta)}			\nonumber\\
RNG, \alpha=0, \beta=1&:&\ci{gsl\_rng\_uniform(r)}			\nonumber
}

More distribution models, and a few more coded-out examples, will be added to this catalog.

\summary{
\item Probability theorists through the ages have developed models that 
indicate that if a process follows certain guidelines, the data will
have a predictable form.
\item A single draw from a binary event with fixed probability has a
Bernoulli distribution.
\item The sum of a series of $n$ draws from a Bernoulli distribution will have a 
Binomial distribution.
\item If $\overline x$ is the mean of a set of independent, identically
distributed draws from {\em any} nondegenerate distribution, then
the distribution of $\overline x$ approaches a Normal distribution. This
is the Central Limit Theorem.
\item If events occur at the rate of $\lambda$ per period, the
rate of event occuence as a function of time will have a Poisson
distribution.
\item Given the same setup, the population present in each period has an
Exponential distribution.
\item The Beta distribution is useful for modeling a variety of
variables that are restricted to $[0, 1]$.
}

\section{Using the sample distributions}

This section forthcoming.

\subsection{Looking up figures} If I have fifty draws from a Bernoulli
event with probabilty .25, what is the likelihood that I will have more
than twenty successes?

\codefig{normaltable}{How to produce a density table. Each row $x$ is a distance  to
the mean (i.e., assume $\mu=0$), each column is a value of $\sigma$,
and the intersection shows the CDF of a Normal with the given $\sigma$ up to $x$.}
\comment{
\begin{figure}
\begin{verbatim}
                                                sigma
                 0.1     0.2     0.3     0.4     0.5     0.6     0.7     0.8     0.9       1
value           --------------------------------------------------------------------------------
 0.1    |       0.8413  0.6915  0.6306  0.5987  0.5793  0.5662  0.5568  0.5497  0.5442  0.5398
 0.2    |       0.9772  0.8413  0.7475  0.6915  0.6554  0.6306  0.6125  0.5987  0.5879  0.5793
 0.3    |       0.9987  0.9332  0.8413  0.7734  0.7257  0.6915  0.6659  0.6462  0.6306  0.6179
 0.4    |          1    0.9772  0.9088  0.8413  0.7881  0.7475  0.7161  0.6915  0.6716  0.6554
 0.5    |          1    0.9938  0.9522  0.8944  0.8413  0.7977  0.7625  0.734   0.7107  0.6915
 0.6    |          1    0.9987  0.9772  0.9332  0.8849  0.8413  0.8043  0.7734  0.7475  0.7257
 0.7    |          1    0.9998  0.9902  0.9599  0.9192  0.8783  0.8413  0.8092  0.7816  0.758
 0.8    |          1       1    0.9962  0.9772  0.9452  0.9088  0.8735  0.8413  0.813   0.7881
 0.9    |          1       1    0.9987  0.9878  0.9641  0.9332  0.9007  0.8697  0.8413  0.8159
   1    |          1       1    0.9996  0.9938  0.9772  0.9522  0.9234  0.8944  0.8667  0.8413

\end{verbatim}
\caption{What stats textbooks used to look like.}
\end{figure}
}

In the past, statistics textbooks included an appendix presenting tables of common
distributions, but those tables are effectively obsolete, and more
modern textbooks refer the reader to the appropriate function in a stats
package. But for those who long for the days of grand tables, Figure
\ref{normaltable} presents the code for producing a neatly formatted
table of CDFs for a set of Normal distributions (the $p$-value 
often reported with hypothesis tests is one minus the listed value). 
Almost all of the work in this code is formatting and printing neat
lines on the screen; the math consists of the single line that
calculates \ci{gsl\_cdf\_gaussian\_P} toward the bottom. Notice that all
constants are at the top of the file, so that you can more easily create
a taller, wider, or otherwise different table without modifying the code
itself. 

\subsection{Generating data from a distribution}
Since each distribution neatly summarize an oft-modeled story, they can
be used to simulate or summarize real-world processes. Modelers will want
to make random draws from these distributions. This topic is currently
covered in some detail in Chapter \ref{boot}.

\codefig{normalgrowth}{A model of Normally distributed growth.}

Figure \ref{normalgrowth} gives a quick initial example. It is based on
work by Axtell and others [CITATIONS HERE] regarding \vocab{Zipf's law},
that cities, firms, and other such agglomerations tend to take on an
Exponential distribution. In the model here, this comes about because
agents' growth rates are Normally distributed.


First, the program produces a set of agents with one characteristic: size,
stored in a \ci{gsl\_vector}. The \ci{initialize} function draws agent
sizes from a Uniform$[0,100]$ distribution.  To do this, it required a
\ci{gsl\_rng}, which \ci{main} allocates using \cind{apop\_rng\_alloc}
and passes to \ci{initialize}.

Each period, the firms grow by a Normally distributed rate. That is, the
\ci{grow} function randomly draws $g$ from a \ci{gsl\_rng\_gaussian}, and
the agent's size is assigned to $size \leftarrow size * \exp(g)$ (though, the
\ci{gsl\_vector} syntax forces the actual code to be much more verbose).
The most likely growth rate is therefore $\exp(0) = 1$; 
when $g<0$, $\exp(g)<1$; and when $g>0$, $\exp(g)>1$. Also,
$\exp(g)*\exp(-g)=1$, and by the symmetry of the Normal distribution,
$g$ and $-g$ have equal likelihood, so it is easy for an agent to have
good luck in period one reversed by equally bad luck in period two,
leaving it near where it had started.

Each period, the program dumps a histogram to a Gnuplottable output
file. With a \gi{pause} between each histogram, the output file becomes
an animation, showing a quick transition from a Uniform distribution to
a steep Exponential distribution, where most agents are fast approaching
zero size, but a handful have size approaching 1,000.\footnote{Here,
the $x$-axis is the firm size, and the $y$-axis is the number of
firms. Typically, the Zipf distribution is displayed somewhat differently:
the $x$-axis is the \cind{rank} of the firm, $1^{\rm st}$,  $2^{\rm nd}$,
$3^{\rm rd}$, et cetera, and the $y$-axis is the size of the so-ranked
firm. Converting to this form is left as an exercise to the reader. [Hint:
use \ci{apop\_vector\_percentiles}.]}




\subsection{Fitting existing data}
The common goal throughout the book is to estimate the parameters of the
model with data, and you can see above that almost every parameter can
be solved---sometimes oversolved---using the mean and variance. Most of
the above distributions have an \ci{apop\_model} associated (see page
\pageref{modellist} for a list), and the \ci{estimate} function therein
will use the mean or the mean and variance to estimate the parameters.

This is known as the \vocab{method of moments} estimation, and it
matches the maximum likelihood estimate of the parameters in almost all
cases. For the Uniform, the method of moments doesn't work: the
expression $(\beta - \alpha)$ is oversolved but there is no way to solve
for $\alpha$ or $\beta$ alone. However, a few moments' thought will show that the
most likely value for $(\alpha, \beta)$ is simply $(\min(x), \max(x))$.


\section{Factor analysis} \label{pca} \index{factor analysis}
This is also known as \ind{principal component
analysis}, or \ind{spectral decomposition}, depending upon your field. 
The first phase (calculating the eigenvalues) is known as the \vocab{singular value decomposition}.

It is a purely descriptive method.  The idea is that we want a few
dimensions that will capture the most variance possible---usually two,
because we can plot two dimensions. That is, we will project the data
onto the best plane, where \airq{best} means that it captures as much
variance in the data as possible.

After plotting the data, perhaps with markers for certain observations,
we may find intuitive descriptions for the dimensions on which we had just plotted the
data. My favorite example of this is \citet{poole:rosenthal},
who did a principal component analysis\footnote{They actually did
the analysis using an intriguing maximum likelihood method, rather
than the eigenvector method here. Nonetheless, the end result and its
interpretation is the same.} on all of the U.S. Congresses. They found
that 90\% of the variance in vote patterns could be explained by two dimensions.
Studying the data points, they determined that one of these dimensions could be
described as `fiscal issues' and the other as `social issues'. This method stands
out because Poole \& Rosenthal did not have to look at bills and place them on
either scale---the data placed itself, and they just had to name the scales.


It can be shown that the best $n$ axes, in the sense above, are the
$n$ eigenvectors of the data's covariance matrix with the $n$ largest
associated eigenvalues.

\subsection{Coding it}
This section will show you the computation of a principal component
analysis on three levels. The first goes through the steps of
calculating eigenvectors yourself; the second uses the 
GSL's built-in singular value decomposition function; the third is a
single call in Apophenia.

The only hard part is finding the eigenvalues of
$\Xv'\Xv$; the GSL saw us coming, and gives us the \cinline{gsl\_eigen\_symm} functions
to calculate the \ind{eigenvectors} of a symmetric matrix.

The GSL is too polite to allocate large vectors behind our backs, so
it asks that we pass in pre-allocated workspaces when it needs such
things. The eigenvalue-finding function requires such a workspace, so
here is a function that allocates the workspace, calls the eigenfunction,
then frees the workspace:
\begin{lstlisting}
void find_eigens(gsl_matrix **subject, gsl_vector *eigenvals, gsl_matrix *eigenvecs){
   gsl_eigen_symmv_workspace * w = gsl_eigen_symmv_alloc(*subject->size1);
   gsl_eigen_symmv(*subject, eigenvals, eigenvecs, w);
   gsl_eigen_symmv_free (w);
   gsl_matrix_free(*subject);
   *subject  = NULL;
}
\end{lstlisting}

Notice that I free the matrix whose eigenvalues are being calculated at
the end.  This is because the matrix is destroyed in the calculations,
and should not be referred to again. Now, when a pointer is freed, its value
can take on any value. It could be set to \ci{NULL}, but if the compiler
is feeling lazy, it may just leave the pointer pointing to garbage.
Explicitly setting it to \ci{NULL} makes it clear that this pointer has
been freed.

Here is how this function is used. In the tradition of C, the code is
mostly declarations, and in the last two lines, it will calculate the
matrix $\Xv'\Xv$ for the data set, and then find its eigenvalues
and eigenvectors.  
I will assume that you have already got a data matrix ready, named
\cinline{data}, as per the last chapter. Now we need to remove the means
from the data (using \cind{apop\_matrix\_normalize}), and find $\Xv'\Xv$:

\cindex{apop\_dot}
\begin{lstlisting}
int ds=data->size2;
gsl_matrix *xpx;
gsl_vector *eigenvals   = gsl_vector_alloc(ds);
gsl_matrix *eigenvecs   = gsl_matrix_alloc(ds, ds);

apop_matrix_normalize(data, 'm');
gsl_blas_dgemm(CblasTrans,CblasNoTrans, x, x, xpx);
xpx = apop_dot(x, x, 1, 0);
find_eigens(&xpx, eigenvals, eigenvecs);
\end{lstlisting}

Now we have the eigenvectors and their associated eigenvalues; we need only find
the largest eigenvalues, and project the data onto their associated eigenvectors.
The GSL helps us by giving us functions for
finding the indices of the largest elements of a vector.
\begin{lstlisting}
int dimensions_we_want = 2;
gsl_matrix *pc_space = gsl_matrix_alloc(ds,dimensions_we_want);
gsl_vector temp_vector;
int indexes[dimensions_we_want];
int i;

gsl_sort_vector_largest_index(indexes, dimensions_we_want, eigenvals);

for (i=0;i<dimensions_we_want; i++){
   temp_vector  = gsl_matrix_column(eigenvecs, indexes[i]).vector;
   gsl_matrix_set_col(pc_space, i, &temp_vector);
}
\end{lstlisting}

All that's left to do is the projection. Notice the convention I used:
the \cinline{pc\_space} has eigenvectors on its columns, and as many columns as the
dimensionality we want in the end. Below, I transpose that before premultiplying
the data set by the principal component matrix.

\begin{lstlisting}
gsl_matrix *projected = gsl_matrix_alloc(data->size1, dimensions_we_want);
gsl_blas_dgemm(CblasTrans,CblasNoTrans, pc_space, data, projected);
\end{lstlisting}

You will probably want to plot the \cinline{projected} matrix; the means
of doing so are discussed in Chapter \ref{gnuplot}

\paragraph{The easy way}

Figure \ref{svdecomposition} presents a function that takes some
shortcuts from the above. It is a
simplification of \cind{apop\_sv\_de\-comp\-osi\-tion}, which is what you would use in practice. The function simply makes
a copy of the data and feeds that in to the GSL's built-in singular-value
decomposition function, \cind{gsl\_linalg\_SV\_decomposition}, which eats the copy, and must also be fed an
additional dummy vector for its work. The function then returns the first
\cinline{dimensions\_we\_want} eigenvectors and all of the eigenvalues. I
assume that all of the input arguments have had space allocated before
calling the function.  

\codefig{svdecomposition}{A function to find the singular-value decomposition of a data set.}


\summary{
\item Factor analysis projects data of several dimensions onto the
dimensions that display the most variance.
\item Given the data matrix $\Xv$, the process involves finding the
eigenvalues of the matrix $\Xv'\Xv$ associated with the largest
eigenvalues, and then projecting the data onto the space defined by
those eigenvectors.
\item \ci{apop\_sv\_decomposition} runs the entire process for the
efficiently lazy user.
}


\section{Linear models}
\label{cat}

Assume that our variable of interest, $\yv$, is
described by a linear combination of the explanatory variables, $\Xv$,
plus maybe a Normally-distributed error term, $\uv$. In short,
$\yv=\Xv\betav + \uv$, where we will estimate the values of $\betav$. 

Unlike the models to this point, this model implicitly makes a
causal claim: the variables listed in $\Xv$ cause $\yv$ to take the values they
do. However, 
there is no true concept of \ind{causality} in statistics.
The question of when statistical evidence of causality is valid is a tricky
one which will be left to the volumes that cover this question in
detail; for the purposes here, the reader should merely note the shift in
descriptive goal, from fitting distributions to telling a causal story.

There is a catalog of methods to estimate the values of $\betav$,
depending on the assumptions behind how the $\Xv$s are related. If they
are basically unrelated, then we can use the model known as \vocab{ordinary
least squares} (fully specified below). When we believe that some parts
of the $\Xv$ matrix are correlated to other rows, such as rows $t$ and
$t+1$ of 
time series data, then we need to write down a matrix
$\Sigma$ to describe the covariances.

\comment{
Thus, we can write down a catalog of models. The baseline is OLS, but by
providing different forms for $\Sigma$ or other minor modifications, we
can write down other models. The computational process is similar
for all.

[A coding note: if $\Xv$ is ten by a million, then $\Sigma$ is a million
by a million, which is still orders of magnitude beyond our computing
power. There are two approaches one could take: one would be to use
regularities in the $\Sigma$ matrix to do pencil-and-paper math before
doing the computations, so that something nearer to a closed-form
solution can be calculated without writing down $\Sigma$. The other
option would be to use a \index{sparse matrices}sparse matrix, in which
we write down only those elements of the matrix that are not zero. 

Apophenia intends to use the sparse matrix method where possible, but this is not yet
implemented. \cind{apop\_GLS.estimate}() currently takes a full matrix
for $\Sigma$, making it wholly unusable for data sets larger than maybe a
thousand observations. The sparse-matrix implementation of GLS is
left as an exercise to the reader. In the mean time, the reader has the
setup-specific models.]
}

Let $\betav$ be the true parameters of the process that produced the
data. There is no way to truly know $\betav$, but we can estimate it
using techniques derived elsewhere and described below. Let 
$\hat\betav$ be the estimate of $\betav$.  There are many ways to
describe when $\hat\betav$ does a better or worse job at estimating 
$\betav$; the key feature we will be demanding is that the estimator be
unbiased, meaning that in the limit, $E(\hat\betav) = \betav$.

The catalog describes the mean and variance of the $\hat\betav$s that
each model derives, because the Normal distribution---the basis for
almost all the hypothesis tests associated with linear models---requires
those two inputs, and without them we can do no post-estimation testing.
In Section \ref{tstat} we get rid of $\sigma$, but we need to know
$\mu$, and unbiasedness means that if all is going well, $\mu
=\hat\betav$.

\subsection{OLS}\label{olsdef}
\index{Ordinary Least Squares|(textbf}\index{OLS|see{Ordinary Least Squares}}
The model: 
\begin{itemize}
\item $\yv=\Xv\betav + \uv$
\item $N$=the number of observations. $\yv$ and $\uv$ are $N \times 1$
matrices.
\item $K$=the number of parameters to be estimated. $\Xv$ is $N \times K$;
$\betav$ is $K$ by 1
\item It is customary that the first column of $\betav$ is a column of
ones. If you do not do that, then you need to replace every column $\xv_i$ below
with $\xv_i-\overline x_i$, the equivalence of which is left as an exercise
for the reader.\footnote{If you would like to take the route of
normalizing each column to have mean zero, try
\cind{apop\_matrix\_norm\-al\-ize}\ci{(dataset, 'm')}.}
\end{itemize}

Assumptions:

\begin{itemize}
\item $\uv$ is normally distributed.
\item $E(\uv) = 0$.
\item $\var(u_i) = \sigma^2$, a constant, $\forall\ i$.
\item $\cov(u_i, u_j) = 0$, $\forall\ i\neq j$. Along with the above
assumption, this
means that the variance-covariance matrix for the observations' errors is
$\Sigma =
\sigma^2{\bf I}$.
\item $\var(\Xv_k)>0$, and as $n\to\infty$, it remains finite.
\item The columns of $\Xv$ are not collinear (so $\det(\Xv'\Xv)\neq 0$).
\item $N>K$.
\end{itemize}

When all of that holds, then:

\eqnbox{
\hat\betav_{\rm OLS}  &=& (\Xv'\Xv)^{-1}(\Xv'\yv)			\nonumber\\
E(\hat\betav_{\rm OLS} ) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm OLS} ) &=& \sigma^2(\Xv'\Xv)^{-1}		\nonumber
}

	\index{Ordinary Least Squares|)textbf}
\subsection{GLS} \label{GLS}\index{GLS|see{generalized least squares}}
\blindvocab{generalized least squares} 
Generalized Least Squares generalizes OLS by allowing $\uv'\uv$ to be a
known matrix $\Sigma$, with no additional restrictions.
Note how neatly plugging $\sigma^2{\bf I}$ in to the
estimator of $\betav$ and its variance below reduces the equations here to the OLS
versions.

\eqnbox{
\hat\betav_{\rm GLS} &=& (\Xv'\Sigma^{-1}\Xv)^{-1}(\Xv'\Sigma^{-1}\yv)			\nonumber\\
E(\hat\betav_{\rm GLS}) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm GLS}) &=& (\Xv'\Sigma^{-1}\Xv)^{-1}		\nonumber
%var(\hat\betav_{GLS}) &=& 
% 			(\Xv'\Xv)^{-1}\Xv'\Sigma\Xv(\Xv'\Xv)^{-1}\nonumber
}

\subsection{IV} \blindvocab{instrumental variable} 
\index{IV|see{instrumental variables}}	\label{IV} 
If a column of $\Xv$ is measured with error, then $\hat\betav_{OLS}$ is
inconsistent and asymptotically biased toward zero (in most cases, see
e.g. \cite{kmenta}, p 349). Bias is a big deal, so we need an alternate
method. Let $\xv_i$ be the column that is measured with error, let
$\zv$ be a column of alternate data (the instrument), and let $\Zv$ be
the original data set $\Xv$ with the column $\xv_i$ replaced by $\zv$.

If $\cov(\zv,\uv)=0$, then the following holds:

\eqnbox{
\hat\betav_{\rm IV} &=& (\Zv'\Xv)^{-1}(\Zv'\yv)			\nonumber\\
E(\hat\betav_{\rm IV}) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm IV}) &=& 
	\sigma^2(\Zv'\Xv)^{-1}\Zv'\Zv(\Xv'\Zv)^{-1}	\nonumber
}

Note how the variance shrinks as $\Zv'\Xv$ increases, and if
$\Zv'\Xv \to {\bf 0}$ (which happens when $\cov(\zv, \xv_i)\to 0$),
you can not use this method, since $\det(\Zv'\Xv) \to \infty$.

\subsection{Constrained LS \index{least squares!constrained}}
\label{constrainedls}

Add the constraint that $\Qv'\betav=\cv$, for some fixed $\Qv$ and $\cv$. 
Page \pageref{ftestsec} provides a number of examples of constraints in
this form. Then:

%\wideeqnbox{6.2in}{
\wideeqnbox{\textwidth-0.5cm}{
\hat\betav_{CLS} &=& \hat\betav_{OLS}-
(\Xv'\Xv)^{-1}\Qv(\Qv'(\Xv'\Xv)^{-1}\Qv)^{-1}
		(\Qv'\hat\beta-\cv)			\nonumber\\
%E(\hat\betav) &=& \beta					\nonumber\\
\var(\hat\betav_{CLS}) &=& \sigma^2[(\Xv'\Xv)^{-1} - 		
(\Xv'\Xv)^{-1}\Qv(\Qv'(\Xv'\Xv)^{-1}\Qv)^{-1}\Qv'(\Xv'\Xv)^{-1}]
\nonumber
}
(See, e.g., \cite{amemiya:ez}.)
If the constraint is satisfied by $\hat\beta$, then
$(\Qv'\hat\beta-\cv)$ is zero and $\hat\betav_{CLS}=\hat\betav_{OLS}$.
Though, you have to ask yourself why you are imposing a constraint that
you have not tested; perhaps you should first have a look at the \ind{F test}
based on this form, in Section \ref{ftestsec}.

\subsection{\ind{FGLS}} \index{Generalized Least Squares!Feasible}

If we do not know $\Sigma$, which is universally the case except on
test questions, we need some way to make the GLS estimation Feasible. 

\label{ts}\index{time series|(}
\paragraph{\ind{AR(1)} time series model}
For example, assume that $\Sigma$ is a function of one parameter, $\Sigma(\rho)$.  
In an AR(1) model, errors are AutoRegressed by one period.\footnote{AR(4) is
also well studied, because of seasonal data.} I.e.,
$$y_t=\Xv_t\betav + \rho\epsilon_{t-1} + u_t$$
where $u_t$ is truly distributed ${\cal N}(0,1)$, and $\epsilon_t=
\rho\epsilon_{t-1} + u_t$. This can come about from  a form like
$y_t=\Xv_t\beta + y_{t-1}\gamma + \epsilon_t.$


Since $\epsilon_{t-1}$ includes $\rho\epsilon_{t-2}$, 
$\epsilon_{t-2}$ includes $\rho\epsilon_{t-3}$, et cetera,
$\rho^n u_{t-n}$
appears in the regression for period $t$. The covariance matrix should
be clear in your head now: it's $\sigma^2$ times one on the diagonals,
$\rho$ on the off-diagonal, $\rho^2$ on the off-off-diagonals, \&c. In
this form, the $\rho$ really is the correlation coefficient---$\rho$
from Section \ref{correlation}.

So all we need to apply GLS is $\hat\rho$. It so happens that OLS is
consistent for the AR(1) case, so we can find $\hat\epsilon$, and then
just find $r=(\sum \hat\epsilon_t \hat\epsilon_{t-1})/(\sum
\hat\epsilon_t^2)$. From there, proceed as with GLS.
\comment{[Two-stage least squares here]}

\index{time series|)}

\paragraph{\ind{Weighted Least Squares}}\vocabmarker{weighted least squares}
Let $\Sigma$ be a diagonal matrix. That is, errors among different
observations are uncorrelated, but the error for each observation itself
is different. This is \vocab{heteroskedastic} data. The classic example
in Econometrics is due to differences in income: we expect that our
errors in measurement regarding somebody earning \$10,000/year will be
about a thousandth as large as our measurement errors regarding somebody
earning \$10 million/year.

It can be shown (e.g., \citet{kmenta}) that the optimum for this
situation, where $\sigma_i$ is known for each observation $i$, is to use
the GLS equations above, with $\Sigma$ set to zero everywhere but the
diagonal, where the $i$th element is $1\over {\sigma_i^2}$.

The GLS equations about $\betav$ now apply directly to produce Weighted 
Least Squares estimates. For a data set of a million elements,
$\Sigma$ is $10^6 \times 10^6=10^{12}$ (a trillion) elements, all but a
million of which will be zero. Thus, it would be nice to do computation
using just a vector of diagonal elements, and there is a simple trick to
do so. Let $\sqrt\Sigma$ be a vector where each element is the square
root of $\Sigma$'s corresponding diagonal element. For WLS, the $i$th
element of $\sqrt\Sigma$ is thus $1\over {\sigma_i}$. Now let
$\yv_\Sigma$ be a vector whose $i$th element 
is the $i$th element of $\yv$ times the $i$th element of
$\sqrt\Sigma$, and 
$\Xv_\Sigma$ be the column-wise product of $\Xv$ and
$\sqrt\Sigma$. That is:
\begin{lstlisting}
void columnwise_product(gsl_matrix *data, gsl_vector *sqrt_sigma){
  int i;
    for (i=0; i< data->size2; i++){
        v   = gsl_matrix_column(data, i).vector;
        gsl_vector_mul(&v, sqrt_sigma);
    }
}
\end{lstlisting}
Then the reader can verify that 
$\Xv_\Sigma'\Xv_\Sigma = \Xv' \Sigma \Xv$, 
$\Xv_\Sigma'\yv_\Sigma = \Xv' \Sigma \yv$, and so:

\eqnbox{
\hat\betav_{\rm WLS} &=& \left(\Xv_\Sigma'\Xv_\Sigma\right)^{-1}(\Xv_\Sigma'\yv_\Sigma)			\nonumber\\
E(\hat\betav_{\rm WLS}) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm WLS}) &=& 
	\sigma^2\left(\Xv_\Sigma'\Xv_\Sigma\right)^{-1}\nonumber
}

Thus, we can solve for the Weighted Least Squares elements without ever
writing down the full $\Sigma$ matrix in all its excess. This is the
method used by \cind{apop\_WLS} (which is what you would use in
practice, rather than calculating $\Xv_\Sigma$ yourself).

\section{Fitting it} 
As above,
the coefficients of  the line of best fit for the OLS model is the value of $\betav_{\rm OLS}$ that solves:
$ \betav_{\rm OLS} = (\Xv'\Xv)^{-1}\Xv'\yv$.

I will assume that we have already read a data matrix in to \cinline{apop\_data *set}. First, here is the easy way:
\begin{lstlisting}
apop_estimate_show(apop_OLS.estimate(set, NULL));
\end{lstlisting}
But if the assumptions of OLS do not fit, then you will need to know how
the \cinline{a\-pop\_OLS.\-est\-i\-mate} function works, so the remainder of this
section goes over the steps one would take to estimate $\betav$.  \cindex{apop\_OLS}

Typically, the $\yv$
values are the first column of the data set, so we would like to extract that to a separate variable:
\begin{lstlisting}
gsl_vector      *y_data         = gsl_vector_alloc(set->matrix->size1);
gsl_matrix_get_col(y_data, set->matrix, 0);
\end{lstlisting}

We now have the $\yv$-values safely stored.  When doing an OLS projection,
there needs to be a column of ones in the data set (because OLS is an
\vocab{affine linear projection}), so we can overwrite the first column
with ones. 
\cindex{gsl\_vector\_set\_all} 
\begin{lstlisting}
gsl_vector v         = gsl_matrix_column(set->matrix, 0).vector;
gsl_vector_set_all(&v, 1);  
\end{lstlisting}


Now we have both the $\yv$ and the $\Xv$ in the equation, so we can find $(\Xv'\Xv)$ and $(\Xv'\yv)$.
Both parts are a
simple application of the linear algebra functions from earlier---a matrix $\cdot$ a matrix, and a matrix $\cdot$ a vector:
\begin{lstlisting}
apop_data *xpx = apop_dot(set,set,1,0);
apop_data *xpy = apop_dot(set,apop_vector_to_data(y_data),1);
\end{lstlisting}

The GSL has a function to solve equations of the type ${\bf A} \betav =
{\bf C}$ using \ind{Householder transformations}, and this happens to be is exactly the form we have here---$(\Xv'\Xv)\betav = (\Xv'\yv)$.  \label{ols}
%\begin{verbatim}
\begin{lstlisting}
        gsl_linalg_HH_solve (xpx->matrix, xpy->vector, *beta);
\end{lstlisting}
%\end{verbatim}

The vector \cinline{beta} now lives up to its name, holding the coefficients
$\betav$. Notice that we never had to take an inverse.  

An alternative approach actually takes the inverse of $(\Xv'\Xv)$. In practice, this is
almost necessary, because the variance is $\sigma^2 (\Xv'\Xv)^{-1}$.
The matrix that projects data onto the space defined by $\Xv$,
often called the `hat matrix', is also derived from the inverse: ${\bf H} = \Xv(\Xv'\Xv)^{-1}\Xv'$. You will recall that taking the inverse is one function call:
\begin{lstlisting}
gsl_matrix *xpxinv;
apop_det_and_inv(xpx->matrix, &xpxinv, 0, 1);
\end{lstlisting}
Two more lines of algebra will calculate the hat matrix for us: 
\lstset{texcl=true}
\begin{lstlisting}
//Find $\Xv(\Xv'\Xv)^{-1}$:
apop_data *first_part = apop_dot(set, apop_matrix_to_data(xpxinv),0,0);
//Find $\Xv(\Xv'\Xv)^{-1}\Xv'$:
apop_data *hat_matrix = apop_dot(first_part, set,0,1);
apop_data_free(first_part);
\end{lstlisting}
\lstset{texcl=false} %for now.

If we have a second data set \cinline{set2} of the appropriate number of
rows (meaning that \cinline{(set->matrix-$>$size1 == set2->matrix-$>$size1)}, then we can use the hat matrix to do the projection:\\
\begin{lstlisting}
apop_data * projected_data = apop_dot(hat_matrix, set2, 0,0);
\end{lstlisting}
If the hat matrix is of few enough dimensions, we can plot the
projection using \cind{apop\_data\_print}; see page \pageref{gnuprint}.

\summary{
\item The Ordinary Least Squares model assumes that the dependent variable is an affine linear function of the others. 
Given this and other assumptions, the likelihood-maximizing parameter
vector is $\betav_{\rm OLS} = (\Xv'\Xv)^{-1}(\Xv'\Yv)$.  
\item If $\Sigma \neq \Iv$, then  $\betav_{\rm GLS} = (\Xv'\Sigma\Xv)^{-1}(\Xv'\Sigma\Yv)$. Depending on the value of $\Sigma$, one can design a number of models.
\item Coding these processes is a simple question of stringing together
lines of linear algebra operations from Chapters \ref{linear_algebra}
and \ref{apop}.
}

\section{Multilevel modeling} A key assumption to the models to this
point is that each observation is independently and identically distributed
relative to the others. However, observations often fall into clusters,
such as families, classrooms, or geographical region. 
A regression that simply includes a family/classroom/region dummy variable
asserts that each observation is iid relative to the others, but its
outcome rises or falls by a fixed amount depending on its group
membership.

But this is often false: family members heavily influence each other.
A better alternative may be to do a model estimation for each group
separately. At the level of the subgroup, the iid assumption is more
likely to hold. Then, once each group has been estimated, the parameters
can be used to fit a model where the observational unit is the group.

Writing a model to execute such a multilevel model is not
difficult---just write a model whose \cinline{estimate} function calls
another model's \cinline{estimate} function. 
An example will be forthcoming.

