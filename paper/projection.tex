\chapter{Describing data} \label{projections}

This chapter covers some methods of describing a data set, via a
number of strategies of increasing complexity. 
The first approach, in Section \ref{basicstats}, consists of simply looking
at summary statistics for a series of observations about a single
variable, like its mean and variance. It imposes no structure on the
data of any sort. The next level of structure is to
assume that the data is drawn from a distribution; instead of finding
the mean or variance, we would instead estimate the parameters that
describe the distribution, using the data. These are primarily
one-dimensional methods.

The remainder of the chapter goes very multi-dimensional, asking how
to describe data when there are more dimensions than we humans could
even visualize. The first approach, taken in in Section \ref{pca} and
known as factor analysis,
is projection: find a two- or three-dimensional subspace that best
describes the fifty-dimensional data. 

The second approach, in Section \ref{cat}, provides still more
structure. The model labels one variable as the dependent variable, and claims
that it is a linear combination of the other, independent, variables.
This is the ordinary least squares (OLS) model, which has dozens of variants.

One way to characterize the two projection approaches is that both
aim to project $N$-di\-men\-sion\-al data onto the best subspace of
significantly less than $N$ di\-men\-sions---but they have different
definitions of \airq{best}.  The standard OLS \index{Ordinary Least
Squares} regression consists of finding the one-dimensional line
that minimizes the sum of squared distances between the data and that
line. Factor analysis consists of finding the few dimensions where the
data's variance is maximized, after being projected onto the subspace.

\section{Models and constraints}
Before embarking on the process of estimating models, it is worth taking
some time to discuss the modeling process and how our models of the
real world translate to estimable parameters.

%A \vocab{hypothesis} is a constraint on otherwise unconstrained parameters. 

Frequently, the statistical and real-world model happen to match very closely.
For example, if we believe that $Y$ is an affine linear sum of $X_1$,
$X_2$, $X_3$, then we are making a claim that 
\begin{equation*}
Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + \alpha_3 X_3.
\end{equation*}

In a parallel manner, a statistician could estimate the $\beta$s in the equation
\begin{equation*}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon
\end{equation*}
using a standard OLS regression as shown later in this chapter.

Given the parameters estimated using OLS, translating those parameters
to the model is rather trivial:
\begin{align*}
\alpha_0 &= \beta_0\\
\alpha_1 &= \beta_1\\
\alpha_2 &= \beta_2\\
\alpha_3 &= \beta_3.
\end{align*}
However, the translation between parameter estimates and model
parameters easily becomes more complex.  For example, let us say that
we are measuring bicycle usage, $B$, in different  cities.  It would be
a function of the number of motorists $M$, the city's population $P$,
the physical size of the city $S$,  and measures of the availability of
public transportation $T$. But we know that the availability of public
transportation is also a function of the city's population and physical
size. Thus, our model is
\begin{align*}
B &= \alpha_0 M + \alpha_1 P + \alpha_2 \sqrt{S} + \alpha_3 T\\
T &= \alpha_4 P + \alpha_5 \sqrt{S}.
\end{align*}

One option for estimating this model is to run two analogous regressions,
\begin{align*}
B &= \beta_0 M + \beta_1 P + \beta_2 \sqrt{S} + \beta_3 T + \epsilon\\
T &= \beta_4 P + \beta_5 \sqrt{S} + \epsilon.
\end{align*}
Now the correspondence between the two parameters is as trivial as
before ($\alpha_0=\beta_0$, $\alpha_1=\beta_1$, et cetera). This is the
simplest example of \vocab{exactly solved parameters}.
But we get a
distorted view of the effect of city size and population on bicycling
prevalence, because some of those variables' effect is subsumed in the fact that
those factors help to determine whether a city has public
transportation. If the population of a city rises ten percent, ridership
will not rise by $.1 \beta_1$, because $T$ will change as well.

Another option is to substitute $T$ as specified in the second equation into
the first. Then, the model is
\begin{align*}
B &= \alpha_0 M + (\alpha_1 +\alpha_3 \alpha_4) P + (\alpha_2 + \alpha_3 \alpha_5) \sqrt{S}.\\
\end{align*}
The OLS analogue of a linear equation still looks like it did before:
\begin{align*}
B &= \beta_0 M + \beta_1 P + \beta_2 \sqrt{S} + \epsilon.
\end{align*}
Now, the conversion is nontrivial:
\begin{align}
\alpha_0 &= \beta_0     \label{consteq1}\\
\alpha_1 +\alpha_3 \alpha_4 &= \beta_1        \label{consteq2}\\
\alpha_2 + \alpha_3 \alpha_5 &= \beta_2.       \label{consteq3}
\end{align}
Equation \ref{consteq1} still gives us an estimate of $\alpha_0$,
but all our information about  $\alpha_1$ through $\alpha_5$ is
embodied in Equations \ref{consteq2} and \ref{consteq3}. With four
unknowns and two equations, the parameters are \airq{oversolved}. 
\index{undersolved parameter}

We have a better view of the effect of the variable $P$ on $B$, but we
do not have enough information to disaggregate that effect into the
direct effect ($\alpha_1$) and the indirect effect ($\alpha_3 \alpha_4$)
caused by the effect population has on public transport availability
($\alpha_4$). 

Depending on your situation and goals, one setup or the other may be
more appropriate. However, we must acknowledge the limitations of an
undersolved system: $n$ parameter estimates can inform at most $n$ model
parameters.

Now, let us say that a prior study has shown that $\alpha_2= 2.204$.
Discarding the equation for $T$ for now, our model is now:
\begin{align*}
B &= \alpha_0 M + \alpha_1 P + \alpha_2 \sqrt{S} + \alpha_3 T\\
\alpha_2 &= 2.204.
\end{align*}
and after estimating the parameters using a simple OLS estimation, the 
translation from estimated $\beta$s to model $\alpha$s is:
\begin{align*}
\alpha_0 &= \beta_0\\
\alpha_1 &= \beta_1\\
\alpha_2 &= \beta_2\\
\alpha_2 &= 2.204\\
\alpha_3 &= \beta_3.
\end{align*}
Unless we have the incredible luck that $\beta_2=2.20400$, $\alpha_2$ is an
\vocab{oversolved parameter} that has no solution.

There are two things that one can do with an oversolved system. One is
to use the extra constraints to inform the parameter estimation routine;
in the example above, we would estimate the parameters under the
assumption that $\beta_2$ is fixed at 2.204.
You can do this using constrained least squares (Section
\ref{constrainedls}) or via maximum likelihood estimation as in Chapter
\ref{mle}. The other option is to compare the unconstrained version of
the model with the constrained version to see how much distortion occurs
when the constraint binds. This is \airq{testing the constraints}, aka,
a \airq{hypothesis test}.
\index{least squares!constrained}

Recall that the two primary activities of the statistician are estimating
models and testing models.  For an undersolved system, we can neither
estimate nor test the model. For an exactly solved system, we can always
estimate the parameters of the model, but can not verify or refute the
model: you give me a data set, and I will always be able to give you
some set of model parameters. For an oversolved system, we can both
estimate the parameters and test whether the model is correct, because
there will not always be a way to consistently transform the estimated
parameters into model parameters.

Because there is so little persuasive power in an estimated but not
tested model, we often make up
ways to convert an exactly solved system into an oversolved one.
For example, one could add to any equation the additional equation
$\alpha_i = 0$ for all $i$.
By specifying
$Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + \cdots$ and $\alpha_1=0$,
we have an oversolved system whose validity we can test. Indeed, this
is the system of equations behind the $t$-tests that are universally
reported with every regression.

This may seem like belaboring the language with such a simple system,
but it is easy to extend this framework to systems of equations, 
exotic constraints, and other variants of the mapping from parameter
estimates to model parameters and testing oversolved equations.

Notably, it could be that the model can't be expressed as an affine
linear sum at all. Perhaps your model is a function in the programming
sense, that takes in a vector of inputs \ci{x1, x2, \dots, xn} and a
vector of parameters \ci{p1, p2, \dots, pm}, runs
through a thousand lines of code, and produces a value \ci{y}. The
problem is still the same---estimate the parameters of the model using
data---but the linear methods are no longer useful. In some cases, there
is a linear system that does not do too much violence to reality; in
other cases, you will need to use the maximum likelihood methods of
Chapter \ref{mle}. However, one could overconstrain the parameters of
this type of model as easily as those above, such as claiming that
\cinline{p1 + p2 == 1}. Chapter \ref{mle} will also discuss likelihood
ratio tests to evaluate the hypotheses embodied in constraints.

By the way, a statement like `We reject
the claim $\alpha_1=0$' is only half right. We are actually rejecting
the claim that both
$Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + \cdots$ {\em and}
$\alpha_1=0$. 
The first and second equations are mathematically of equal
status, but the custom is to presume that only the second equation is a
subordinate equation which is under scrutiny. Thus, one would correctly
say `we reject this system of equations'. Having given this caveat, the
remainder of this chapter will use the traditional terminology that
takes the base model as given, reducing
the above pair of equations to the hypothesis $H_0: \alpha_1=0$.

\summary{
\item If the parameters of the model of the real world can not be solved using the 
parameters of the statistical model (e.g., two unknowns and one
equation), then the model is
\vocab{undersolved}. Undersolved variables can neither be estimated nor
tested.
\item If the parameters of the model of the real world can be solved uniquely using the 
parameters of the statistical model, then the model is
\vocab{exactly solved}. Exactly solved variables can be estimated but not
tested.
\item If the parameters of the model of the real world have multiple,
contradictory solutions using the parameters of the statistical model
(e.g., $\alpha_1 = \beta_1$ and $\alpha_1 = 2.2$), then the model is
\vocab{oversolved}. Oversolved systems can be both estimated and
tested.
}


\section{Characteristics of data and distributions}\label{basicstats}
The first step in analyzing a data set is always to get a quick lay of
the land: where do the variables generally lie? How far do they wander?
As variable $A$ goes up, does variable $B$ generally go up as well?

\paragraph{Expected value:} 
First, let us say that a value of $x$ has probability $P(x)$. Then
if $f(x)$ is an arbitrary function, 
$$E\left(f(x)\right)=\int_{\forall x} f(x)p(x) dx.$$

If we have a sample of data points, then we take each single observation
to be equally likely: $p(x_i)={1\over n}\ \forall i$.
The expected value for a sample then becomes a familiar calculation
$E(X)=(\sum x)/n$, and is \comment{(I should check this) }the BLUE of
the true mean $\mu$. Section \ref{CLT} will discuss the magical properties of the
sample expected value that make statistical analysis possible.

\paragraph{More moments:}\index{skew} \index{kurtosis} \index{moments} 
If so inclined, one could also write out the
expectation in the variance equation as an integral:
$$\var(f(x))=\int_{\forall x} \left(f(x) - \overline{f(x})\right)^2 p(x) dx.$$
But we can calculate a similar integral for higher powers as well:
\begin{eqnarray*}
{\rm skew}\left(f(x)\right)&=&\int_{\forall x} \left(f(x) - \overline{f(x})\right)^3 p(x) dx\\
{\rm kurtosis}\left(f(x)\right)&=&\int_{\forall x} \left(f(x) - \overline{f(x})\right)^4 p(x) dx.
\end{eqnarray*}
These three integrals are the \vocab{central moments} of $f(x)$. They
are central because we subtracted the mean from the function before
taking the second, third, or fourth power.\footnote{The
central first moment is always zero; the non-central second, third,
\dots, moments are difficult to interpret and basically ignored. Thus,
some authors refer to all of these as \airq{the $n$th moment}, $n \in
\{1, 2, 3, 4\}$, and leave
it as understood when the moment is central or non-central.}

Positive skew indicates that a distribution is upward leaning, a negative
skew indicates a downward lean. Kurtosis is typically put in plain
English as \airq{fat tails}: how much density is in the tails of the
distribution? For example, the kurtosis of a ${\cal N}(0,1)$ is three,
while the kurtosis of a \ind{Student's $t$ distribution} with $n$ degrees of
freedom is greater than three, and decreases as $n$ grows, converging to
three. 

\cindex{gsl\_stats\_variance} \cindex{apop\_vector\_var}
\cindex{apop\_vector\_skew} \cindex{apop\_vector\_kurtosis}
\label{kurtskew}
The same caveat about unbiased estimates of the sample versus population
variance holds for skew and kurtosis: $\sum (x-\overline x)^m/(n-1)$
is an unbiased estimate of the sample central moment for $m \in\{2, 3,
4, \dots\}$, and $\sum (x-\overline x)^m/n$ is an unbiased estimate of
the population central moment. All of the moment-finding functions in
the GSL and Apophenia (e.g.,
\cinline{gsl\_stats\_var\-i\-ance},
\cinline{apop\_vec\-tor\_var}, and
\cinline{apop\_vec\-tor\_skew}) all return the sample central
moment, presuming that your data is much more likely to be a sample than
a population. Multiply the output by $(n-1)/n$ to get the population
central moment.\footnote{The underlying GSL functions 
\cind{gsl\_stats\_skew} and \cind{gsl\_stats\_kurtosis} are a bit
eccentric.   
First, they are 
normalized by dividing by the square root of the sample variance, which
is a digression from the standard equations in this text. Second, although
\cind{gsl\_stats\_variance} is a sum divided by $n-1$, the higher moments
are a sum divided by $n$. That is, after they are normalized by a standard
deviation assuming a sample, the calculation proceeds as if the data are a
population. Of course, this detail is only relevant for for small $n$.}


What information can we get from the higher moments?  Below, we will
see the powerful Central Limit Theorem, which says that if a variable
represents the mean of a set of independent and identical draws, then it
will have a ${\cal N}(\mu,\sigma)$ distribution, where $\mu$ and $\sigma$
are unknowns that can be estimated from the data. But given the standard
deviation, the kurtosis must be $3\sigma^4$. If the kurtosis is larger,
then this typically means that the assumption of independent draws is
false---the observations are interconnected.\label{kurt1} One often sees
this among social networks or other systems where independent entities
observe and imitate each other.

\paragraph{Covariance:} The population \ind{covariance}, given a data set,
is $\sigma_{xy} = E[(x-\overline x)(y-\overline y)]$, which is equivalent
to $E[xy]-E[x]E[y]$. The \ind{variance} is a special case where $\xv=\yv$.

The sample covariance is $s_{xy}=\sigma_{xy}\cdot {n\over n-1}$. This
is
the expectation if you  divided the sum of $(x-\overline
x)(y-\overline y)$ by $n-1$ instead of $n$. This
can be proven to be unbiased.

\paragraph{Correlation/Cauchy-Schwarz:} The correlation coefficient is
defined as $$\rho_{xy}\equiv{s_{xy}\over s_x s_y}.$$ This is mostly just another
statistic useful for describing two columns of data,
but it makes a surprise appearance in Section \ref{ts}. This
also allowed Cauchy \& Schwarz to state the \ind{Cauchy-Schwarz
inequality}: $0\leq \rho^2 \leq 1$.  \label{correlation}

\paragraph{Coding it} Given a vector, Apophenia provides functions to
calculate most of the above, e.g.:

\cindex{apop\_vector\_mean} \cindex{apop\_vector\_var}
\cindex{apop\_data\_show} \cindex{apop\_vector\_skew}
\cindex{apop\_vector\_correlation}
\cindex{apop\_matrix\_summarize}
\begin{lstlisting}
apop_data *set = gather_data();
gsl_vector v1, v2;
v1 = gsl_matrix_col(set->matrix, 0).vector;
v2 = gsl_matrix_col(set->matrix, 1).vector;
double mean1 = apop_vector_mean(&v1);
double var1 = apop_vector_var(&v1);
double skew1 = apop_vector_skew(&v1);
double kurt1 = apop_vector_kurtosis(&v1);
double covar = apop_vector_covar(&v1, &v2);
double corr = apop_vector_correlation(&v1, &v2);
apop_data_show(apop_matrix_summarize(set));
\end{lstlisting}
The last item in the code, \cind{apop\_matrix\_summarize} produces a table of
some summary statistics for every column of the data set.

\paragraph{\treesymbol{} Weighted data} Your data may be aggregated so
that one line of data represents multiple observations. For example, the
2000 U.S. Census found that 75.1\% of the U.S. population is White,
12.3\% is Black or African-American, and 3.6\% is Asian. If your survey
includes exactly 1000 people in each category, your sample will be more
representative if you take each White respondent to represent 751
people, each Black or African-American respondent to represent 123
people, and each Asian to represent 36 people. 

This is not the place to go into details about statistically sound means
of weighting data. But if your data includes weightings, then you will
want means to find the weighted moments of the data. Assuming you have
data in vectors \ci{v}, \ci{v1}, and \ci{v2} and corresponding weights in a vector
\ci{w}, then you can use the following functions. Notice that the
covaiance function takes only one weighting, because it does not make
sense to try to find the covariance of differently-weighted data, so the
\ci{w} vector is assumed to apply to both vectors.
\cindex{apop\_vector\_weighted\_mean} \cindex{apop\_vector\_weighted\_var}
\cindex{apop\_data\_weighted\_show} \cindex{apop\_vector\_weighted\_skew}
\cindex{apop\_vector\_weighted\_cov}
\begin{lstlisting}
double mean_w = apop_vector_weighted_mean(v, w);
double var_w = apop_vector_weighted_var(v, w);
double skew_w = apop_vector_weighted_skew(v, w);
double kurt_w = apop_vector_weighted_kurtosis(v, w);
double corr = apop_vector_cov(v1, v2, w);
\end{lstlisting}

\summary{
\item The most basic means of describing data is via its moments. The
basic moments should be produced and skimmed for any data set; in simple cases, there is
no need to go further.
\item The mean and variance are well known, but there is also
information in higher moments---the skew and kurtosis.
\item It is also important to know how variables interrelate, which can
be summarized using the variance-covariance matrix.
\item There is a one-line function to produce each of these pieces of information.
Notably, \ci{apop\_matrix\_summarize} produces a summary of each column
of a data matrix.
}

\section{Sample distributions}\index{sample distributions}
\label{distlist}


Here are some distributions that a variable may take on. They and are
worth memorizing before any major statistics test. Common distributions
of statistical parameters are discussed in Section \ref{dist2}.

Each has a story attached, which is directly useful for
modeling. For example, if you think that a variable is the outcome of $n$
independent, binary events, then the variable should be modeled as a
Binomial distribution, and once you estimate the parameter to the
distribution, you will have a full model of that variable, that you can
even test if so inclined.

\paragraph{Bernoulli\index{Bernoulli distribution}}

The number of times that an event which has likelihood $p$ happens in
one trial. I.e., $x$ is zero or one here.

\eqnbox{
P(x|p)&=&p^x (1-p)^{(1-x)}				\nonumber\\
E(x)&=&p						\nonumber\\
\var(x)&=&p(1-p)					\nonumber
}

\paragraph{Binomial\index{Binomial distribution}}

The number of times that an event which has likelihood $p$ happens over the
course of $n$ trials.

The notation $\left({n\atop x}\right)$ indicates
\airq{$n$ choose $x$}, the number of sets of $x$ elements that can be
pulled from $n$ objects. The equation is $$\left({n\atop x}\right) =
{n!\over x!(n-x)!},$$ and the function is \cinline{gsl\_sf\_choose(n,x)} 
(in the GSL's Special Functions section).
\index{choose!gsl\_sf\_choose@\cinlinetwo{gsl\_sf\_choose}} \cindex{gsl\_sf\_choose}

\eqnbox{
%P(x|p)&=&\left(\matrix{n\cr x}\right) p^x (1-p)^{(n-x)}		\nonumber\\
P(x|p)&=&\left(\begin{matrix}n\cr x\end{matrix}\right) p^x (1-p)^{(n-x)}		\nonumber\\
E(x)&=&np							\label{binome}\\
\var(x)&=&np(1-p)						\label{binomv}
}

\begin{itemize}
\item If $X \sim {\rm Bernoulli}(p)$, then for the sum of $n$ independent draws,
$\sum_{i=1}^n X_i \sim {\rm Binomial}(n,p)$.
\item As $n\to\infty$, Binomial$(n,p)$ $\to$ Poisson$(np)$ or ${\cal
N}(np, np(1-p))$.
\end{itemize}

Since $n$ is known and $E(x)$ and $\var(x)$ are values calculated from
the data, Equations \ref{binome} and \ref{binomv} are an oversolved
system of two variables for one unknown, $p$. Thus, you can test for
\vocab{excess variance}, which indicates that there are interactions that
falsify that the observations were iid Bernoulli events.

\paragraph{Normal\index{Normal distribution|\textbf}}

You know and love the bell curve, aka the Gaussian
distribution.\footnote{Typography note: this book will use \airq{Normal},
in an attempt to avoid un-mnemonic proper names, and with a capital to indicate
that this is a proper name and not an adjective.} See the CLT (Section
\ref{CLT}) for its application.

\eqnbox{ \label{normal}
P(x|\mu,\sigma)&=&{1\over \sigma\sqrt{2\pi}} e^{-{1\over 2}[{(x-\mu)\over\sigma}]^2}\\ 
E(x)&=&\mu							\nonumber\\
\var(x)&=&\sigma^2						\nonumber
}

\begin{itemize}
\item If $X \sim {\cal N} (a,b)$ and $Y \sim {\cal N} (c,d)$ then $X+Y \sim {\cal N}
(a+c,b+d)$.
\end{itemize}

\paragraph{Multivariate Normal}\index{multivariate normal distribution}
All of the other distributions are for a single variable---one
dimension. Let us say that we have a data set $\Xv$, which includes a
thousand observations and seven variables (so $\Xv$ is a 1000$\times$7
matrix). We know its mean is
$\mu$ (a vector of length seven) and that the covariance among the
variables is $\Sigma$ (a seven by seven matrix). Then the Multivariate
Normal distribution that one would fit to this data is:

\eqnbox{
p(\Xv|\mu, \Sigma) &=& {\exp\left(-{1\over 2} (\Xv-\mu)' \Sigma^{-1} (\Xv-\mu)\right) 
\over
   \sqrt{(2 \pi)^n \det(\Sigma)}}            \nonumber\\
   E(\Xv) &=& \mu                           \nonumber\\
   \var(\Xv) &=& \Sigma            \nonumber
}					

\begin{itemize}
\item When $\Xv$ has only one column and $\Sigma = \sigma^2\Iv$,
this reduces to the univariate Normal distribution.
\end{itemize}

Apophenia provides the function \cind{apop\_multivariate\_normal\_prob}
to evaluate this likelihood for a given data point.

\paragraph{Poisson\index{Poisson distribution}}

Say that independent events occur at the mean rate of $\lambda$ events per 
period. What is the probaility that there will be $x$ events in a single 
period? Or, say that  objects are independently scattered
with mean density $\lambda$ per square km. What is the probability that
there are $x$ objects in a randomly-selected square km?

\eqnbox{
P(x|\lambda)&=&{e^{-\lambda}\lambda^x\over x!}			\nonumber\\
E(x)&=&	\lambda							\nonumber\\
\var(x)&=&\lambda						\nonumber
}
\begin{itemize}
\item If $X \sim {\rm Poisson}(\lambda_1)$ and $Y \sim {\rm
Poisson}(\lambda_2)$ (and $X$ and $Y$ are independent), then $(X + Y) \sim {\rm Poisson}(\lambda_1 +
\lambda_2)$.  \item As $\lambda\to\infty$,  ${\rm Poisson}(\lambda)
\to {\cal N}(\lambda, \lambda)$.  \end{itemize}

\paragraph{Exponential\index{Exponential distribution}}

The story of exponential decay is that in each period, $\beta$ percent
of the current population drops out of the population.

To give an example from my own research, say that we believe that in
each period, $\beta$ crew members of a ship are flogged. If the flogging
is purely random, then $\beta$ members are flogged once, $\beta^2$
members are flogged twice, et cetera.

This is sometimes called the \vocab{Negative exponential distribution}.

\eqnbox{
p(x|\beta) &=& \frac{1}{\beta}e^{\frac{-x}{\beta}}		\nonumber\\
E(x)&=&	\beta							\nonumber\\
\var(x)&=&\beta^2						\nonumber
}


\paragraph{Beta distribution} The \ind{Beta distribution} is a flexible
way to describe data inside the range $[0, 1]$.\footnote{The \ind{Gamma
function} is defined as $\Gamma(x) = \int_0^\infty  t^{x-1} \exp(-t)
dt$. One may think of it as a continuous version of the \ind{factorial}
function, because $\Gamma(x) = (x-1)!$. It is not to be confused with
the Gamma distribution, which is so named because it is defined using
the Gamma function.}
See page \pageref{beta} for more on its uses.

\eqnbox{
P(x|\alpha,\beta) &=& {\Gamma(\alpha + \beta) \over \Gamma(\alpha) \Gamma(\beta)} x^{\alpha - 1}
(1-x)^{\beta - 1}\nonumber\\
E(x)&=&{\alpha\over\alpha+\beta}							\nonumber\\
\var(x)&=&{\alpha\beta\over(\alpha+\beta)^2(\alpha + \beta + 1)}             \nonumber
}

\begin{itemize}
\item If $\alpha = \beta = 1$, then this is the ${\rm Uniform}[0,1]$ distribution.\index{Uniform distribution}
\end{itemize}


\paragraph{\ind{Uniform distribution}} What discussion of distributions
would be complete without mention of the Uniform? It represents a neutral
prior belief that any value within $[\alpha, \beta]$ is equally possible.

\eqnbox{
P(x) &=& \frac{1}{\beta - \alpha}       \nonumber\\
E(x) &=& \frac{\beta - \alpha}{2}       \nonumber\\
\var(x) &=& \frac{(\beta - \alpha)^2}{12}   \nonumber
}

More distribution models, and a few more coded-out examples, will be added to this catalog.

\paragraph{Using the distributions} How you code the distributions
depends on what you hope to do with them. The common goal throughout the
book is to estimate the parameters of the model with data, and you can
see above that almost every parameter can be solved---sometimes
oversolved---using the mean and variance. Most of the above distributions
have an \ci{apop\_model} associated (see page \pageref{modellist} for a
list), and the \ci{estimate} function therein will use the mean or the
mean and variance to estimate the parameters.

This is known as the \vocab{method of moments} estimation, and it
matches the maximum likelihood estimate of the parameters in almost all
cases. For the Uniform, the method of moments doesn't work: the
expression $(\beta - \alpha)$ is oversolved but there is no way to solve
for $\alpha$ or $\beta$ alone. However, the reader may verify that the
most likely value for $(\alpha, \beta)$ is simply $(\min(x), \max(x))$.

These distributions are also useful for simulation, since they each
neatly summarize an oft-modeled story. Modelers will want to make random
draws from these distributions, a topic covered in Chapter \ref{boot}.

\summary{
\item Probability theorists through the ages have developed models that 
indicate that if a process follows certain guidelines, the data will
have a predictable form.
\item A single draw from a binary event with fixed probability has a
Bernoulli distribution.
\item The sum of a series of $n$ draws from a Bernoulli distribution will have a 
Binomial distribution.
\item If $\overline x$ is the mean of a set of independent, identically
distributed draws from {\em any} nondegenerate distribution, then
the distribution of $\overline x$ approaches a Normal distribution. This
is the Central Limit Theorem.
\item If events occur at the rate of $\lambda$ per period, the
rate of event occuence as a function of time will have a Poisson
distribution.
\item Given the same setup, the population present in each period has an
Exponential distribution.
\item The Beta distribution is useful for modeling a variety of
variables that are restricted to $[0, 1]$.
}

\section{Factor analysis} \label{pca} \index{factor analysis}
This is also known as \ind{principal component
analysis}, or \ind{spectral decomposition}, depending upon your field. 
The first phase (calculating the eigenvalues) is known as the \ind{singular value decomposition}.

It is a purely descriptive method.  The idea is that we want a few
dimensions that will capture the most variance possible---usually two,
because we can plot two dimensions. That is, we will project the data
onto the best plane, where \airq{best} means that it captures as much
variance in the data as possible.

After plotting the data, perhaps with markers for certain observations,
we may find intuitive descriptions for the dimensions on which we had just plotted the
data. My favorite example of this is \citet{poole:rosenthal},
who did a principal component analysis\footnote{They actually did
the analysis using an intriguing maximum likelihood method, rather
than the eigenvector method here. Nonetheless, the end result and its
interpretation is the same.} on all of the U.S. Congresses. They found
that 90\% of the variance in vote patterns could be explained by two dimensions.
Studying the data points, they determined that one of these dimensions could be
described as `fiscal issues' and the other as `social issues'. This method stands
out because Poole \& Rosenthal did not have to look at bills and place them on
either scale---the data placed itself, and they just had to name the scales.


It can be shown that the best $n$ axes, in the sense above, are the
$n$ eigenvectors of the data's covariance matrix with the $n$ largest
associated eigenvalues.

\subsection{Coding it}
This section will show you the computation of a principal component
analysis on three levels. The first goes through the steps of
calculating eigenvectors yourself; the second uses the 
GSL's built-in singular value decomposition function; the third is a
single call in Apophenia.

The only hard part is finding the eigenvalues of
$(X'X)$; the GSL saw us coming, and gives us the \cinline{gsl\_eigen\_symm} functions
to calculate the \ind{eigenvectors} of a symmetric matrix.

The GSL is too polite to allocate large vectors behind our backs, so
it asks that we pass in pre-allocated workspaces when it needs such
things. The eigenvalue-finding function requires such a workspace, so
here is a function that allocates the workspace, calls the eigenfunction,
then frees the workspace:
\begin{lstlisting}
void find_eigens(gsl_matrix **subject, gsl_vector *eigenvals, gsl_matrix *eigenvecs){
   gsl_eigen_symmv_workspace * w = gsl_eigen_symmv_alloc(*subject->size1);
   gsl_eigen_symmv(*subject, eigenvals, eigenvecs, w);
   gsl_eigen_symmv_free (w);
   gsl_matrix_free(*subject);
   *subject  = NULL;
}
\end{lstlisting}

Notice that I free the matrix whose eigenvalues are being calculated at
the end.  This is because the matrix is destroyed in the calculations,
and should not be referred to again. Now, when a pointer is freed, its value
can take on any value. It could be set to \ci{NULL}, but if the compiler
is feeling lazy, it may just leave the pointer pointing to garbage.
Explicitly setting it to \ci{NULL} makes it clear that this pointer has
been freed.

Here is how this function is used. In the tradition of C, the code is
mostly declarations, and in the last two lines, it will calculate the
covariance matrix $X'X$ for the data set, and then find its eigenvalues
and eigenvectors.  
I will assume that you have already got a data matrix ready, named
\cinline{data}, as per the last chapter. Now we need to remove the means
from the data (using \cind{apop\_matrix\_normalize}), and find $X'X$:

\cindex{apop\_dot}
\begin{lstlisting}
int ds=data->size2;
gsl_matrix *xpx;
gsl_vector *eigenvals   = gsl_vector_alloc(ds);
gsl_matrix *eigenvecs   = gsl_matrix_alloc(ds, ds);

apop_matrix_normalize(data, 'm');
gsl_blas_dgemm(CblasTrans,CblasNoTrans, x, x, xpx);
xpx = apop_dot(x, x, 1, 0);
find_eigens(&xpx, eigenvals, eigenvecs);
\end{lstlisting}

Now we have the eigenvectors and their associated eigenvalues; we need only find
the largest eigenvalues, and project the data onto their associated eigenvectors.
The GSL helps us by giving us functions for
finding the indices of the largest elements of a vector.
\begin{lstlisting}
const int dimensions_we_want = 2;
gsl_matrix *pc_space = gsl_matrix_alloc(ds,dimensions_we_want);
gsl_vector temp_vector;
int indexes[dimensions_we_want];
int i;

gsl_sort_vector_largest_index(indexes, dimensions_we_want, eigenvals);

for (i=0;i<dimensions_we_want; i++){
   temp_vector  = gsl_matrix_column(eigenvecs, indexes[i]).vector;
   gsl_matrix_set_col(pc_space, i, &temp_vector);
}
\end{lstlisting}

All that's left to do is the projection. Notice the convention I used:
the \cinline{pc\_space} has eigenvectors on its columns, and as many columns as the
dimensionality we want in the end. Below, I transpose that before premultiplying
the data set by the principal component matrix.

\begin{lstlisting}
gsl_matrix *projected = gsl_matrix_alloc(data->size1, dimensions_we_want);
gsl_blas_dgemm(CblasTrans,CblasNoTrans, pc_space, data, projected);
\end{lstlisting}

You will probably want to plot the \cinline{projected} matrix; the means
of doing so are discussed in Chapter \ref{gnuplot}

\paragraph{The easy way}

Figure \ref{svdecomposition} presents a function that takes some
shortcuts from the above. It is a
simplification of \cind{apop\_sv\_de\-comp\-osi\-tion}, which is what you would use in practice. The function simply makes
a copy of the data and feeds that in to the GSL's built-in singular-value
decomposition function, \cind{gsl\_linalg\_SV\_decomposition}, which eats the copy, and must also be fed an
additional dummy vector for its work. The function then returns the first
\cinline{dimensions\_we\_want} eigenvectors and all of the eigenvalues. I
assume that all of the input arguments have had space allocated before
calling the function.  

\codefig{svdecomposition}{A function to find the singular-value decomposition of a data set}


\summary{
\item Factor analysis projects data of several dimensions onto the
dimensions that display the most variance.
\item Given the data matrix $\Xv$, the process involves finding the
eigenvalues of the matrix $\Xv'\Xv$ associated with the largest
eigenvalues, and then projecting the data onto the space defined by
those eigenvectors.
\item \ci{apop\_sv\_decomposition} runs the entire process for the
efficiently lazy user.
}


\section{Linear models}
\label{cat}

Assume that our variable of interest, $\yv$, is
described by a linear combination of the explanatory variables, $\Xv$,
plus maybe a Normally-distributed error term, $\uv$. In short,
$\yv=\Xv\betav + \uv$, where we will estimate the values of $\betav$. 

Unlike the models to this point, this model implicitly makes a
causal claim: the variables listed in $\Xv$ cause $\yv$ to take the values they
do. However, 
there is no true concept of \ind{causality} in statistics.
The question of when statistical evidence of causality is valid is a tricky
one which will be left to the volumes that cover this question in
detail; for the purposes here, the reader should merely note the shift in
descriptive goal, from fitting distributions to telling a causal story.

There is a catalog of methods to estimate the values of $\betav$,
depending on the assumptions behind how the $\Xv$s are related. If they
are basically unrelated, then we can use the model known as {\em ordinary
least squares} (fully specified below). When we believe that some parts
of the $\Xv$ matrix are correlated to other rows, such as rows $t$ and
$t+1$ of 
time series data, then we need to write down a matrix
$\Sigma$ to describe the covariances.

Thus, we can write down a catalog of models. The baseline is OLS, but by
providing different forms for $\Sigma$ or other minor modifications, we
can write down other models. The computational process is similar
for all.

[A coding note: if $\Xv$ is ten by a million, then $\Sigma$ is a million
by a million, which is still orders of magnitude beyond our computing
power. There are two approaches one could take: one would be to use
regularities in the $\Sigma$ matrix to do pencil-and-paper math before
doing the computations, so that something nearer to a closed-form
solution can be calculated without writing down $\Sigma$. The other
option would be to use a \index{sparse matrices}sparse matrix, in which
we write down only those elements of the matrix that are not zero. 

Apophenia intends to use the sparse matrix method where possible, but this is not yet
implemented. \cind{apop\_GLS.estimate}() currently takes a full matrix
for $\Sigma$, making it wholly unusable for data sets larger than maybe a
thousand observations. The sparse-matrix implementation of GLS is
left as an exercise to the reader. In the mean time, the reader has the
setup-specific models.]

Let $\betav$ be the true parameters of the process that produced the
data. There is no way to truly know $\betav$, but we can estimate it
using techniques derived elsewhere and described below. Let 
$\hat\betav$ be the estimate of $\betav$.  There are many ways to
describe when $\hat\betav$ does a better or worse job at estimating 
$\betav$; the key feature we will be demanding is that the estimator be
unbiased, meaning that in the limit, $E(\hat\betav) = \betav$.

The catalog describes the mean and variance of the $\hat\betav$s that
each model derives, because the Normal distribution---the basis for all
hypothesis tests---requires those two inputs,
and without them we can do no post-estimation testing.  
In Section \ref{tstat} we get rid of $\sigma$, but we
need to know $\mu$, and unbiasedness guarantees us that if all is going
well, $\mu =\hat\betav$.

\subsection{OLS}
\index{Ordinary Least Squares|(textbf}\index{OLS|see{Ordinary Least Squares}}
The model: 
\begin{itemize}
\item $\yv=\Xv\betav + \uv$
\item $N$=the number of observations. $\yv$ and $\uv$ are $N \times 1$
matrices.
\item $K$=the number of parameters to be estimated. $\Xv$ is $N \times K$;
$\betav$ is $K$ by 1
\item It is customary that the first column of $\betav$ is a column of
ones. If you do not do that, then you need to replace every column $\xv_i$ below
with $\xv_i-\overline x_i$, the equivalence of which is left as an exercise
for the reader.\footnote{If you would like to take the route of
normalizing each column to have mean zero, try
\cind{apop\_matrix\_norm\-al\-ize}\ci{(dataset, 'm')}.}
\end{itemize}

Assumptions:

\begin{itemize}
\item $\uv$ is normally distributed.
\item $E(\uv) = 0$.
\item $\var(u_i) = \sigma^2$, a constant, $\forall\ i$.
\item $\cov(u_i, u_j) = 0$, $\forall\ i\neq j$. This plus the above rule
mean that the variance-covariance matrix for the observations' errors is
$\Sigma =
\sigma^2{\bf I}$.
\item $\var(\Xv_k)>0$, and as $n\to\infty$, it remains finite.
\item The columns of $\Xv$ are not collinear (so $\det(\Xv'\Xv)\neq 0$).
\item $N>K$.
\end{itemize}

When all of that holds, then:

\eqnbox{
\hat\betav_{\rm OLS}  &=& (\Xv'\Xv)^{-1}(\Xv'\yv)			\nonumber\\
E(\hat\betav_{\rm OLS} ) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm OLS} ) &=& \sigma^2(\Xv'\Xv)^{-1}		\nonumber
}

	\index{Ordinary Least Squares|)textbf}
\subsection{GLS} \label{GLS}\index{GLS|see{Generalized Least Squares}}
\index{Generalized Least Squares} 
Generalized Least Squares generalizes OLS by allowing $\uv'\uv$ to be a
known matrix $\Sigma$, with no additional restrictions.
Note how neatly plugging $\sigma^2{\bf I}$ in to the
estimator of $\betav$ and its variance below reduces the equations here to the OLS
versions.

\eqnbox{
\hat\betav_{\rm GLS} &=& (\Xv'\Sigma^{-1}\Xv)^{-1}(\Xv'\Sigma^{-1}\yv)			\nonumber\\
E(\hat\betav_{\rm GLS}) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm GLS}) &=& (\Xv'\Sigma^{-1}\Xv)^{-1}		\nonumber
%var(\hat\betav_{GLS}) &=& 
% 			(\Xv'\Xv)^{-1}\Xv'\Sigma\Xv(\Xv'\Xv)^{-1}\nonumber
}

\subsection{IV} \index{instrumental variables} 
\index{IV|see{instrumental variables}}	\label{IV} 
If $\Xv$ is measured with error, then $\hat\betav_{OLS}$ is inconsistent and
asymptotically biased toward zero (in most cases, see e.g. \cite{kmenta},
p 349). Bias is a big deal, so we need an alternate method. If there exists
another instrumental variable $\Zv$ such that $\cov([Z_i-\overline
Z][u_i-\overline u])=0$, then the following holds:

\eqnbox{
\hat\betav_{\rm IV} &=& (\Zv'\Xv)^{-1}(\Zv'\yv)			\nonumber\\
E(\hat\betav_{\rm IV}) &=& \beta					\nonumber\\
\var(\hat\betav_{\rm IV}) &=& 
	\sigma^2(\Zv'\Xv)^{-1}\Zv'\Zv(\Xv'\Zv)^{-1}	\nonumber
}

Note how the variance shrinks as $\Zv'\Xv$ increases, and if
$\Zv'\Xv\approx{\bf 0}$, you can not use this method (since
$\det(\Zv'\Xv) \to \infty$). 

\subsection{Constrained LS \index{least squares!constrained}}
\label{constrainedls}

Add the constraint that $\Qv'\betav=\cv$. Then:

%\wideeqnbox{6.2in}{
\wideeqnbox{\textwidth-0.5cm}{
\hat\betav_{CLS} &=& \hat\betav_{OLS}-
(\Xv'\Xv)^{-1}\Qv(\Qv'(\Xv'\Xv)^{-1}\Qv)^{-1}
		(\Qv'\hat\beta-\cv)			\nonumber\\
%E(\hat\betav) &=& \beta					\nonumber\\
\var(\hat\betav_{CLS}) &=& \sigma^2[(\Xv'\Xv)^{-1} - 		
(\Xv'\Xv)^{-1}\Qv(\Qv'(\Xv'\Xv)^{-1}\Qv)^{-1}\Qv'(\Xv'\Xv)^{-1}]
\nonumber
}
(See, e.g., \cite{amemiya:ez}.)
If the constraint is satisfied by $\hat\beta$, then
$(\Qv'\hat\beta-\cv)$ is zero and $\hat\betav_{CLS}=\hat\betav_{OLS}$.
Though, you have to ask yourself why you are imposing a constraint that
you have not tested; perhaps you should first have a look at the \ind{F test}
based on this form, in Section \ref{ftestsec}.

\subsection{\ind{FGLS}} \index{Generalized Least Squares!Feasible}

If we do not know $\Sigma$, which is universally the case except on
test questions, we need some way to make the GLS estimation Feasible. So we
assume that $\Sigma$ is a function of one parameter, $\Sigma(\rho)$.

\label{ts}\index{time series|(}
\paragraph{\ind{AR(1)} time series model}

This is where errors are autoregressed by one period.\footnote{AR(4) is
also well studied, because of seasonal data.} I.e.,
$$y_t=\Xv_t\betav + \rho\epsilon_{t-1} + u_t$$
where $u_t$ is truly distributed ${\cal N}(0,1)$, and $\epsilon_t=
\rho\epsilon_{t-1} + u_t$. This can come about from  a form like
$y_t=\Xv_t\beta + y_{t-1}\gamma + \epsilon_t.$


Since $\epsilon_{t-1}$ includes $\rho\epsilon_{t-2}$, 
$\epsilon_{t-2}$ includes $\rho\epsilon_{t-3}$, et cetera,
$\rho^n u_{t-n}$
appears in the regression for period $t$. The covariance matrix should
be clear in your head now: it's $\sigma^2$ times one on the diagonals,
$\rho$ on the off-diagonal, $\rho^2$ on the off-off-diagonals, \&c. In
this form, the $\rho$ really is the correlation coefficient---$\rho$
from Section \ref{correlation}.

So all we need to apply GLS is $\hat\rho$. It so happens that OLS is
consistent for the AR(1) case, so we can find $\hat\epsilon$, and then
just find $r=(\sum \hat\epsilon_t \hat\epsilon_{t-1})/(\sum
\hat\epsilon_t^2)$. From there, proceed as with GLS.
\comment{[Two-stage least squares here]}

\index{time series|)}

\subsection{Fitting it} 
To summarize,
the coefficients of  the line of best fit for the OLS model is the value of $\betav_{\rm OLS}$ that solves:
$ \betav_{\rm OLS} = (\Xv'\Xv)^{-1}\Xv'\yv$.

I will assume that we have already read a data matrix in to \cinline{apop\_data *set}. First, here is the easy way:
\begin{lstlisting}
apop_estimate_show(apop_OLS.estimate(set, NULL));
\end{lstlisting}
But if the assumptions of OLS do not fit, then you will need to know how
the \cinline{a\-pop\_OLS.\-est\-i\-mate} function works, so the remainder of this
section goes over the steps one would take to estimate $\betav$.  \cindex{apop\_OLS}

Typically, the $\yv$
values are the first column of the data set, so we would like to extract that to a separate variable:
\begin{lstlisting}
gsl_vector      *y_data         = gsl_vector_alloc(set->matrix->size1);
gsl_matrix_get_col(y_data, set->matrix, 0);
\end{lstlisting}

We now have the $\yv$-values safely stored.  When doing an OLS projection,
there needs to be a column of ones in the data set (because OLS is an
{\sl affine} linear projection), so we can overwrite the first column
with ones. 
\cindex{gsl\_vector\_set\_all} 
\begin{lstlisting}
gsl_vector v         = gsl_matrix_column(set->matrix, 0).vector;
gsl_vector_set_all(&v, 1);  
\end{lstlisting}


Now we have both the $\yv$ and the $\Xv$ in the equation, so we can find $(\Xv'\Xv)$ and $(\Xv'\yv)$.
Both parts are a
simple application of the linear algebra functions from earlier---a matrix $\cdot$ a matrix, and a matrix $\cdot$ a vector:
\begin{lstlisting}
apop_data *xpx = apop_dot(set,set,1,0);
apop_data *xpy = apop_dot(set,apop_vector_to_data(y_data),1);
\end{lstlisting}

The GSL has a function to solve equations of the type ${\bf A} \betav =
{\bf C}$ using \ind{Householder transformations}, and this happens to be is exactly the form we have here---$(\Xv'\Xv)\betav = (\Xv'\yv)$.  \label{ols}
%\begin{verbatim}
\begin{lstlisting}
        gsl_linalg_HH_solve (xpx->matrix, xpy->vector, *beta);
\end{lstlisting}
%\end{verbatim}

The vector \cinline{beta} now lives up to its name, holding the coefficients
$\betav$. Notice that we never had to take an inverse.  

An alternative approach actually takes the inverse of $(\Xv'\Xv)$. In practice, this is
almost necessary, because the variance is $\sigma^2 (\Xv'\Xv)^{-1}$.
The matrix that projects data onto the space defined by $\Xv$,
often called the `hat matrix', is also derived from the inverse: ${\bf H} = \Xv(\Xv'\Xv)^{-1}\Xv'$. Although it may
make the computer sweat for large matrices, we humans need only make one
function call to find $(\Xv'\Xv)^{-1}$:
\begin{lstlisting}
gsl_matrix *xpxinv;
apop_det_and_inv(xpx->matrix, &xpxinv, 0, 1);
\end{lstlisting}
Two more lines of algebra will calculate the hat matrix for us: 
\lstset{texcl=true}
\begin{lstlisting}
//Find $\Xv(\Xv'\Xv)^{-1}$:
apop_data *first_part = apop_dot(set, apop_matrix_to_data(xpxinv),0,0);
//Find $\Xv(\Xv'\Xv)^{-1}\Xv'$:
apop_data *hat_matrix = apop_dot(first_part, set,0,1);
apop_data_free(first_part);
\end{lstlisting}
\lstset{texcl=false} %for now.

If we have a second data set \cinline{set2} of the appropriate number of
rows (meaning that \cinline{(set->matrix-$>$size1 == set2->matrix-$>$size1)}, then we can use the hat matrix to do the projection:\\
\begin{lstlisting}
gsl_matrix * projected_data = gsl_matrix_calloc(set->matrix->size1,set2->matrix->size2);
gsl_blas_dgemm(CblasNoTrans,CblasNoTrans, hat_matrix, set2->matrix, projected_data);
\end{lstlisting}
If the hat matrix is of few enough dimensions, we can plot the
projection using \cind{apop\_data\_print}; see page \pageref{gnuprint}.

\summary{
\item The Ordinary Least Squares model assumes that the dependent variable is an affine linear function of the others. 
Given this and other assumptions, the likelihood-maximizing parameter vector is $\betav_{\rm OLS} = (\Xv'\Xv)^{-1}(\Xv\Yv)$.
\item If $\Sigma \neq \Iv$, then  $\betav_{\rm GLS} = (\Xv'\Sigma\Xv)^{-1}(\Xv\Sigma\Yv)$. Depending on the value of $\Sigma$, one can design a number of models.
\item Coding these processes is a simple question of stringing together
lines of linear algebra operations from Chapters \ref{linear_algebra}
and \ref{apop}.
}

\section{Multilevel modeling} A key assumption to the models to this
point is that each observation is independently and identically distributed
relative to the others. However, observations often fall into clusters,
such as families, classrooms, or geographical region. 
A regression that simply includes a family/classroom/region dummy variable
asserts that each observation is iid relative to the others, but its
outcome rises or falls by a fixed amount depending on its group
membership.

But this is often false: family members heavily influence each other.
A better alternative may be to do a model estimation for each group
separately. At the level of the subgroup, the iid assumption is more
likely to hold. Then, once each group has been estimated, the parameters
can be used to fit a model where the observational unit is the group.

Writing a model to execute such a multilevel model is not
difficult---just write a model whose \cinline{estimate} function calls
another model's \cinline{estimate} function. 

An example will be forthcoming.

