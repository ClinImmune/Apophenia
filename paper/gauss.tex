\def\var{\hbox{var}} \def\cov{\hbox{cov}}
%@-     so lgrind will play nice with makeindex
\chapter[Gaussian tricks]{Hypothesis testing with Gaussian distributions} \label{gauss}

This subsection covers most of what we traditionally learn in first-year statistics. 
\comment{
Much of it is obsolete. It covers parameters which we know to have a Normal (aka Gaussian) distribution,
or those things which are derived therefrom. These are nice because the math has already been done, by
people who lived before computers, meaning that we don't need metaphorically heavy machinery to do most of
the calculations here.} Most of the work will consist of taking a dot product, maybe inverting a matrix,
and then looking up a number in a table. 

Everything here depends on the Central Limit Theorem. If your data doesn't fit the CLT, then please don't use
these techniques. Work out how your data is distributed, to the best of your abilities
(try bootstrapping, Chapter \ref{boot}), and then write down a likelihood function. If you're looking to
estimate model parameters, do a maximum likelihood estimation; if you're looking to test a hypothesis,
write down a likelihood ratio based on the distribution you've just calculated.

\section{Meet the Gaussian family}

\subsubsection{The \ind{Central Limit Theorem}} is the first piece of magic
that we need. As $n\to \infty$, we know that $\sqrt{n} (\overline X -
\mu)/\sigma \sim {\cal N}(0,1)$, regardless of where $X$ came
from. With that regularity of nature, we can derive all of the following distributions.
\label{CLT}

\subsection{Normal\index{normal distribution}} 

Here's what we mean by ${\cal}N(\mu,\sigma)$:
\begin{eqnarray}					\label{normal}
P(x|\mu,\sigma)&=&{1\over \sigma\sqrt{2\pi}} e^{-(1/2)[(x-\mu)/\sigma]^2}\\ 
E(x)&=&\mu							\nonumber\\
\var(x)&=&\sigma^2						\nonumber
\end{eqnarray}

Its big problem is that it depends on $\sigma$, an
unknown. It also depends on $\mu$, but if the statistic
being tested is unbiased---like most of the estimators of $\beta$ in the
catalog above---that gives $\mu$ to you.

\begin{itemize}
\item $\{{\cal N} (a,b)\} + \{{\cal N} (c,d)\} \sim {\cal N}
(a+c,b+d)$ (where $\{x\}$ is a variable with the given distribution).
\end{itemize}

\subsection{$\chi^2$}\index{chi squared distribution@$\chi^2$ distribution} $[\sum_n (x-\overline
x)^2]/\sigma^2\sim \chi^2_{n-1}$. The numerator there is the sample
variance times $n$, so we can test that the sample variance equals a
given $\sigma^2$ with this. But unless we assume $\sigma$ like that,
we're up the same creek as with the Normal.

\begin{itemize}
\item $\{{\cal N} (0,1)\}^2 \sim \chi^2_1$

\item $\{\chi^2_1\}$ + \dots + ($n$ of these) + \dots + $\{\chi^2_1\} \sim \chi^2_n$

\end{itemize}			\label{chisq}

Notice that the sample variance at the top of this section is $\sim
\chi^2_{n-1}$, not $n$, because given the first $n-1$ data points, the
last one can actually be calculated from that data; meaning that we
effectively have the sum of $n-1$ $\{\chi^2_1\}$s plus a constant.

\subsection{Student's t\index{t distribution}}  ${\overline X - \mu \over S/\sqrt{n}}\sim
t_{n-1}$, where $s=\uv'\uv/n$. This is a work of genius by Mr. Student.
By dividing a normal by $\sqrt{\chi^2_{n-1}/(n-1)}$, the unknown
$\sigma$s cancel, and we have a function whose elements are all known,
and its distribution.  If $n=2$, we call it a Cauchy distribution.
\label{tstat}


\subsection{$F$\index{F distribution}}  This is the same cancel-the-$\sigma$s trick as with the $t$, but here the form
is $[\chi^2_m/m]/[\chi^2_n/n]\sim F(m,n)$. For example, if the numerator
is a ${\cal N}(0,1)$, and the denominator is a $\chi^2$ based on the
sample variance, then this is the square of the $t$ statistic.
The nice thing about the $t$ distribution is that it's easy and it
allows one-tailed hypothesis tests. But if you want to test a
multivalued hypothesis, then you need to do an $F$ test.

\subsection{Calling these functions from the GSL}
There are four things that cover most of what you will be doing. 
The first is simply looking up the vaule of the PDF at a given point. Here are the functions:
%\begin{verbatim}
%[
#include <gsl/gsl_randist.h>
double gsl_ran_gaussian_pdf (double X, double SIGMA);
double gsl_ran_tdist_pdf (double X, double NU);
gsl_ran_fdist_pdf (double X, double NU1, double NU2);
double gsl_ran_chisq_pdf (double X, double NU);
%]
%\end{verbatim}
The prefix {\tt gsl\_ran} indicates that these functions are from the
random number generation module. (Random number generation itself will be delayed to page
\pageref{randomnumbers}.) Notice that there is no mean for the Normal and neither mean nor variance
for the T-distribution, so we may need to modify $X$ accordingly. For
example, here is a function for a t-distribution with approaches the
mean $M$ and variance $v$ as {\tt NU}$\to \infty$:
%\begin{verbatim}
%[
#include <gsl/gsl_randist.h>
#include <math.h>      //sqrt
double gsl_ran_tdist_pdf_with_mv(double X, double mean,
                               double var, double NU){
    double X_scaled = (x - mean)/sqrt(var);
    return gsl_ran_tdist_pdf(X_scaled, NU);
}
%]
%\end{verbatim}

Next, there's calculating the CDF above or below a point. 
The {\tt P-}functions below calculate the CDF below a point, i.e.
$\int_{-\infty}^X f(y) dy,$
while the {\tt Q-}functions calculate the CDF above a point, i.e.
$\int^{\infty}_X f(y) dy.$
These will obviously add to one, so you can express any area in terms of whichever function is clearest.

For example, if we find that our Normally-distributed mean is 2.2 standard
deviations above zero, then we can reject the one-tailed hypothesis that
the mean is zero with probability {\tt 1 - gsl\_cdf\_gaussian\_Q(2.2, 1)}.

\index{Gaussian distribution|see{normal distribution}}
\index{chi squared distribution@$\chi^2$ distribution!gsl\_cdf\_chisq\_P@{\tt gsl\_cdf\_chisq\_P}}
\index{t distribution!gsl\_cdf\_tdist\_P@{\tt gsl\_cdf\_tdist\_P}}
\index{F distribution!gsl\_cdf\_fdist\_P@{\tt gsl\_cdf\_fdist\_P}}
\index{normal distribution!gsl\_cdf\_gaussian\_P@{\tt gsl\_cdf\_gaussian\_P}}
\ttindex{gsl\_cdf\_...}
%\begin{verbatim}
%[
#include <gsl/gsl_cdf.h>
double gsl_cdf_gaussian_P (double X, double SIGMA);
double gsl_cdf_tdist_P (double X, double NU);
double gsl_cdf_fdist_P (double X, double NU1, double NU2);
double gsl_cdf_chisq_P (double X, double NU);
%]
%\end{verbatim}
\dots plus all of these with the {\tt P} replaced by a {\tt Q}.


In the other direction, we may want to know where we will need to be to reject a hypothesis with 95\%
certainty. For example, a Value-at-risk oriented regulator will want to know what a bank is likely to lose 
one day in the month. That is, what is the value of the 1-in-20, or 5\%, point on the CDF?
Assuming a Normal(M,S) distribution of profit and loss,\footnote{This is false. Securities
typically have leptokurtic (fat-tailed) returns.} the bank will report a \index{value at risk} of {\tt
gsl\_cdf\_gaussian\_Pinv (0.05, S)+M}. Here are the other function delcarations:
\index{chi squared distribution@$\chi^2$ distribution!gsl\_cdf\_chisq\_Pinv@{\tt gsl\_cdf\_chisq\_Pinv}}
\index{t distribution!gsl\_cdf\_tdist\_Pinv@{\tt gsl\_cdf\_tdist\_Pinv}}
%\begin{verbatim}
%[
#include <gsl/gsl_cdf.h>
gsl_cdf_gaussian_Pinv (double X, double SIGMA);
double gsl_cdf_chisq_Pinv (double P, double NU);
double gsl_cdf_tdist_Pinv (double P, double NU);
%]
%\end{verbatim}
\dots plus all of these with the {\tt P}s replaced by {\tt Q}s.
Notice that there is no inverse for the CDFs of the F distribution.


\section{Testing a hypothesis}
We have all the ingredients we need to test a hypothesis; all that
remains is to glue them all together.  In the last chapter, the code
found the coefficient vector {\tt beta} and {\tt xpx\_inv}, which is the
variance-covariance matrix of {\tt beta}.  Now assume that {\tt beta}
is normally distributed (for example, the original data set consists
of iid draws from a homoskedatic distribution). Then we can use the
above functions to find the probability that each element of {\tt beta}
is different from zero:

%\begin{verbatim}
%[
\\We enter with the vector beta and matrix xpx_inv
#include <gsl/gsl_cdf.h>  //CDF functions
#include <math.h>         //abs
int i;
double confidence[beta->size];
for (i=0;i< beta->size; i++){
    confidence[i] = gsl_cdf_gaussian_Q( abs(gsl_vector_get(beta,i)), 
                         gsl_matrix_get(xpx_inv, i,i));
    confidence[i] -= .5;
    confidence[i] *= 2;
}
%]
%\end{verbatim}
The last two steps converted from the one-tailed area below {\tt beta}
to a two-tailed area between $\pm${\tt beta}.

But we have the whole variance-covariance matrix to work with, not just the diagonal, so we can readily
test any joint hypothesis that suits our fancy.

\comment{
\section{Good ol' OLS}
This section would discuss how to write your very own {\tt regress} function, which, as noted above,
just consists of solving for the $\beta$ in $(X'X)\beta = X'Y$. 
The last chapter showed us the code to find the betas with the smallest squared error; this section will
cover testing hypotheses about those betas.

\subsection{GLS} Generalized least squares refers to any method that uses a variance-covariance matrix
that isn't the identity matrix. Having written our {\tt regress} function, it's almost trivial to
generalize to GLS. But the fun of GSL is in working out what that matrix should be.  This section would
give examples of favorites such as AR-1 processes from time series analysis (and hey, why not AR-$N$?).

The easiest thing to do is simply calculate the variances and covariances of the data itself:

%\begin{verbatim}
%[
#include "gsl_convenience_fns.h"
gsl_vector_view one_col, another_col;
gsl_matrix *var_covar=gsl_matrix_alloc(data->size2, data->size2);
int i,j;

for(i=0;i< data->size2; i++){
   for(j=0;j<= i; j++){
      one_col = gsl_matrix_column(data, i);
      another_col = gsl_matrix_column(data, j);
      covar = cov(&one_col.vector, &another_col.vector);
      gsl_matrix_set(var_covar, i, j, covar);
      if (j!=i) gsl_matrix_set(var_covar, j, i, covar);
   }
}
%]
%\end{verbatim}

Now that we've got this variance-covariance matrix, $\Sigma$, we can
apply the formula $\beta_{GLS} = (X'\Sigma X)^{-1} (X'\Sigma Y)$.
}

\comment{
This gives us  $N\cdot (N+1)/2$ covariances that need to be calculated; your data is probably not up to
the task of providing enough information to allow this to be estimated, in which case you'll need to
compromise and impose some number of restrictions. 

For example, FGLS (Feasible GLS) assumes that the var-covar matrix is of the form 
$$\left[\matrix{
1 &\rho & \rho^2 & \rho^3\cr
\rho &1 & \rho  & \rho^2\cr
\rho^2 & \rho & 1 & \rho\cr
\rho^3 & \rho^2  & \rho &1\cr
}\right].$$
This makes life easy because we now only have one variable to estimate.
}

\section{Hypotheses about the variance}
Chi-squared tests and their utility. We need to calculate the sum of errors squared 
and then compare that to the Chi-squared table. 

\section{F tests}

You can test any hypothesis in the OLS world using an appropriate F test.
Mathematically, it's a generalization of OLS, and we can implement it
in the code as such.


\section{Testing the assumptions} Errors have to be normally distributed, or else
the whole OLS system here doesn't apply. Many stats package user's manuals
suggest plotting the errors and then squinting at the picture. Me, I'm a
fan of a slightly more scientific approach, which is based on the fact
that a Normal distribution has only two parameters: the mean and the
standard deviation. Everything else about the Normal distribution is
defined therefrom. Notably, the third moment is zero, and the fourth
moment is $3 \sigma^4$. We've already written everything we need to calculate all of these easily.

\comment{
\begin{verbatim}
#include "gsl_wrappers.h"

gsl_vector *errors = gsl_vector_alloc(beta_dot_x->size);
int i;
for (i=0;i< beta_dot_x->size; i++)
   gsl_vector_set(errors, i, gsl_vector_get(beta_dot_x, i) 
                                 - gsl_matrix_get(data,0,i))

double three_var_squared = 3* var(error)*var(error);
printf("The mean is: %g\n", mean(error));
printf("The var  is: %g\n", var(error));
printf("The skew is: %g\n", skew(error));
printf("The un-normalized kurtosis is: %g, and should be %g, for a difference of %g.\n", 
                         skew(error), three_var_squared,
                         skew(error)-three_var_squared);
\end{verbatim}
}

[I had some code here, but I don't like it. It just prints the first four moments of a data vector.]

You can either just eyeball this, and decide based on the scale of the variance
whether the skew and the kurtosis look like they're far off from where they should
be, or you could bootstrap the variance of the kurtosis, which would let you find
a confidence interval around $3 \sigma^4$ and state with a certain percentage
of certainty that the kurtosis is or is not where it should be. Hint: the CLT applies to the kurtosis,
so you know it has a Gaussian distribution.

[Oh, and by the way, the GSL gives you normalized \index{kurtosis},
not actual kurtosis.  Will discuss that here, since I didn't notice that
line in the documentation 
when I first started using the GSL and it really screwed me up for a
while.]

