\section{Why C?}
\index{statistics packages!rants regarding|(}
You may be surprised to see a book entitled \airq{Modern Statistical
Computing} that is based on a language written in 1972. 
Why use C instead of a specialized language like SAS, Stata, SPSS,
S-Plus, SIENA, SUDAAN, SST, SHAZAM, GAUSS, GAMS, GLIM, GENSTAT, GRETL, EViews,
PcGive, MatLab, Minitab, Mupad, Maple, Mplus, Maxima, Mathematica, WinBUGS, TSP,
HLM, R, RATS, LISREL, LIMDEP, Lisp-Stat, Octave, OxMetrics, Weka, or Yorick?
There are more than enough books that advocate for the use of such
languages; as one editor at a well-known scientific publishing company
explained to me, ``\dots most of the statistics [book] market is quite
heavily influenced by the software companies that service the field.''
This may be the only book to advocate statistical computing with
a general computing language, so I will take some time motivating the
choice as clearly as possible. Over the course of this, you should also
get a better idea of why C is the way it is, and why modern numerical
analysis
is best done in an old language.

\paragraph{Computational speed---really} Using a stats package sure
beats inverting matrices by hand, but as computation goes, many stats
packages are still relatively slow, and that slowness can make otherwise
useful statistical methods infeasible.

R and Apophenia use identical C code for doing the \ind{Fisher
exact test}, so it makes a good basis for comparison.\comment{The
Apophenia code was literally cut and pasted from R's source code. The
only key change was that instances of \ci{R\_malloc} were replaced by
\ci{malloc}. This is not the place for a full exposition on memory
management in packages like R, but as the example shows, its basic design
emphasizes minimal maintenance at the cost of speed.} On my laptop,
a C program using Apophenia does 10,000 of these tests in 10.1 seconds;
a minimal R script does the same work in nine minutes---over fifty times
as long. Nor is this just an isolated test case: I can't count how many
times people have told me stories about an analysis or simulation that
took days or weeks in a stats package but ran in minutes after they
rewrote it in C.

OLS used to be the only technique that we had the computing power to
actually implement. But when you can do a maximum likelihood search
or Monte Carlo simulation in reasonable time, you can write models
that rely on fewer false assumptions. The Monte Carlo examples in Section
\ref{billiondraws} were produced using over a billion draws from $t$
distributions. If your stats package can't produce a few hundred thousand 
draws per second (some can't), such work will be unfeasibly
slow.\footnote{If you can produce random draws from $t$ distributions
as a batch (\ci{draws <- rt(5000000, df)}), then R takes a mere three
times as long as comparable C code. But if you need to produce them
individually (\ci{for (i in 1:5000000) draw <- rt(1, df)}), then R 
takes about ten times as long as comparable C code. On my laptop, R in
batch mode produced draws at the rate of $\approx 122,000/{\rm sec}$,
while C produced draws at $\approx 425,000/{\rm sec}$.}

The features of C that stats package users will find most annoying are
exactly the features that make it orders of magnitude faster than the cuter
alternatives.  Type declarations mean that the computer does not need
to guess the data type you mean at every use, saving time on
every calculation.  Pointers and the call-by-reference mechanism reduce
internal copying during function calls.

Further, C's syntactic unpleasantness pays off immensely when extending
quick-and-dirty scripts to involved, complex analyses.
Not declaring types seems convenient at first, but playing guess-the-type
with a package is often frustrating for complex data types, such
as a list of potentially empty lists. C's multiple levels of scope and 
modular files and functions may be counterintuitive at first,
but those features are essential to your sanity when
writing programs with more than a few pages of code.

\paragraph{Extensibility}
There is nothing more embarrassing than a presenter who answers a question
about an anomaly in the data or analysis with `Stata didn't have a function to
correct that.'\footnote{Yes, I have heard this in a real live presentation
by a real live researcher.} Users need a means of breaking open canned
statistical routines to make sure they fit the situation at hand.

Connoisseurs of computing languages often refer to high-level and
low-level languages. The low-level languages are close to the machine,
referring to individual memory addresses and register operations,
giving you perfect control but demanding a lot of work. The high-level
languages do complex things with one command, like a {\tt
regress(data)} command to produce pages of output, giving you no control
but ease-of-use.

\codefig{regress}{C programs do not have to be filled with memory
management commands and byte-shifting. With the right commands, they are
as brief as programs in hipper modern languages.}

C can be used anywhere along the spectrum. Figure \ref{regress} gives
an example on the high end, showing a complete C program that works on
the level of data sets and statistical models. It reads in a data set in
one line, runs a regression in the next, and displays a page of
results in the next.

You pick the balance of laziness and precision that you desire. You
can write code to manipulate individual memory locations, use the GNU Scientific
Library to manipulate matrices, or use Apophenia to estimate models---and
you can do it all in the same program.

The how-to code in this book is often repeated at multiple levels. I
will first show you the low-level way of doing things over the course
of three pages, and then tell you the single function call that will do
all the work for you. You can use the prepackaged function calls for
most work, but when the assumptions don't match your data you will be
ready to crack open the black box and modify it to suit.

\paragraph{The lingua franca}
%Here in the modern day, everybody is computer-literate and connected.
Science progresses through the open dissemination of information, data,
and analyses. But with dozens of mutually incompatible stats packages
floating around, the open marketplace of ideas turns into a tower of Babel.
Meanwhile, C is the lingua franca of computing. The software I discuss
in this book is available for the system you are using, and for the
system your colleagues are using, right now, for free.

Further, it is important to be compatible with yourself. Will you be able
to verify and modify your work five years from now? Since C was written
in 1972, dozens (maybe hundreds) of stats packages have come and gone,
and those who try to follow the trends have on their hard drives dozens
of scripts that they can't run anymore.  Meanwhile, correctly written
C programs written in the 1970s will compile and run on new PCs.

\paragraph{But C is ugly!} C's detractors point out that C code is
always filled with cruft about memory management and endless declarations
instead of practical code to do things. It requires piecing together bits
that all work at a level lower than that at which anyone would want to
work these days.

They get this impression because many C textbooks, some from the 1970s
and some of more recent vintage---are written for people writing basic
software for machines with comically limited memory---situations where
memory management and reliable byte-wrangling are essential.

But that is where the \airq{modern} part of this book's title comes
in. Have another look at Figure \ref{regress}.  It does not bother with
memory management, because it is a short program and your computer has
literally a million times more memory. It includes the declarations
that some find to be so onerous, but they consist simply of preceding
the first use of \ci{dataset} and \ci{est} with type names.
Most importantly, the commands themselves are not
about bit-shifting, but using high-level structures like data sets and
statistical models.  Yes, there are thousand-page code bases written in
baroque, bit-twiddling C, like the Windows and Linux operating systems
or most of the stats packages listed above. But Figure \ref{regress}
shows that C code can also be short, concise, and thoroughly modern.

There is the complaint that C is a compiled language, meaning that
you must write your instructions to a text file and compile the file
to an excecutable before seeing results. Interpreted systems,
like Perl, Python, or most of the packages above, provide a means of
giving immediate feedback for every command. Interpreted languages
certainly have their convenience, and are useful for certain settings,
such as asking a few quick questions of a data set. But an un-replicable
analysis based on clicking an arbitrary sequence of on-screen buttons
is as useful as no analysis at all.  Your analysis is not done until you
have a script that takes the data as far as possible along the pipeline
from raw format to final published output. In this context, writing \ci{program.c} for
a compiler and writing \ci{script.do} for an interpreter become about
equivalent---especially since compilation on a modern computer takes on
the order of 0.0 seconds.

C is by no means the best language for all possible purposes.
Different systems have built-in syntax for communicating with other programs, 
handling text, building Web pages, or producing certain graphics. But
for data analysis, C is very effective. It has its syntactic warts: you
will forget to append semicolons to every line, and will be frustrated
that \ci{3/2==1} while \ci{3/2.==1.5}. But then,
Perl also requires semicolons after every line, and \ci{3/2} is one in
Python, too. Type declarations are one more detail to remember,
but the alternatives have their own warts: Perl basically requires that
you declare the type of your variable (\ci{@, \$, or \#}) with every use,
and R will guess the type you meant to use, but will often guess wrong,
such as thinking that the text string \ci{"14"} is an integer. No system is
wart-free, but C earns its warts by being fast, extensible, and portable.

\index{statistics packages!rants regarding|)}



\comment{
\index{statistics packages!rants regarding|(}
There are so many other languages in which you could do
statistics. Why use C instead of SAS, Stata, SPSS, S-Plus, SST, SHAZAM, GAUSS, GAMS, GRETL,
EViews, PcGive, MatLab, Minitab, Mupad, Maple, Mathematica, WinBUGS, TSP, R, RATS, LIMDEP, Octave, OxMetrics, or Yorick? 
There are more than enough books that advocate for the use of such
languages; 
as one editor at a well-known scientific publishing company
explained to me: ``\dots most of the statistics [book] market is quite heavily
influenced by the software companies that service the
field.''  Thus, this may be the only book to advocate statistical computing
with a general computing language, so I will spend a few pages motivating
the choice as clearly as possible. Over the course of this, you should
also get a better idea of why C is the way it is, and why things that
seem annoying on the surface will pay off in the long run.
\ifbook
\subsection{Reason \#1: C will help you do better science.}
As noted above, it is no longer OK to use OLS for everything.
OLS, with all its assumptions, used to be the only technique that we had the computing
power to actually implement. But my four-year old laptop regularly executes
feats of computation that were entirely impossible fifty years ago, and your
computer can do the same.  {\it So why are
we still using theorems written to facilitate computation?} More importantly, why
are we using them in cases where their assumptions aren't true?

Unfortunately, the statistics packages are written around the
specialized, assumption-heavy theorems, and because people do what the
technology facilitates, people who use stats packages are very
likely to assume OLS is valid. No amount of clicking of the
\airq{regress} button will prepare a user to modify the OLS routine so
even experienced users may not be prepared to deal with minor
digressions from the assumptions.

There is nothing more embarrassing than a presenter who answers a question
about the assumptions or results of a model with `that's just Stata's
default'---or still worse, (and yes, I have heard this in a real live
presentation by a real live researcher) `I would have corrected this
anomaly in my data, but Stata didn't have a function to do that.' This
is beyond unpersuasive and into the realm of confidence-eroding.

Stats packages are not designed around the general results, though
it is technically possible to retrofit these packages to use them. Since
they are Turing complete,\footnote{Alan Turing wrote down an imaginary
machine which could execute a handful of instructions.  All
modern programming languages implement these instructions in one way or
another, and are therefore equivalent to Turing's theoretical computer;
by transitivity, they are all equivalent to each other. With enough
perseverance, you could write a C compiler in MatLab.} you could
write anything in them: maybe a word processor or a painting program. But
why? It is just as easy to write the functions in C using the packages I discuss
here, and the resulting program will be more robust and orders of
magnitude faster, as per Reason \#2.
	\else
[Reason \#1 has been omitted from this excerpt.]
	\fi

\subsection{Reason \#2: Stats packages will break your heart.} Stats packages
are wonderful at first, making it easy to sit down and start working
quickly. As you get better with the language, you will 
grow to depend on the stats package for more and more
things. And then, one day, you will get to a problem that is too far
out from what the language designers had in mind, or a problem that is
too large-scale for the language to handle. Your favorite language just
won't be able to do it, even after you spend hours rewriting and
optimizing.  And after all that, you will have nothing but a broken heart.

The features of C that stats package users will find most annoying are
the ones that make it most well-suited for large-scale work. 
\begin{itemize}
\item Type declarations mean that the computer does not need to guess
the data type you mean at every use, saving a touch of time for every
calculation. For very complex data types,
such as lists of lists, it is often difficult (and for exceptionally
perverse cases, impossible) to get the computer to correctly guess what the
type of a variable should be.
\item Pointers and the call-by-reference mechanism reduce internal copying
during function calls.  As you read \ifbook Chapter \ref{c_crash}\else
this overview\fi, imagine that it ends at Section \ref{prepointers}
and you will have an idea of what most stats packages are like.
\item Different levels of scope (function, file, global) allow modular
coding techniques that are essential
to your sanity when writing programs with more than a few pages of
code. Again, this is something that you will not notice is missing until
you invest heavily in a language and begin writing lengthier analyses.
\end{itemize}

All of this may seem trivial, but it adds up. R and Apophenia use
identical code (in C) for doing the \ind{Fisher exact test}, so it makes
a good basis for comparison. On
my laptop, a C program using Apophenia does 10,000 of these tests in
10.1 seconds; a minimal R script does the same work in nine
minutes---over fifty times as long. I can't count how many times people
have told me similar stories about an analysis that took days or weeks
in a stats package, but ran in minutes after they rewrote it in C.

As you read \ifbook Chapter \ref{c_crash}\else this overview\fi, these
problems are worth bearing in mind. C may seem to have some ugly
features, but they will eventually save you heartbreak.
\index{statistics packages!rants regarding|)}

\subsection{Reason \#3: C is universal} 
The software I discuss in this book is available for the system you
are using, and for the system you will be using five years from now,
for free. There is no other language I could say that about with such
confidence.

Since C was written in 1972, dozens (maybe hundreds) of stats packages
have come and gone. Those who try to follow the trends for some period of
time have learned a half-dozen languages and have on their hard drives
dozens of scripts that won't run anymore.  Meanwhile, correctly written
C programs written in the 1970s will compile cleanly on new PCs.

Further, we increasingly
expect that the data and analysis behind a work be publicly available.
For example, it is a requirement for funding from the U.S. National
Science Foundation. But if your analysis uses a stats package which
isn't universally available, then you will break the hearts of 
your fellow researchers who want to work with your analysis but for whom
it is logistically impossible to do so. Are you sure your colleague in
Madras has an SAS license?


\ifbook
\subsection{Reason \#4: C is on all levels at once} 
Connoisseurs of computing languages often refer to high-level and
low-level languages. The low-level languages are close to the machine,
referring to individual memory addresses and register operations,
giving you perfect control but demanding a lot of work. The high-level
languages do complex things with one command, like {\tt regress(data)}
to produce pages of output, giving you no control but ease-of-use.

On this hierarchy, C is typically billed as a low-level language,
but this is only partly true. C can indeed be used as a rock-bottom
low-level language, with its bit-shifting operators and pointers to
memory addresses.  But on the other hand, there is a library of functions
to do whatever high-level operations you may have in mind, such as the
\cinline{apop\_OLS.estimate(data)} function from the Apophenia library.

You get to pick the balance of laziness and precision that you desire. You
can write code to manipulate individual addresses, use the GNU Scientific
Library to manipulate matrices, or use Apophenia to estimate models---and
you can do it all in the same program.

The how-to code in this book is often repeated at multiple levels. I will first
show you the low-level way of doing things over the course of three
pages, and then tell you the single function call that will do all the work
for you. This is partly because of a didactic philosophy that you should
know what your black boxes are doing before you use them, and partly
to prepare you for when the black box doesn't do what you need. Because all of the
functions described here are open source, you can copy and paste the
functions into your own program and modify them to suit your needs
and data better. Of course, this relies on your being able to understand
the code at lower levels than the function you are tearing apart.
\fi
}
