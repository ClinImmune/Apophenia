\section{Why C?}
\index{statistics packages!rants regarding|(}
You may be surprised to see a book entitled \airq{Modern Statistical
Computing} that is based on a language written in 1972. 
Why use C instead of a specialized language like SAS, Stata, SPSS,
S-Plus, SIENA, SUDAAN, SST, SHAZAM, GAUSS, GAMS, GLIM, GENSTAT, GRETL, EViews,
PcGive, MatLab, Minitab, Mupad, Maple, Mplus, Maxima, Mathematica, WinBUGS, TSP,
HLM, R, RATS, LISREL, LIMDEP, Lisp-Stat, Octave, OxMetrics, Weka, or Yorick?
There are more than enough books that advocate for the use of such
languages; as one editor at a well-known scientific publishing company
explained to me, ``\dots most of the statistics [book] market is quite
heavily influenced by the software companies that service the field.''
This may be the only book to advocate statistical computing with
a general computing language, so I will take some time motivating the
choice as clearly as possible. Over the course of this, you should also
get a better idea of why C is the way it is, and why modern numerical
analysis
is best done in an old language.

\paragraph{Computational speed---really} Using a stats package sure
beats inverting matrices by hand, but as computation goes, many stats
packages are still relatively slow, and that slowness can make otherwise
useful statistical methods infeasible.

R and Apophenia use identical C code for doing the \ind{Fisher
exact test}, so it makes a good basis for comparison.\comment{The
Apophenia code was literally cut and pasted from R's source code. The
only key change was that instances of \ci{R\_malloc} were replaced by
\ci{malloc}. This is not the place for a full exposition on memory
management in packages like R, but as the example shows, its basic design
emphasizes minimal maintenance at the cost of speed.} On my laptop,
a C program using Apophenia does 10,000 of these tests in 10.1 seconds;
a minimal R script does the same work in nine minutes---over fifty times
as long. Nor is this just an isolated test case: I can't count how many
times people have told me stories about an analysis or simulation that
took days or weeks in a stats package but ran in minutes after they
rewrote it in C.

OLS used to be the only technique that we had the computing power to
actually implement. But when you can do a maximum likelihood search
or Monte Carlo simulation in reasonable time, you can write models
that rely on fewer false assumptions. The Monte Carlo examples in Section
\ref{billiondraws} were produced using over a billion draws from $t$
distributions. If your stats package can't produce a few hundred thousand 
draws per second (some can't), such work will be unfeasibly
slow.\footnote{If you can produce random draws from $t$ distributions
as a batch (\ci{draws <- rt(5000000, df)}), then R takes a mere three
times as long as comparable C code. But if you need to produce them
individually (\ci{for (i in 1:5000000) draw <- rt(1, df)}), then R 
takes about ten times as long as comparable C code. On my laptop, R in
batch mode produced draws at the rate of $\approx 122,000/{\rm sec}$,
while C produced draws at $\approx 425,000/{\rm sec}$.}

The features of C that stats package users will find most annoying are
exactly the features that make it orders of magnitude faster than the cuter
alternatives.  Type declarations mean that the computer does not need
to guess the data type you mean at every use, saving time on
every calculation.  Pointers and the call-by-reference mechanism reduce
internal copying during function calls.

Further, C's syntactic unpleasantness pays off immensely when extending
quick-and-dirty scripts to involved, complex analyses.
Not declaring types seems convenient at first, but playing guess-the-type
with a package is often frustrating for complex data types, such
as a list of potentially empty lists. C's multiple levels of scope and 
modular files and functions may be counterintuitive at first,
but those features are essential to your sanity when
writing programs with more than a few pages of code.

\paragraph{Extensibility}
There is nothing more embarrassing than a presenter who answers a question
about an anomaly in the data or analysis with `Stata didn't have a function to
correct that.'\footnote{Yes, I have heard this in a real live presentation
by a real live researcher.} Users need a means of breaking open canned
statistical routines to make sure they fit the situation at hand.

Connoisseurs of computing languages often refer to high-level and
low-level languages. The low-level languages are close to the machine,
referring to individual memory addresses and register operations,
giving you perfect control but demanding a lot of work. The high-level
languages do complex things with one command, like {\tt regress(data)}
to produce pages of output, giving you no control but ease-of-use.

C can be used as a rock-bottom low-level language, but there is also a
library of C functions to execute any high-level operations you may
desire, such as the \cinline{apop\_OLS.estimate(data, ...)} function from
the Apophenia library.  
You get to pick the balance of laziness and precision that you desire. You
can write code to manipulate individual memory locations, use the GNU Scientific
Library to manipulate matrices, or use Apophenia to estimate models---and
you can do it all in the same program.

The how-to code in this book is often repeated at multiple levels. I
will first show you the low-level way of doing things over the course
of three pages, and then tell you the single function call that will do
all the work for you. You can use the prepackaged function calls for
most work, but when the assumptions don't match your data you will be
ready to crack open the black box and modify it to suit.

\paragraph{The lingua franca}
%Here in the modern day, everybody is computer-literate and connected.
Science progresses through the open dissemination of information, data,
and analyses. But with dozens of mutually incompatible stats packages
floating around, the open marketplace of ideas turns into a tower of Babel.
Meanwhile, C is the lingua franca of computing. The software I discuss
in this book is available for the system you are using, and for the
system your colleagues are using, right now, for free.

Further, it is important to be compatible with yourself. Will you be
able to verify and modify your work five years from now?  Chapter
\ref{c_crash} shows you how to build a library of functions so you can
put together next month's analysis in half the time, but the library is
only as useful as the underlying language.  Since C was written in 1972,
dozens (maybe hundreds) of stats packages have come and gone, and those
who try to follow the trends have on their hard drives dozens of scripts
that they can't run anymore.  Meanwhile, correctly written C programs
written in the 1970s will compile and run on new PCs.  \index{statistics
packages!rants regarding|)}



\comment{
\index{statistics packages!rants regarding|(}
There are so many other languages in which you could do
statistics. Why use C instead of SAS, Stata, SPSS, S-Plus, SST, SHAZAM, GAUSS, GAMS, GRETL,
EViews, PcGive, MatLab, Minitab, Mupad, Maple, Mathematica, WinBUGS, TSP, R, RATS, LIMDEP, Octave, OxMetrics, or Yorick? 
There are more than enough books that advocate for the use of such
languages; 
as one editor at a well-known scientific publishing company
explained to me: ``\dots most of the statistics [book] market is quite heavily
influenced by the software companies that service the
field.''  Thus, this may be the only book to advocate statistical computing
with a general computing language, so I will spend a few pages motivating
the choice as clearly as possible. Over the course of this, you should
also get a better idea of why C is the way it is, and why things that
seem annoying on the surface will pay off in the long run.
\ifbook
\subsection{Reason \#1: C will help you do better science.}
As noted above, it is no longer OK to use OLS for everything.
OLS, with all its assumptions, used to be the only technique that we had the computing
power to actually implement. But my four-year old laptop regularly executes
feats of computation that were entirely impossible fifty years ago, and your
computer can do the same.  {\it So why are
we still using theorems written to facilitate computation?} More importantly, why
are we using them in cases where their assumptions aren't true?

Unfortunately, the statistics packages are written around the
specialized, assumption-heavy theorems, and because people do what the
technology facilitates, people who use stats packages are very
likely to assume OLS is valid. No amount of clicking of the
\airq{regress} button will prepare a user to modify the OLS routine so
even experienced users may not be prepared to deal with minor
digressions from the assumptions.

There is nothing more embarrassing than a presenter who answers a question
about the assumptions or results of a model with `that's just Stata's
default'---or still worse, (and yes, I have heard this in a real live
presentation by a real live researcher) `I would have corrected this
anomaly in my data, but Stata didn't have a function to do that.' This
is beyond unpersuasive and into the realm of confidence-eroding.

Stats packages are not designed around the general results, though
it is technically possible to retrofit these packages to use them. Since
they are Turing complete,\footnote{Alan Turing wrote down an imaginary
machine which could execute a handful of instructions.  All
modern programming languages implement these instructions in one way or
another, and are therefore equivalent to Turing's theoretical computer;
by transitivity, they are all equivalent to each other. With enough
perseverance, you could write a C compiler in MatLab.} you could
write anything in them: maybe a word processor or a painting program. But
why? It is just as easy to write the functions in C using the packages I discuss
here, and the resulting program will be more robust and orders of
magnitude faster, as per Reason \#2.
	\else
[Reason \#1 has been omitted from this excerpt.]
	\fi

\subsection{Reason \#2: Stats packages will break your heart.} Stats packages
are wonderful at first, making it easy to sit down and start working
quickly. As you get better with the language, you will 
grow to depend on the stats package for more and more
things. And then, one day, you will get to a problem that is too far
out from what the language designers had in mind, or a problem that is
too large-scale for the language to handle. Your favorite language just
won't be able to do it, even after you spend hours rewriting and
optimizing.  And after all that, you will have nothing but a broken heart.

The features of C that stats package users will find most annoying are
the ones that make it most well-suited for large-scale work. 
\begin{itemize}
\item Type declarations mean that the computer does not need to guess
the data type you mean at every use, saving a touch of time for every
calculation. For very complex data types,
such as lists of lists, it is often difficult (and for exceptionally
perverse cases, impossible) to get the computer to correctly guess what the
type of a variable should be.
\item Pointers and the call-by-reference mechanism reduce internal copying
during function calls.  As you read \ifbook Chapter \ref{c_crash}\else
this overview\fi, imagine that it ends at Section \ref{prepointers}
and you will have an idea of what most stats packages are like.
\item Different levels of scope (function, file, global) allow modular
coding techniques that are essential
to your sanity when writing programs with more than a few pages of
code. Again, this is something that you will not notice is missing until
you invest heavily in a language and begin writing lengthier analyses.
\end{itemize}

All of this may seem trivial, but it adds up. R and Apophenia use
identical code (in C) for doing the \ind{Fisher exact test}, so it makes
a good basis for comparison. On
my laptop, a C program using Apophenia does 10,000 of these tests in
10.1 seconds; a minimal R script does the same work in nine
minutes---over fifty times as long. I can't count how many times people
have told me similar stories about an analysis that took days or weeks
in a stats package, but ran in minutes after they rewrote it in C.

As you read \ifbook Chapter \ref{c_crash}\else this overview\fi, these
problems are worth bearing in mind. C may seem to have some ugly
features, but they will eventually save you heartbreak.
\index{statistics packages!rants regarding|)}

\subsection{Reason \#3: C is universal} 
The software I discuss in this book is available for the system you
are using, and for the system you will be using five years from now,
for free. There is no other language I could say that about with such
confidence.

Since C was written in 1972, dozens (maybe hundreds) of stats packages
have come and gone. Those who try to follow the trends for some period of
time have learned a half-dozen languages and have on their hard drives
dozens of scripts that won't run anymore.  Meanwhile, correctly written
C programs written in the 1970s will compile cleanly on new PCs.

Further, we increasingly
expect that the data and analysis behind a work be publicly available.
For example, it is a requirement for funding from the U.S. National
Science Foundation. But if your analysis uses a stats package which
isn't universally available, then you will break the hearts of 
your fellow researchers who want to work with your analysis but for whom
it is logistically impossible to do so. Are you sure your colleague in
Madras has an SAS license?


\ifbook
\subsection{Reason \#4: C is on all levels at once} 
Connoisseurs of computing languages often refer to high-level and
low-level languages. The low-level languages are close to the machine,
referring to individual memory addresses and register operations,
giving you perfect control but demanding a lot of work. The high-level
languages do complex things with one command, like {\tt regress(data)}
to produce pages of output, giving you no control but ease-of-use.

On this hierarchy, C is typically billed as a low-level language,
but this is only partly true. C can indeed be used as a rock-bottom
low-level language, with its bit-shifting operators and pointers to
memory addresses.  But on the other hand, there is a library of functions
to do whatever high-level operations you may have in mind, such as the
\cinline{apop\_OLS.estimate(data)} function from the Apophenia library.

You get to pick the balance of laziness and precision that you desire. You
can write code to manipulate individual addresses, use the GNU Scientific
Library to manipulate matrices, or use Apophenia to estimate models---and
you can do it all in the same program.

The how-to code in this book is often repeated at multiple levels. I will first
show you the low-level way of doing things over the course of three
pages, and then tell you the single function call that will do all the work
for you. This is partly because of a didactic philosophy that you should
know what your black boxes are doing before you use them, and partly
to prepare you for when the black box doesn't do what you need. Because all of the
functions described here are open source, you can copy and paste the
functions into your own program and modify them to suit your needs
and data better. Of course, this relies on your being able to understand
the code at lower levels than the function you are tearing apart.
\fi
}
