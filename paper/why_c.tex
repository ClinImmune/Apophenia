\section{Why C?}
\index{statistics packages!rants regarding|(}
You may be surprised to see a book entitled \airq{Modern Statistical
Computing} that is based on a language written in 1972. 
Why use C instead of a specialized language like SAS, Stata, SPSS,
S-Plus, SIENA, SUDAAN, SST, SHAZAM, GAUSS, GAMS, GLIM, GENSTAT, GRETL, EViews,
PcGive, MatLab, Minitab, Mupad, Maple, Mplus, Maxima, Mathematica, WinBUGS, TSP,
HLM, R, RATS, LISREL, LIMDEP, Lisp-Stat, Octave, Orange, OxMetrics, Weka, or Yorick?
\comment{There are more than enough books that advocate for the use of such
languages; as one editor at a well-known scientific publishing company
explained to me, ``\dots most of the statistics [book] market is quite
heavily influenced by the software companies that service the field.''}
This may be the only book to advocate statistical computing with
a general computing language, so I will take some time motivating the
choice as clearly as possible. Over the course of this, you should also
get a better idea of why C is the way it is, and why modern numerical
analysis is best done in an old language.

\paragraph{Computational speed---really} Using a stats package sure
beats inverting matrices by hand, but as computation goes, many stats
packages are still relatively slow, and that slowness can make otherwise
useful statistical methods infeasible.

\lstset{numbers=left, numberstyle=\scshape}
\codefig{timefisher}{C code to time a Fisher exact test. It runs the same
test 10,000 times.}

\codefigure{Rtimefisher}{}{R code to do the same test as Listing
\ref{timefisher}.}
\lstset{numbers=none}

R and Apophenia use identical C code for doing the \ind{Fisher
exact test}, so it makes a good basis for comparison.\footnote{That is,
if you download the source code for R's \ci{fisher.test} function,
you will find a set of procedures written in C.  Save for a few minor
modifications, the code underlying the \ci{apop\_test\_fisher\_exact}
function is line-for-line identical.} Listings \ref{timefisher} and
\ref{Rtimefisher} show programs in C and R (respectively) that will run
a Fisher exact test 10,000 times on the same data set.  You can see that
the C program is generally a bit more verbose: the steps taken in lines
3--8 of the C code and lines 1--6 of the R code are identical, but those lines
are longer in C, and the C program has some preliminary and concluding
code that the R script did not have.

On my laptop, Listing \ref{timefisher} runs in
10.1 seconds, while Listing \ref{Rtimefisher} does the same work in nine
minutes---over fifty times as long.\footnote{The online source to which
the caption refers is \samplecodelocation, so if you already have R and
a C compiler set up, you can download the code and try these timing
tests on your own computer.} So the investment of a little more
verbosity and a few extra stars and semicolons returns a fifty-fold
speed gain. Nor is this just an isolated test case: I can't count how
many times people have told me stories about an analysis or simulation
that took days or weeks in a stats package but ran in minutes after they
rewrote it in C.
\comment{The
Apophenia code was literally cut and pasted from R's source code. The
only key change was that instances of \ci{R\_malloc} were replaced by
\ci{malloc}. This is not the place for a full exposition on memory
management in packages like R, but as the example shows, its basic design
emphasizes minimal maintenance at the cost of speed.}

OLS used to be the only technique that we had the computing power to
actually implement. But when you can do a maximum likelihood search
or Monte Carlo simulation in reasonable time, you can write models
that rely on fewer false assumptions. The Monte Carlo examples in Section
\ref{billiondraws} were produced using over a billion draws from $t$
distributions; if your stats package can't produce a few hundred thousand 
draws per second (some can't), such work will be unfeasibly
slow.\footnote{If you can produce random draws from $t$ distributions
as a batch (\ci{draws <- rt(5000000, df)}), then R takes a mere three
times as long as comparable C code. But if you need to produce them
individually (\ci{for (i in 1:5000000) draw <- rt(1, df)}), then R 
takes about ten times as long as comparable C code. On my laptop, R in
batch mode produced draws at the rate of $\approx 122,000/{\rm sec}$,
while C produced draws at $\approx 425,000/{\rm sec}$.}
\comment{
The features of C that stats package users will find most annoying are
exactly the features that make it orders of magnitude faster than the cuter
alternatives.  Type declarations (the \ci{int}, \ci{double}, and
\ci{apop\_data*} on lines 3, 4, and 6 of Listing \ref{timefisher}) mean
that the computer does not need to guess the data type you mean at every
use, saving time on every calculation.  Pointers and the call-by-address
mechanism reduce internal copying during function calls.

Further, C's syntactic unpleasantness pays off immensely when extending
quick-and-dirty scripts to involved, complex analyses.
Not declaring types seems convenient at first, but playing guess-the-type
with a package is often frustrating for complex data types, such
as a list of potentially empty lists. C's multiple levels of scope and 
modular files and functions may be counterintuitive at first,
but those features are essential to your sanity when
writing programs with more than a few pages of code.}

\paragraph{Extensibility}
There is nothing more embarrassing than a presenter who answers a question
about an anomaly in the data or analysis with `Stata didn't have a function to
correct that.'\footnote{Yes, I have heard this in a real live presentation
by a real live researcher.} Users need a means of breaking open canned
statistical routines to make sure they fit the situation at hand.

Connoisseurs of computing languages often refer to high-level and
low-level languages. The low-level languages are close to the machine,
referring to individual memory addresses and register operations,
giving you perfect control but demanding a lot of work. The high-level
languages do complex things with one command, like a {\tt
regress(data)} command to produce pages of output, giving you no control
but ease-of-use.

C can be used anywhere along the spectrum. Listing \ref{regress} gives
an example on the high end, at the level of data sets and statistical
models. It reads in a data set in one line, runs a regression in the
next, and displays a page of results in the next. Other how-to code in
this book will show you how to do many of these steps at a lower level
when necessary.

You pick the balance of laziness and precision that you desire. You
can manipulate individual memory locations, use the GNU Scientific
Library to manipulate matrices, or use Apophenia to estimate models---and
you can do it all in the same program.

\paragraph{The lingua franca}
%Here in the modern day, everybody is computer-literate and connected.
Science progresses through the open dissemination of information, data,
and analyses. But with dozens of mutually incompatible stats packages
floating around, the open marketplace of ideas turns into a tower of Babel.
Meanwhile, C is the lingua franca of computing. The software I discuss
in this book is available for the system you are using, and for the
system your colleagues are using, right now, for free.

Further, it is important to be compatible with yourself. Will you be able
to verify and modify your work five years from now? Since C's debut
in 1972, dozens (maybe hundreds) of stats packages have come and gone,
and those who try to follow the trends have on their hard drives dozens
of scripts that they can't run anymore.  Meanwhile, correctly written
C programs written in the 1970s will compile and run on new PCs.

\paragraph{But C is ugly!} C's longevity has had the odd effect of giving
it a bad reputation. Skimming the bookshelves, you will find texts
filled with minuti\ae{} about computer-oriented memory management and bit
shifting instead of human-oriented data analysis. This is because the
C textbooks from the 1970s, about writing basic software for machines
with comically limited memory, are still correct and still useful to
those who are writing operating systems and statistics packages.

But that is where the \airq{modern} part of this book's title comes
in. With the libraries discussed in this book, you can work on a sensible
level, and ignore the bit-shifting tricks upon which the programmers of
decades past focused.  Listings \ref{regress} and \ref{timefisher} do
not bother with memory management, because they are short programs and
your computer has literally a million times more memory. They include
the declarations that some find to be so onerous, but those consist
simply of preceding the first use of certain variables with type names.
Most importantly, the commands themselves are not about bit-shifting,
but using high-level structures like data sets and statistical models.
Yes, there are thousand-page code bases written in baroque, bit-twiddling
C, like the Windows and Linux operating systems or most of the stats
packages listed above. But these listings show that C code can also be
short, concise, and thoroughly modern.

There is the complaint that C is a compiled language, meaning that
you must write your instructions to a text file and compile the file
to an excecutable before seeing results. \blindvocab{interpreter}Interpreted systems,
like Perl, Python, or most of the packages above, provide a means of
giving immediate feedback for every command. Interpreted languages
certainly have their convenience, and are useful for certain settings,
such as asking a few quick questions of a data set. But an un-replicable
analysis based on clicking an arbitrary sequence of on-screen buttons
is as useful as no analysis at all.  This book focuses on producing
replicable analysis via scripts 
that take the data as far as possible along the pipeline
from raw format to final published output. In this context, writing \ci{program.c} for
a compiler and writing \ci{script.do} for an interpreter become about
equivalent---especially since compilation on a modern computer takes on
the order of 0.0 seconds.

C is by no means the best language for all possible purposes.
Different systems have built-in syntax for communicating with other programs, 
handling text, building Web pages, or producing certain graphics. But
for data analysis, C is very effective. It has its syntactic warts: you
will forget to append semicolons to every line, and will be frustrated
that \ci{3/2==1} while \ci{3/2.==1.5}. But then,
Perl also requires semicolons after every line, and \ci{3/2} is one in
Python, too. Type declarations are one more detail to remember,
but the alternatives have their own warts: Perl basically requires that
you declare the type of your variable (\ci{@, \$, or \#}) with every use,
and R will guess the type you meant to use, but will often guess wrong,
such as thinking that the one-element list \ci{\{14\}} is really just
an integer. No system is wart-free, but C justifies its warts by being fast,
extensible, and portable.

\index{statistics packages!rants regarding|)}



\comment{
\index{statistics packages!rants regarding|(}
There are so many other languages in which you could do
statistics. Why use C instead of SAS, Stata, SPSS, S-Plus, SST, SHAZAM, GAUSS, GAMS, GRETL,
EViews, PcGive, MatLab, Minitab, Mupad, Maple, Mathematica, WinBUGS, TSP, R, RATS, LIMDEP, Octave, OxMetrics, or Yorick? 
There are more than enough books that advocate for the use of such
languages; 
as one editor at a well-known scientific publishing company
explained to me: ``\dots most of the statistics [book] market is quite heavily
influenced by the software companies that service the
field.''  Thus, this may be the only book to advocate statistical computing
with a general computing language, so I will spend a few pages motivating
the choice as clearly as possible. Over the course of this, you should
also get a better idea of why C is the way it is, and why things that
seem annoying on the surface will pay off in the long run.
\ifbook
\subsection{Reason \#1: C will help you do better science.}
As noted above, it is no longer OK to use OLS for everything.
OLS, with all its assumptions, used to be the only technique that we had the computing
power to actually implement. But my four-year old laptop regularly executes
feats of computation that were entirely impossible fifty years ago, and your
computer can do the same.  {\it So why are
we still using theorems written to facilitate computation?} More importantly, why
are we using them in cases where their assumptions aren't true?

Unfortunately, the statistics packages are written around the
specialized, assumption-heavy theorems, and because people do what the
technology facilitates, people who use stats packages are very
likely to assume OLS is valid. No amount of clicking of the
\airq{regress} button will prepare a user to modify the OLS routine so
even experienced users may not be prepared to deal with minor
digressions from the assumptions.

There is nothing more embarrassing than a presenter who answers a question
about the assumptions or results of a model with `that's just Stata's
default'---or still worse, (and yes, I have heard this in a real live
presentation by a real live researcher) `I would have corrected this
anomaly in my data, but Stata didn't have a function to do that.' This
is beyond unpersuasive and into the realm of confidence-eroding.

Stats packages are not designed around the general results, though
it is technically possible to retrofit these packages to use them. Since
they are Turing complete,\footnote{Alan Turing wrote down an imaginary
machine which could execute a handful of instructions.  All
modern programming languages implement these instructions in one way or
another, and are therefore equivalent to Turing's theoretical computer;
by transitivity, they are all equivalent to each other. With enough
perseverance, you could write a C compiler in MatLab.} you could
write anything in them: maybe a word processor or a painting program. But
why? It is just as easy to write the functions in C using the packages I discuss
here, and the resulting program will be more robust and orders of
magnitude faster, as per Reason \#2.
	\else
[Reason \#1 has been omitted from this excerpt.]
	\fi

\subsection{Reason \#2: Stats packages will break your heart.} Stats packages
are wonderful at first, making it easy to sit down and start working
quickly. As you get better with the language, you will 
grow to depend on the stats package for more and more
things. And then, one day, you will get to a problem that is too far
out from what the language designers had in mind, or a problem that is
too large-scale for the language to handle. Your favorite language just
won't be able to do it, even after you spend hours rewriting and
optimizing.  And after all that, you will have nothing but a broken heart.

The features of C that stats package users will find most annoying are
the ones that make it most well-suited for large-scale work. 
\begin{itemize}
\item Type declarations mean that the computer does not need to guess
the data type you mean at every use, saving a touch of time for every
calculation. For very complex data types,
such as lists of lists, it is often difficult (and for exceptionally
perverse cases, impossible) to get the computer to correctly guess what the
type of a variable should be.
\item Pointers and the call-by-address mechanism reduce internal copying
during function calls.  As you read \ifbook Chapter \ref{c_crash}\else
this overview\fi, imagine that it ends at Section \ref{prepointers}
and you will have an idea of what most stats packages are like.
\item Different levels of scope (function, file, global) allow modular
coding techniques that are essential
to your sanity when writing programs with more than a few pages of
code. Again, this is something that you will not notice is missing until
you invest heavily in a language and begin writing lengthier analyses.
\end{itemize}

All of this may seem trivial, but it adds up. R and Apophenia use
identical code (in C) for doing the \ind{Fisher exact test}, so it makes
a good basis for comparison. On
my laptop, a C program using Apophenia does 10,000 of these tests in
10.1 seconds; a minimal R script does the same work in nine
minutes---over fifty times as long. I can't count how many times people
have told me similar stories about an analysis that took days or weeks
in a stats package, but ran in minutes after they rewrote it in C.

As you read \ifbook Chapter \ref{c_crash}\else this overview\fi, these
problems are worth bearing in mind. C may seem to have some ugly
features, but they will eventually save you heartbreak.
\index{statistics packages!rants regarding|)}

\subsection{Reason \#3: C is universal} 
The software I discuss in this book is available for the system you
are using, and for the system you will be using five years from now,
for free. There is no other language I could say that about with such
confidence.

Since C was written in 1972, dozens (maybe hundreds) of stats packages
have come and gone. Those who try to follow the trends for some period of
time have learned a half-dozen languages and have on their hard drives
dozens of scripts that won't run anymore.  Meanwhile, correctly written
C programs written in the 1970s will compile cleanly on new PCs.

Further, we increasingly
expect that the data and analysis behind a work be publicly available.
For example, it is a requirement for funding from the U.S. National
Science Foundation. But if your analysis uses a stats package which
isn't universally available, then you will break the hearts of 
your fellow researchers who want to work with your analysis but for whom
it is logistically impossible to do so. Are you sure your colleague in
Madras has an SAS license?


\ifbook
\subsection{Reason \#4: C is on all levels at once} 
Connoisseurs of computing languages often refer to high-level and
low-level languages. The low-level languages are close to the machine,
referring to individual memory addresses and register operations,
giving you perfect control but demanding a lot of work. The high-level
languages do complex things with one command, like {\tt regress(data)}
to produce pages of output, giving you no control but ease-of-use.

On this hierarchy, C is typically billed as a low-level language,
but this is only partly true. C can indeed be used as a rock-bottom
low-level language, with its bit-shifting operators and pointers to
memory addresses.  But on the other hand, there is a library of functions
to do whatever high-level operations you may have in mind, such as the
\cinline{apop\_OLS.estimate(data)} function from the Apophenia library.

You get to pick the balance of laziness and precision that you desire. You
can write code to manipulate individual addresses, use the GNU Scientific
Library to manipulate matrices, or use Apophenia to estimate models---and
you can do it all in the same program.

The how-to code in this book is often repeated at multiple levels. I will first
show you the low-level way of doing things over the course of three
pages, and then tell you the single function call that will do all the work
for you. This is partly because of a didactic philosophy that you should
know what your black boxes are doing before you use them, and partly
to prepare you for when the black box doesn't do what you need. Because all of the
functions described here are open source, you can copy and paste the
functions into your own program and modify them to suit your needs
and data better. Of course, this relies on your being able to understand
the code at lower levels than the function you are tearing apart.
\fi
}
