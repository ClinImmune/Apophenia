\startonecol \chapter[Gaussian tricks]{Hypothesis testing with the CLT} \label{testing} \label{gauss} \endonecol

This chapter covers most of what we traditionally learn in first-year statistics. 
\comment{
Much of it is obsolete. It covers parameters which we know to have a Normal (aka Gaussian) distribution,
or those things which are derived therefrom. These are nice because the math has already been done, by
people who lived before computers, meaning that we don't need metaphorically heavy machinery to do most of
the calculations here.} Most of the work will consist of taking a dot product, maybe inverting a matrix,
and then looking up a number in a table. 

Everything here depends on the Central Limit Theorem, and more
generally, assumes that your real-world data has a textbook
distribution. Typically, the textbooks state that as the number of draws
$n \to \infty$, the distribution is correct, but whether it is correct
for your data set is a question of \ind{asymptotic theory}.

If your data doesn't fit the CLT, one option is to work out how your data
is distributed, and then write down a likelihood function. If you are
looking to estimate model parameters, do a maximum likelihood estimation;
if you are looking to test a hypothesis, write down a likelihood ratio
based on the distribution you have just calculated.  Another alternative
is to use the Monte Carlo methods of Chapter \ref{boot} to determine
the properties of your model for $n$ significantly smaller than infinity.

This chapter goes over the process of testing an oversolved system. The
key to constructing a test is in knowing how the distribution of the
data informs the likely distribution of our estimates of the $\alpha$s,
and the key to that process is the Central Limit Theorem. The first part
of this chapter will thus return to probability theory, describing how
the CLT affects various parameter distributions, and then the remainder
of the chapter considers how those parameter distributions can be used
to test hypothesized models.

\section{Meet the Gaussian family} \label{dist2}
With the exception of the Normal, the distributions below are distinct
from the distributions of Section \ref{distlist}. The distributions
there
are typically used to describe data that we have observed in the real
world. But having gathered data, we often model the data using techniques
such as linear regressions, that produce means of model parameters,
model variances, and other parameters. Those model parameters have their
own distributions, and those are typically one of the distributions below.


\paragraph{The \ind{Central Limit Theorem}} The CLT is the first piece of magic
that we need. It tells us that:
$$\overline X \xrightarrow{a} \left[\mu,{\sigma^2\over n}\right].$$
In fact, we not only have $\overline X$ in the limit, but its distribution:
As $n\to \infty$, $$\sqrt{n} \frac{\left(\overline X - \mu\right)}{\sigma} \sim {\cal N}(0,1),$$ 
regardless of where $X$ came from. With that regularity of nature,
we can derive all of the following distributions.  \label{CLT}

\subsection{Normal\index{Normal distribution}} 

The Normal distribution, defined on page \pageref{normal},
will also be used to describe some of the parameters derived below.
The big problem with the Normal distribution is that it depends on $\sigma$, an
unknown. It also depends on $\mu$, but if the statistic
being tested is unbiased---like most of the estimators of $\beta$ in the
catalog from Chapter \ref{projections}---then you have $\mu$. Thus, much of the trickery in the
section on test design below involves combining distributions in ways
such that the unknown $\sigma$s cancel out.

\begin{itemize}
\item $\{{\cal N} (a,b)\} + \{{\cal N} (c,d)\} \sim {\cal N}
(a+c,b+d)$ (where $\{x\}$ indicates a variable with the given distribution).
\end{itemize}

\subsection{$\chi^2$ distribution}\index{chi squared distribution@$\chi^2$ distribution} $[\sum_n (x-\overline
x)^2]/\sigma^2\sim \chi^2_{n-1}$. The numerator there is the sample
variance times $n$, so we can test that the sample variance equals a
given $\sigma^2$ with this. But unless we assume $\sigma$ like that,
we are up the same creek as with the Normal.

\begin{itemize}
\item $\{{\cal N} (0,1)\}^2 \sim \chi^2_1$

\item $\{\chi^2_1\}$ + \dots + ($n$ of these) + \dots + $\{\chi^2_1\} \sim \chi^2_n$

\end{itemize}			\label{chisq}

Notice that the sample variance is $\sim
\chi^2_{n-1}$, not $n$, because given the first $n-1$ data points, the
last one can actually be calculated from that data; meaning that we
effectively have the sum of $n-1$ $\{\chi^2_1\}$s plus a no longer
stochastic constant.

\subsection{Student's t distribution\index{t distribution}} Let $\uv$ be
a vector of data (such as the error terms in a regression). Then
${\overline X - \mu \over S/\sqrt{n}}\sim
t_{n-1}$, where $S=\uv'\uv/n$ (a scalar). This is a work of genius by
Mr. Student.\footnote{\airq{Mr. Student} is actually Mr. William Sealy
Gosset, who published the t test in 1908 based on his work as an employee
of the Guinness Brewery. \index{Student} \index{William S Gosset}}
By dividing a normal by $\sqrt{\chi^2_{n-1}\over (n-1)}$, the unknown
$\sigma$s cancel, and we have a function whose elements are all known,
and its distribution.  If $n=2$, we call it a Cauchy distribution.
\label{tstat}


\subsection{$F$ distribution\index{F distribution}}  This is the same cancel-the-$\sigma$s trick as with the $t$, but here the form
is $[\chi^2_m/m]/[\chi^2_n/n]\sim F(m,n)$. For example, if the numerator
is a ${\cal N}(0,1)$, and the denominator is a $\chi^2$ based on the
sample variance, then this is the square of the $t$ statistic.
The $\chi^2$ distribution 
allows simple one-tailed hypothesis tests, but if you want to test a
multivalued hypothesis, then you need to do an $F$ test.

\subsection{Lookup tables}
There are three things that cover most of what you will be doing with a
distribution: finding values of its PDF, values of its CDF, and inverse
values of its CDF.
Here are the functions to look up the value of the PDF at a given point:

\begin{lstlisting}
double gsl_ran_gaussian_pdf (double X, double SIGMA);
double gsl_ran_tdist_pdf (double X, double NU);
double gsl_ran_fdist_pdf (double X, double NU1, double NU2);
double gsl_ran_chisq_pdf (double X, double NU);
\end{lstlisting}

The prefix \cinline{gsl\_ran} indicates that these
functions are from the random number generation module
(\cinline{gsl/gsl\_randist.h}. Random number generation itself will be
delayed to page \pageref{randomnumbers}. Notice that there is no mean
for the Normal, so we may need to modify $X$ accordingly, e.g., if $X$
is drawn from a ${\cal N}(7,1)$, then you will need to ask the GSL for 
\cinline{gsl\_ran\_gaussian\_pdf(X-7, 1)}.


The next distribution calculation found in tables in the back of
statistics texts is calculating the CDF above or below a point. 
The \cinline{P-}functions below calculate the CDF below a point, i.e.
$\int_{-\infty}^X f(y) dy,$
while the \cinline{Q-}functions calculate the CDF above a point, i.e.
$\int^{\infty}_X f(y) dy.$
These will obviously add to one, so you can express any area in terms of whichever function is clearest.

For example, if we find that our Normally distributed mean is 2.2 standard
deviations above zero, then we can reject the one-tailed hypothesis that
the mean is zero with probability \cinline{1 - gsl\_cdf\_gaussian\_Q(2.2, 1) ==
gsl\_cdf\_gaussian\_P(2.2, 1)}.

Here is the list of functions:
\index{Gaussian distribution|see{Normal distribution}}
\index{chi squared distribution@$\chi^2$ distribution!gsl\_cdf\_chisq\_P@\cinline{gsl\_cdf\_chisq\_P}}
\index{t distribution!gsl\_cdf\_tdist\_P@\cinline{gsl\_cdf\_tdist\_P}}
\index{F distribution!gsl\_cdf\_fdist\_P@\cinline{gsl\_cdf\_fdist\_P}}
\index{Normal distribution!gsl\_cdf\_gaussian\_P@\cinline{gsl\_cdf\_gaussian\_P}}
\ttindex{gsl\_cdf\_...}
\begin{lstlisting}
double gsl_cdf_gaussian_P (double X, double SIGMA);
double gsl_cdf_tdist_P (double X, double NU);
double gsl_cdf_fdist_P (double X, double NU1, double NU2);
double gsl_cdf_chisq_P (double X, double NU);
\end{lstlisting}
\dots plus all of these with the \cinline{P} replaced by a \cinline{Q}.


In the other direction, we may want to know where we will need to be to reject a hypothesis with 95\%
certainty. For example, a value-at-risk oriented regulator will want to know what a bank is likely to lose 
one day in the month. That is, what is the value of the 1-in-20, or 5\%, point on the CDF?
Assuming a Normal(M,S) distribution of profit and loss,\footnote{This assumption is false. Securities
typically have \ind{leptokurtic} (fat-tailed) returns; see page \ref{kurt1}.} the bank will report a \ind{value at risk} of {\tt
gsl\_cdf\_gaussian\_Pinv (0.05, S)+M}. Here are the requisite function declarations:
\index{chi squared distribution@$\chi^2$ distribution!gsl\_cdf\_chisq\_Pinv@\cinline{gsl\_cdf\_chisq\_Pinv}}
\index{t distribution!gsl\_cdf\_tdist\_Pinv@\cinline{gsl\_cdf\_tdist\_Pinv}}
\begin{lstlisting}
double gsl_cdf_gaussian_Pinv (double X, double SIGMA);
double gsl_cdf_chisq_Pinv (double P, double NU);
double gsl_cdf_fdist_Pinv (double P, double NU1, double NU2);
double gsl_cdf_tdist_Pinv (double P, double NU);
\end{lstlisting}
\dots plus all of these with the \cinline{P}s replaced by \cinline{Q}s.
The inverse for the CDFs of the F distribution is a new addition as of
April 2006 (version 1.8), and so may not yet be available on prepackaged
distributions of the GSL.





\section{Testing a hypothesis}




We have all the ingredients we need to test a hypothesis; all that
remains is to glue them all together. For example, in the last chapter, the code
found the coefficient vector \cinline{beta} and \cinline{xpx\_inv}, which is the
variance-covariance matrix of \cinline{beta}.  Now assume that \cinline{beta}
is Normally distributed (for example, the original data set consists
of iid draws from a homoskedatic distribution). Then we can use the
above functions to find the probability that each element of \cinline{beta}
is different from zero:

\lstset{texcl=true}
\begin{lstlisting}
//We enter with the vector $\betav$ and matrix {\tt xpx\_inv}
int i;
double confidence[beta->size];
for (i=0;i< beta->size; i++){
    confidence[i] = gsl_cdf_gaussian_Q(fabs(gsl_vector_get(beta,i)), gsl_matrix_get(xpx_inv, i,i));
    confidence[i] -= .5;
    confidence[i] *= 2;
}
\end{lstlisting}
\lstset{texcl=false}
\exercise{
The last two steps to the code above are the poor man's way to convert from the one-tailed area below \cinline{beta}
to a two-tailed area between $\pm$\cinline{beta}.

Write a function with header \ci{double two\_tailify(double
*one\_tail)}, that takes in a one-tailed confidence level and returns a
two-tailed confidence level, assuming a symmetric distribution. 

This is infrequently done, but what would be the correct means of
calculating a two-tailed interval for an asymmetric distribution?
}


\subsection{The t-test} \label{ttest} \index{t test|(textbf}
\index{t test!apop\_t\_test@\cinline{apop\_t\_test}}
\index{t test!apop\_paired\_t\_test@\cinline{apop\_paired\_t\_test}}
Among the most common and simplest questions one asks is: are two 
sets of observations from the same process? Chapter \ref{sql} already showed how one
would do such a test; let us briefly revisit the process now.
Let \cinline{n\_side} be the income of households
drawn from the side of town North of the railroad tracks, and
\cinline{s\_side} be the income of households drawn from the South side of
town. Are North side incomes different from South side incomes? 

If $\mu$,
$\sigma^2$, and $n$ are the estimated mean, variance, and actual count
of elements of each data set,
$${\mu_a + \mu_b \over \sqrt{\sigma^2_a/n_a + \sigma^2_b/n_b}} \sim t_{n_a + n_b -2}.$$
Using the ingredients above, the reader could construct a function to
test the hypothesis that the number above, the $t$-statistic, is
different from zero for any given confidence level. Apophenia provides a
high-level function to do the work for you:

\cindex{apop\_t\_test}
\begin{lstlisting}
gsl_vector *a = gather_data("n_side");
gsl_vector *b = gather_data("s_side");
apop_data  *t = apop_t_test(a, b);
apop_data_show(t);
\end{lstlisting}
As per the discussion on page \pageref{testoutput}, Apophenia's testing
functions return a data table listing test statistics, degrees of
freedom, confidence, et cetera. You can print the entire set as above or
just pull a single element, e.g.,
\cindex{apop\_data\_get\_tn}
\begin{lstlisting}
double pval = apop_data_get_tn(t, "p_val%", -1);
\end{lstlisting}

Now let us say that the data is paired, in the sense that for each
element in the first set, there is an element in the second set that is
related; put another way, this means that for each element $a_i$, there
is a corresponding element $b_i$ such that the difference $a_i - b_i$
makes real-world sense. For example, we could look at student scores on
a test before a set of lessons and the same students after the lessons.
Then, rather than looking at the $t$-distribution for the before data
and comparing it to the $t$-distribution for the after data, we could
look at the vector of differences $a_i - b_i$ and test where zero falls
in the appropriate $t$-distribution. This is generally a more powerful test,
meaning that we are more likely to reject the null hypothesis of no
difference between the two vectors, therefore the paired $t$-test is
generally preferable over the unpaired version above (when it is
applicable). Apophenia provides the \cind{apop\_paired\_t\_test}
function to run this test where appropriate.
 \index{t test|)textbf}

\subsection{The $\chi^2$ test}
\index{chi squared test@$\chi^2$ test}

By itself, this is only useful for testing whether the sample variance
is equal to a given $\sigma^2$, because unless you assume $\sigma^2$ 
or are doing a homework question that says \airq{$\sigma^2$
is known}, you don't have it. Otherwise, in the case of multiple variables, you need to use
an $F$ test; see below.

$H_0$ asks, is ${\Qv'
\betav} = \cv$?  This is a very versatile question. E.g., $\Qv =
\vector{1\cr 0\cr 0}$ plus $\cv = [7]$ gives $H_0: \beta_1 = 7$. Or, $\Qv = \vector{1 \cr
-1}$ and $\cv = \vector{0}$ gives $H_0: \beta_1=\beta_2$. 
Or, say we want to test that $\beta_2 = 2\beta_3$. Then let $\Qv=\vector{0 \cr 1 \cr -2}$ and $\cv = 0$.
To test multiple hypotheses at once, simply stack the above:
\begin{equation}
\begin{matrix}\Qv' = \vector{1 & 0 & 0  \cr
                0 &1 &-2} 
                & \cv = \vector{7 \cr 0}\end{matrix}.\label{qc}\end{equation}

I dare
say that every linear hypothesis having to do with a mean or a $\beta$ can be
fit into this form.\footnote{If our concern is a mean, then set
$\Yv=$your data, $\Xv=$ a column of ones, and $\beta$ will then be
$\mu$. Recall that we typically expect that the first column of $\Xv$ to
be a column of ones anyway.}
Define $q$ to be the number of constraints (columns
of $\Qv$), $N$ the sample size, and $K$ the number of parameters to be
estimated ($\betav$).

Let $\Xuv$ represent $\Xv$ normalized so that each column has mean
zero.\footnote{That is, \cinline{X\_underbar =
apop\_matrix\_normalize(X, 'c', 'm')}.\cindex{apop\_matrix\_normalize}}
Now, if $H_0$ is true and we are using OLS, then $\Qv' \betav \sim {\cal
N}(\cv, \sigma^2 \Qv' (\Xuv'\Xuv)^{-1} \Qv)$,\footnote{For any other
method, the variance is $\Qv'$(the variance from section
\ref{cat})$\Qv$.} 
and we can use all of the above. The $\chi^2$
statistic now becomes:

\begin{equation}		\label{chi1}
{(\Qv'\hat\betav - \cv)' [\Qv' (\Xuv'\Xuv)^{-1} \Qv]^{-1} (\Qv' \hat\betav - \cv)
\over \sigma^2} \sim \chi^2_q
\end{equation}

If we are testing a variance, we'd have 

\begin{equation}		\label{chi2}
{{\bf u}' {\bf u} \over \sigma^2} \sim \chi^2_{N-K}
\end{equation}


\subsection{The $F$ test\index{F test}}\label{ftestsec}

As above, we can divide Equation \ref{chi1} by Equation \ref{chi2}
to give us a statistic with an $F$ distribution and no unknown
$\sigma^2$ element:


\startonecol
\wideeqnbox{(\textwidth - \mwidth)}{
%\begin{equation}	
{N-K\over q}
{(\Qv'\hat\betav - \cv)' [\Qv' (\Xuv'\Xuv)^{-1} \Qv]^{-1} (\Qv' \hat\betav - \cv)
\over \uv' \uv } \sim F_{q,N-K}. \label{ftest}
%\end{equation}	
}
\endonecol

If you have read this far, you know how to code all of the operations
in Equation \ref{ftest}.  But fortunately, 
Apophenia includes a function that will calculate Equation \ref{ftest}
for you.
\index{F test!apop\_F\_test@\cinline{apop\_F\_test}}
To do this, you will need to feed the function a $\betav$ vector, and a
matrix $\Qv'$ plus a vector $\cv$ that indicate the set of hypotheses
you wish to test. Notice that each {\em row} of the input matrix represents a
hypothesis. Also, recall that functions ending in \cinline{calloc} allocate
a matrix or vector and simultaneously set every element to zero, giving
you a blank slate to fill. Thus, if you already have a data set, and
would like to run an OLS regression and then test the joint hypothesis
of Equation \ref{qc}, here's what you'd type:
\begin{lstlisting}
apop_estimate *e = apop_OLS.estimate(data, NULL);
gsl_matrix *Q   = gsl_matrix_calloc(3,2);
gsl_vector *c   = gsl_vector_calloc(2);
gsl_matrix_set(Q, 0, 0, 1);
gsl_matrix_set(Q, 1, 1, 1);
gsl_matrix_set(Q, 1, 2, -2);
gsl_vector_set(Q, 0, 7);
apop_F_test(e, Q, c);
\end{lstlisting}

Here is a useful simplification.
Let $R^2$ be the coefficient of determination (defined further below),
$n$ be the number of data points, $K$ be the number of parameters
(including the parameter for the constant term), and $\phi$ be the
$F$-statistic
based on $\Qv = {\bf I}$ and $\cv = {\bf 0}$. Then it can be shown that 
\begin{equation}
\frac{(n-K) R^2}{K (1-R^2)} = \phi.    \label{rsqandf}
\end{equation}
Since statistical custom is based on the availability of computational
shortcuts, this specific $F$-test often appears in the default output
of many regression packages.  \index{R squared@$R^2$}

\exercise{\label{randfexercise} \index{SSE} \index{SSR}
Verify the identity of Equation \ref{rsqandf} using Equation
\ref{ftest} and these definitions:
\begin{align*}
R^2 &\equiv \frac{SSR}{SSE},  
\intertext{where}
    SSR &\equiv \sum{(Y_{\rm est}-\overline Y)^2},
\intertext{$\Yv_{\rm est}\equiv \Xv\betav$ (the estimated value of $\Yv$), and}
    SSE &\equiv {\uv'\uv}.
    %&= \frac{\sum{(Y-\overline Y)^2}}{\uv'\uv}.
\end{align*}
}

It is up to you
to decide whether the test statistic in Equation \ref{rsqandf} is
relevant for the situation you are dealing with, but because it
is a custom to report it, Apophenia facilitates this hypothesis
test by assuming it as the default when you send in {\tt NULL}
variables, as in \cinline{apop\_F\_test(estimate, NULL, NULL)}. That is,
if $\Qv'=$\cinline{NULL}, Apophenia assumes ${\bf Q}=={\bf I}$ and if
$\cv=$\cinline{NULL}, Apophenia assumes ${\bf c} == {\bf 0}$.

\exercise{Verify the identity of Equation \ref{rsqandf} by running a
linear regression on your favorite data set, and then passing the
\cinline{apop\_estimate} thus produced to \cind{apop\_F\_test} to find
the $F$ statistic and \cind{apop\_coefficient\_of\_determination} to
find the SSE and SSR.}



\comment{
\subsection{The F-test}
But we have the whole variance-covariance matrix to work with, not just
the diagonal, so we can readily test any joint hypothesis that suits
our fancy. First, we need a means of expressing the hypothesis.
}

\comment{
\section{Good ol' OLS}
This section would discuss how to write your very own \cinline{regress}
function, which, as noted above, just consists of solving for the $\beta$
in $(X'X)\beta = X'Y$.  The last chapter showed us the code to find the
betas with the smallest squared error; this section will cover testing
hypotheses about those betas.

\subsection{GLS} Generalized least squares refers to any method that
uses a variance-covariance matrix that isn't the identity matrix. Having
written our \cinline{regress} function, it's almost trivial to generalize
to GLS. But the fun of GSL is in working out what that matrix should be.
This section would give examples of favorites such as AR-1 processes
from time series analysis (and hey, why not AR-$N$?).

The easiest thing to do is simply calculate the variances and covariances of the data itself:

%\begin{verbatim}
\begin{lstlisting}
#include "gsl_convenience_fns.h"
gsl_vector_view one_col, another_col;
gsl_matrix *var_covar=gsl_matrix_alloc(data->size2, data->size2);
int i,j;

for(i=0;i< data->size2; i++){
   for(j=0;j<= i; j++){
      one_col = gsl_matrix_column(data, i);
      another_col = gsl_matrix_column(data, j);
      covar = cov(&one_col.vector, &another_col.vector);
      gsl_matrix_set(var_covar, i, j, covar);
      if (j!=i) gsl_matrix_set(var_covar, j, i, covar);
   }
}
\end{lstlisting}
%\end{verbatim}

Now that we've got this variance-covariance matrix, $\Sigma$, we can
apply the formula $\beta_{GLS} = (X'\Sigma X)^{-1} (X'\Sigma Y)$.
}

\comment{
This gives us  $N\cdot (N+1)/2$ covariances that need to be calculated; your data is probably not up to
the task of providing enough information to allow this to be estimated, in which case you will need to
compromise and impose some number of restrictions. 

For example, FGLS (Feasible GLS) assumes that the var-covar matrix is of the form 
$$\left[\matrix{
1 &\rho & \rho^2 & \rho^3\cr
\rho &1 & \rho  & \rho^2\cr
\rho^2 & \rho & 1 & \rho\cr
\rho^3 & \rho^2  & \rho &1\cr
}\right].$$
This makes life easy because we now only have one variable to estimate.
}

\section{\ind{ANOVA}} ANOVA is a contraction for \vocab{analysis
of variance}, and is a catch-all term for a variety of methods that
aim to decompose a data set's variance into subcategories. Given a few
variables and a few groups, is there more variation between groups or
within groups? Can the variation in a dependent variable be attributed
primarily to some independent variables more than others?

\paragraph{Producing the data set} The typical ANOVA data set is a grid
where each row is a subject or group, and each column is a treatment or
experiment; not all cells need to be filled. In the context of the
database discussion, it is a \vocab{crosstab}, which is in many ways
from the usual data sets.

This section forthcoming.

\section{Goodness of fit}
\index{goodness of fit}

Errors have to be normally distributed, or else
the whole OLS system here does not apply. Many stats package user's manuals
suggest plotting the errors and then squinting at the picture; you can
do this using \cinline{apop\_plot\_histogram}. 
A slightly more rigorous alternative means of testing for Normality
is to check the higher moments of the Normal distribution.  This is the
traditional test, because it is computationally simple; below, a more
general chi-squared goodness-of-fit test is presented.

A Normal distribution has only two parameters, the mean and the
standard deviation, and everything else about the Normal distribution is
defined therefrom. Notably, the third moment is zero, and the fourth
moment is $3 \sigma^4$. 
Un-normalized kurtosis $>3$ is known as \vocab{leptokurtic} and $<3$ is
known as \vocab{platykurtic}; see Figure \ref{tailfig} for a mnemonic.

\begin{figure*}[tb]
%\hspace{-.3in}\scalebox{0.7}{\includegraphics{kurtosis.eps}}
\hspace{-.3in}\includegraphics[width=\textwidth*\real{1.1}]{kurtosis.eps}
\caption{Leptokurtic, \ind{mesokurtic} and \ind{platykurtic}, illustration by Gosset in {\em Biometrika} \citep[p 160]{student:errors}.\index{Student} \index{William S Gosset}\index{leptokurtic}}
\label{tailfig}
\end{figure*}

\marginalia{13}{Kurtosis minus three}{
There is some debate as to whether the kurtosis of a ${\cal N}(0,1)\equiv 3$
or $\equiv 0$. This is simply a definitional issue: some prefer to
normalize all reported kurtoses by subtracting three from the actual
fourth moment. For example, the GSL does the normalization; Apophenia does not.
Bear this in mind when interpreting kurtosis output. If you see a
negative kurtosis, then you are guaranteed that you are getting
normalized kurtosis, since a fourth power can never be negative.
\index{kurtosis}}

We have already written everything we need to calculate all of these easily.
Since the skew and kurtosis are both the mean of an iid process (recall
their definitions on page \pageref{kurtskew}: a sum over $n$), their
square is $\sim \chi^2$. Thus,
$$L_s = n\left[{{\rm skew}^2\over 6}\right]$$
has a $\chi^2$ distribution with one degree of freedom, as does
$$L_k = n\left[{({\rm kurtosis}-3)^2\over 24}\right].$$
Some prefer to test both simultaneously using
$$L_{sk} = n\left[{{\rm skew}^2\over 6} + {({\rm kurtosis}-3)^2\over 24}\right],$$
which has a $\chi^2$ distribution with two degrees of freedom. In code:

\begin{lstlisting}
double  skew    = apop_vector_skew(datavector);
double  kurt    = apop_vector_kurtosis(datavector);
double  statistic = n * (gsl_pow_2(skew)/6. + gsl_pow_2(kurt -3)/24.)
printf("We reject the null that your data is Normal with probability %g.\n", 1 - gsl_cdf_chisq_P(statistic, 2));
\end{lstlisting}

Notice the use of \cind{gsl\_pow\_2}. The GSL provides efficient
power calculators up to \ci{gsl\_pow\_9}, and the catch-all function
\ci{gsl\_pow\_int(value, exponent)}, that will raise
\ci{value} to any integer exponent in a more efficient manner than the
general-purpose \cind{pow}.\cindex{gsl\_pow\_int}

Another alternative, keeping with the theme of this book, would be
to bootstrap the variance of the kurtosis, which would let you find a
confidence interval around $3 \sigma^4$ and state with some percentage
of certainty that the kurtosis is or is not where it should
be.\label{bootkurt}


\subsection{Chi-squared goodness-of-fit test} 
\index{chi-squared goodness-of-fit test} 
Say that we have a histogram that approximates a PDF, and a vector of
points that we claim was drawn from that PDF. It would be nice to test
the confidence with which our claim holds; this is a goodness
of fit test.\footnote{
From the Oxford English Dictionary's definition of \airq{goodness}:
    1b. Of things material or immaterial: Absolute or comparative
    excellence in respect of some specified or implied quality. Now
    somewhat rare.}

Mathematically, it is simple. We have $n$ bins, and two histograms:
\cinline{h0}
holds the approximation of the PDF and \cinline{h1} holds the data. Then 

\begin{equation}    \label{gofstat}
\sum_{i=0}^n {\frac{({\tt h0->bins[i]} - {\tt h1->bins[i]})^2}{{\tt h0->bins[i]}}} \sim \chi^2_{n-1}.
\end{equation}

[Notice: the functions below are subject to change.]

\comment{
Notice how much this chi-squared test looks like the above. Each term is
the actual minus expected squared and scaled, and then the sum of these
elements is still distributed $\chi^2$.
} 
If you have two vectors that are bin-type data, where the first element
is a count of type one observations, the second is a count of type two
observations, et cetera, then you can send them to
\cind{apop\_vectors\_test\_goodness\_of\_fit}, which will return a table
of the $\chi^2$ statistic, $p$-value, and so on.

Or, more commonly, you have a \cinline{gsl\_vector} holding
draws from a distribution, and you have an \cinline{apop\_model}
representing the distribution itself. The common ground is the
\cind{gsl\_histogram} object, which will receive a fuller treatment in
Section \ref{randomnumbers}. For now, it suffices to note that one may
take data from a \cind{gsl\_vector} and put it into bins, and one may make
random draws from an \cind{apop\_model} and put them into bins as well: 
 \cindex{apop\_rng\_init}
\begin{lstlisting}
double normal_params[2]    = {0,1};
gsl_rng         *r  = apop_rng_init(18);
gsl_histogram   *hv = apop_vector_to_histogram(datavector, 1000);
gsl_histogram   *hm = apop_model_to_histogram(hv, apop_normal, 1e7, normal_params, r);
\end{lstlisting}

After \cind{apop\_vector\_to\_histogram} produces a histogram from the
data vector, \cind{apop\_model\_to\_histogram} produces a second histogram
whose bins line up with the bins of the data histogram. It gets the data
from the \cinline{apop\_normal.rng} function, which is why we need to
send the function parameters for the RNG and a \cinline{gsl\_rng}
object. The code above will make ten million draws from the RNG and bin
them into the \cinline{hm} histogram.

With two histograms properly lined up, calculating the $\chi^2$
statistic is easy. You can write a \cinline{for} loop to calculate each
term of Equation \ref{gofstat}, or you can have Apophenia do it for you:
\begin{lstlisting}
apop_data *out = apop_histograms_test_goodness_of_fit(hm, hv);
\end{lstlisting}

If you have two vectors that need to be binned, you can produce 
synced histograms and then test:
\cindex{apop\_vectors\_to\_histogram}
\begin{lstlisting}
gsl_histogram   **h = apop_vectors_to_histogram(datavector, 1000);
apop_data *out = apop_histograms_test_goodness_of_fit(h[0], h[1]);
\end{lstlisting}
In the case of a vector and a model, you can just have
Apophenia produce the histograms and run the test by itself:
\cindex{apop\_model\_test\_goodness\_of\_fit}
\begin{lstlisting}
apop_data *vmtest   = apop_model_test_goodness_of_fit(datavector, apop_normal, 1000, 1e7, normal_params, r);
\end{lstlisting}

\paragraph{\treesymbol Bad data} What happens when the denominator of
one of the terms in the $\chi^2$ statistic is zero?\footnote{I assume
the other histogram has some positive value in the bin; if not, then
we shouldn't use the bin. After all, you could inflate your $\chi^2$
statistic by producing a thousand bins outside of the data's range.} The
statistic becomes $\infty$, so we reject the null (that the data comes
from the statistic) with probability one. This is appropriate: if there is
data in a bin that has probability zero, then the model is clearly false.

One approach is to eliminate the bin. You can do this by putting a
\cinline{GSL\_NAN} in the data bin; the goodness of fit functions will
then throw out that bin. To help with this, you can use the
\cind{apop\_vector\_replace} function:
\begin{lstlisting}
apop_vector_replace(datavector, apop_double_is_zero, GSL_NAN);
\end{lstlisting}
The function takes a vector, a test, and a \cinline{double} value. It
applies the test to every element of the vector, and if the test is
true, that element of the vector is replaced with the given value. Other
popular tests include \cind{gsl\_is\_inf} and its friends from Section
\ref{numbers}.

But the functions won't make the hard decision for you: why is the test
rejecting the hypothesis? It could be a computational reason, that can be solved by  making more random draws to produce the histogram, or using fewer bins.
It could be that there were errors in the  data collection.
But it could be that the data really is doing things that the model
predicts would happen with probability zero, in which case the data's
protests are valid and to be heeded.
