\chapter{Maximum likelihood estimation} \label{mle}
\index{maximum likelihood estimation|ff}
\index{MLE|see{maximum likelihood estimation}}

Maximum likelihood estimators (MLEs) are the bread and butter of
statistics. Most of the techniques discussed so far are MLE
techniques, except mathematicians over the ages have found ways to hide
that fact from you, by proving that under certain conditions the
maximum must have an easy-to-calculate form. But if there is not a nice,
convenient shortcut, you will have to 
search for the optimal parameters yourself. Fortunately, the GSL has the \cinline{gsl\_multimin} family
of objects, to help you find the optimal parameters. On top of those,
Apophenia has an \cind{apop\_maximum\_likelihood} function that
preps and calls the GSL functions for you. You give it a function and
some parameters, and it will find the optimum.

\paragraph{The \ind{log likelihood function}}	\label{the score}
By itself, the PDF is the likelihood that a given value occurs (given
the parameters of the PDF); any of the functions listed in Section
\ref{distlist} will do. That distribution, say $P(x|p)\sim$ Bernoulli,
plus a given value for the parameter $p$, will tell us the likelihood
that we would get the data point that we saw. Or, we could reverse this:
given the data $x$, we can use the Bernoulli distribution to calculate the
likelihood that $p$ has any given value.

Say that we have two data points, $x_1$ and $x_2$. Assume further that
they are independently drawn; that is, $P(\{x_1,x_2\}|p)=P(x_1|p)\cdot P(x_2|p)$. 
The independence assumption allows us to say the the joint probability
is the product of the individual probabilities. The assumption of
identical distributions allows us to write this more neatly, as 
$$P(\{x_1,x_2\}|p)=\Pi_{i=\{1,2\}}P(x_i|p).$$

Define the Log likelihood as $L=\ln f$, the Score\index{score} as

$$S={\partial \ln f\over \partial \theta}$$ 

and the Information variable\index{information variable} as

\begin{eqnarray}
I&=&-{\partial S \over \partial \theta}			\nonumber\\
&=&-{\partial^2 L \over \partial \theta^2}.		\nonumber
\end{eqnarray}

$L$, $S$, and $I$ will appear over and over again, so we may as well get
to know them. First, due to all that exponentiation in the distributions
of Section \ref{distlist}, $L$ is often much easier to deal with, yet
is equivalent to the PDF for most of our purposes---notably, if we have
found a maximum for one, the we have found a maximum for the other.

\marginalia{\ind{Exponential families}}{%
	There is a bit of a sleight of hand in asserting that $S \cdot f = {\partial f
\over \partial \theta}$: the derivative of the expectation is
	${d\over d\theta}\int f dy$; while we want $\int {d f\over
	d\theta} dy=\int S \cdot f dy$ [Note how this means $E(S)=0$, by the
	way, since the integral of the pdf is one, and $d1/d\theta=0$].
	If we can't reverse the integral and derivative like this, none
	of this applies. But we can in the case of any exponential
	family. You can look that up, but here, we can rest assured that
	the Normal, Gamma, Beta, Binomial, Poisson, \&c. are all in this
	family.} 
Further, consider calculating the likelihood function above given
a thousand data points.  Then the probability of observing the data set
we have (fixing $p$ for now) is $\prod_{i=1}^{1000} P(x_i)$.  Since each
$P(x_i)\in (0,1]$, this product is typically on the order of $1\times
10^{-1000}$. This is a delicate number, and details of implementation in
hardware could easily break it; this risks what computer scientists call an
{\sl underflow error}. Taking logs, each value of $p_i$ is now a negative
number, e.g., $\ln(0.5)\approx -0.69$ and $\ln(0.1)\approx -2.3$.  But the
product above is now a sum: $$\ln\left[\prod_{i=1}^{1000} P(x_i)\right]
= \sum_{i=1}^{1000} \ln\left(P(x_i)\right).$$ Thus, the log likelihood of our typical
data set is on the order of -1000 instead of $1\times 10^{-1000}$---much
more robust and manageable.\footnote{Zero probability events are still
a problem: they make the original likelihood product zero, and the log
likelihood $-\infty$. The problem is that if a statistical model
specifies a zero-probability event, and that event happens, then the
model is proven false, and the likelihood function shows it. Near-zero
probability events can have a disproportionately large effect on the
parameter estimation, so care must be taken to evaluate whether events with very
small probability should be taken as signs that the model is false,
should be handled as outliers, or should simply be taken as an unlucky
draw or measurement error.}

The second benefit to using the log likelihood is analytic: since $S \cdot
f = {\partial f \over \partial \theta}$, the derivative of the expectation
in terms of $\theta$ can be rewritten as the expectation of $S$; see box
at left.

There are some fun tricks you can do with these functions.
The trick most worth mentioning is the
\ind{information equality}, that $E(I)=\var(S)$, which you can prove by
calculating ${\partial E(L)\over \partial \theta}$.

\section{Why likelihood functions are great} The process of finding a
maximum of the log likelihood function is 
so useful for two reasons, with four names.

\paragraph{MLEs achieve the Cramer-Rao lower bound} 
        \index{Cramer-Rao Inequality}		\label{cr} 
Say we have an unbiased estimator of $\theta$,
$\hat\theta(y_1,\dots,y_n)$. Cramer \& Rao say:
\begin{eqnarray}
\var(\hat\theta)&\geq&(n \var(S))^{-1}		\nonumber\\
		&=&\left(nE\left({\partial^2 L\over \partial
\theta^2}\right)\right)^{-1}			\label{CRLB}
\end{eqnarray}

\comment{
A sketch of the proof: Take my word for it that $E(\hat\theta
\overline S)={1\over n}$. Since $\overline S=0$, that's equivalent to
$\cov(\hat\theta, \overline S)={1\over n}$.  Plug that into Cauchy-Schwarz:
$\var(\hat\theta)\var(\overline S)\geq {1\over n^2}$. Plug
$\var(S)=n\var(\overline S)$ in to that, and the first line of the above
drops out\cite{goldberger}. The second line uses the information
equality from above.
	}
Just remember: the Cramer-Rao lower bound is the inverse of $n$ times
the expectation of the derivative of the derivative of the log of the
likelihood function.

The Cramer-Rao lower bound is
called a `lower bound' because Mr.s Cramer and Rao proved that any
estimator of $\beta$ must have a variance greater than or equal to the
CRLB. Your favorite probability textbook (e.g., \cite{casella:berger})
will also prove that for the MLE, the variance is actually equal to the
CRLB, meaning that we can not get an estimator of $\beta$ that will
have a smaller variance.

\subsection{How to evaluate a test}
\marginalia{Evaluation vocab}{
Here are some vocabluary terms; if you are in a stats class right now,
you will be tested on this:

Likelihood of {\bf Type I errors} $\equiv\alpha$: rejecting the null when it is true.\index{Type I errors}

Likelihood of {\bf Type II errors} $\equiv\beta$: accepting the null when
it is false.\index{Type II errors}

{\bf Power}$\equiv 1-\beta$, or the likelihood of rejecting a false null.\index{power}

{\bf Unbiased}$\equiv (1-\beta)\geq \alpha\ \forall$ values of the parameter.
I.e., you are less likely to accept the null when it is false than when
it is true.\index{unbiased estimator}

{\bf Consistency}$\equiv$ the power $\to 1$ as $n\to \infty$.\index{consistency}
}

A test will either reject or accept a hypothesis given data. It can be
fooled two ways: the hypothesis could be true but the test rejects it;
or the hypothesis could be false but the test accepts it anyway. There
is a balance to be struck between the two errors: as one rises, the
other falls. But not all estimators are born equal: the coin-flip test, `heads, accept;
tails, reject' gives us a 50\% chance of a Type I error
and a 50\% chance of a Type II error, but there are tests where both
errors are significantly lower than 50\%, and so by any measure we would
call those better tests than the coin-flipping test. 


\subsubsection{Keeping Type I and Type II straight} 
Here is an oft-told story: if a doctor is deciding
whether to give a vaccination, and $H_0=$ the patient is sick, then
rejecting $H_0$ when it is true and thus not vaccinating is much worse
than accepting $H_0$ when it is true and thus vaccinating extraneously,
so the important error is Type I. I always thought this was totally
{\it ad hoc}, especially since $H_0=$ the patient is not sick is more
null. But if the story helps you, run with it.

You are probably familiar with the
standard hypothesis test, which says things like `we reject the null with 90\%
likelihood', and is what the charts in the back of the textbook are about.
That's the Type I error. It comes first because
it's generally all anybody talks about.

\paragraph{The Neyman-Pearson lemma} 
We focus on Type I errors; it is what we mean when we say that our
test has $\alpha=95\%$ confidence. Byt what about Type II errors? 
Neyman and Pearson showed that a likelihood ratio test will have the
minimum possible Type II error of any test with the $\alpha$ that we
selected. After establishing this fact, we just ignore Type II errors.

\subsection{Likelihood ratios} \index{likelihood ratio}

Say the cost to a Type I error is $C_I$ and the cost to a Type II error
is $C_{II}$.  Then you will reject $H_0$ iff the expected cost is less
than the likely cost of not rejecting. That is, reject $H_0$ iff $C_I
P(H_0|\xv)<C_{II} P(H_1|\xv)$.

We need to note two things from here. First, the difference between a
`probability', which is the probability of an event given data; and a
`likelihood', which is the probability that we'd have the data we have
given some event. This is nitpicking that you don't really have to care
about, except that here and there somebody will call you on
it.\footnote{One could write $P(A|B)$ as a simple two-variable function:
$P(A,B)$, with the understanding that $\int_{\forall A} P(a,B)da = 1,
\forall B$.} This may
also clarify for you why some things are called one and some the other.

Second, ?`remember the definition of a \ind{conditional probability}? It's based on the
statement $P(A\cap B)=P(A|B)P(B)$, and symmetrically, $P(A\cap B)=P(B|A)P(A)$.  Equating
the two and shunting over $P(B)$ gives us \ind{Bayes's rule}: $$P(A|B)={P(B|A) P(A)\over P(B)}$$
Applying Bayes's rule to the rejection test is easy: set $A=H_0$, $B=\xv$,
$P(A|B)=P(H_0|\xv)$, and $P(B|A)=L(\xv|H_0)$. Then:
\begin{eqnarray}
C_I P(H_0|\xv)&<&C_{II} P(H_1|\xv)\label{firstnp}\\
\nonumber\\
C_I {L(\xv|H_0)P(H_0)\over P(\xv)}&<& C_{II} {L(\xv|H_1)P(H_1)\over P(\xv)}\label{secondnp}\\
\nonumber\\
c&<& {L(\xv|H_1)\over L(\xv|H_0)}\label{thirdnp}
\end{eqnarray}

Inequality \ref{firstnp} is the rejection rule from above; Inequality
\ref{secondnp} uses Bayes's rule to insert the likelihood functions;
Inequality \ref{thirdnp} just does some cross-division, cancelling out the
$P(\xv)$s, and defining $c\equiv C_IP(H_1)/C_{II}P(H_0)$, i.e., everything
that doesn't depend on $\xv$. If you tell me the shape of the distribution
and some number $\alpha\in(0,1)$, then I can give you a value of $c$
such that Inequality \ref{thirdnp} is true with probability
$\alpha$. The test will then be: fail to reject $H_0$ iff Inequality
\ref{thirdnp} is true.

Neyman and Pearson\index{Neyman-Pearson lemma} proved\footnote{See
e.g. \cite[189--191]{amemiya:ez}, from whom I cribbed
this section.}  that this test is the `best' in the sense that for a
fixed Type I error (fixed at $\alpha$), the probability of a Type II
error is minimized. So we can design any test we like by just fixing $\alpha$
at a value with which we are comfortable (custom says to use
90, 95 or 99\%), calculate a few likelihood functions, and we are assured
that we did the best we could regarding Type II errors. You will see that
many of the tests which follow will be of the likelihood ratio form,
and so Type II errors pretty much never get mentioned, since they're
considered taken care of.




\subsection{An example: probit} \index{probit|(}
The probit is a popular model of aleatory behavior. 
Let the utility from vector $\xv$ be 
$U(\xv|\betav) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_a + \epsilon$,
where the $\beta$s are all parameters to be estimated from
the data and $\epsilon \sim {\cal N}(0,1)$.
The a person consumes $\xv$ iff $U(\xv|\betav)>0$. 

An equivalent way to put the model is to say that $U(\xv)$ is
deterministic---drop $\epsilon$ from the above, and the probability that
the agent consumes is given by
$$P(\xv|\betav) = \int_{-\infty}^{U\left(\xv|\betav\right)} {\cal N}(0,1)(y) dy,$$
where ${\cal N}(0,1)(y)$ indicates the Normal PDF at $y$ (and so the
integral is the CDF up to $U(\xv|\betav)$).

Reversing this, the probability of $\betav$ given that the agent
consumed is $P(\xv)$, and its probability given that the agent did not
consume is $1-P(\xv)$. Let $\xv^C$ be the set of $\xv$s that led to
consumption and $\xv^N$ be the set that led to nonconsumption. Then the
likelihood of $\betav$ given the data set is 
$$L(\betav|\xv) = \prod_{i} P(\xv^C_i|\betav) \cdot \prod_{i} 1-P(\xv^N_i|\betav).$$ 

One row of our data set will consist of a one or zero indicating
consumption or non-consumption, and then (the transpose of) the vector
$\xv$. Then the probit parameters can be estimated using
\cinline{apop\_probit.est\-i\-mate(data, NULL, NULL)}.
\index{probit|)}

\section{Description: Maximum likelihood estimators} 
As in the Probit example, Apophenia's model estimation routines can run
with default values by sending in \cinline{NULL} as the second
argument. However, if you know that the model estimates its parameters
via MLE, then you may be able to improve the estimation routine by
specifying the method and scale of the ML search.

Specifying these parameters simply requires 
filling out a form describing your preferences and then calling the model's \cinline{estimate} method.
The form in question is a \cind{apop\_estimation\_params} structure, that
describes the method, the starting point, the step size, and the
tolerance.
For example:
\begin{lstlisting}
apop_estimation_params *ep  = malloc(sizeof(apop_estimation_params));
double  starting_point[2]   = {1,1};
ep->method      = 000;
ep->step_size   = 1e-1;
ep->tolerance   = 1e-5;
ep->verbose     = 1;
ep->starting_pt = starting_point;
apop_gamma.estimate(my_data, ep)
\end{lstlisting}

Here are the parts to the  \cind{apop\_estimation\_params}
structure you need to set. 

\paragraph{The method} There are currently four you can choose from.
[Also, there are two more that  just aren't hooked in yet. If you are
comfortable exploring the internals of Apophenia, have a look at the
\cinline{wn/wnbridge.c} file.]


\begin{itemize}
\item {\bf 000: The Nelder-Mead simplex algorithm} Draws a polygon and then attempts to shift the corners of the polygon.
\item {\bf 100: Fletcher-Reeves Conjugate gradient} This is the
default. Basically, we take the derivative in all directions and find
the linear combination of these directions that indicates the fastest
climb. Clearly, this depends on knowing derivatives; see below.
\item {\bf 200: BFGS: Broyden-Fletcher-Goldfarb-Shanno Conjugate gradient}  Another conjugate gradient method, that calculates the direction of travel differently. See \cite{avriel:nonlinear} for definitions of all of these methods.
\item {\bf 300: Polak-Ribiere Conjugate gradient} Yet another conjugate gradient method.
\end{itemize}

As a practical matter, your best bet is to either just go with the default (Fletcher-Reeves) or try them all using a tournament as described below. 
If you expect the derivatives to be perverse, then you may want to try the derivative-free simplex algorithm.

There is also the question of how the gradients are calculated. If
analytic gradients are available, then those are probably preferable, but
Apophenia will automatically calculate numerical gradients as necessary.
Since the variance of an MLE is the Cramer-Rao lower bound (Equation
\ref{CRLB}), we will use
the gradients to calculate the variance, so even when using a
derivative-free simplex algorithm, the variance will still be using
gradients, somehow derived.

Just in case the analytic gradients don't work out for you, there is a
means of just using the numeric gradients. This is primarily useful for debugging:

\begin{itemize}
\item 0: If (and only if) no gradient is available, use numerical approximations.  (default)
\item 1: Use numerical approximations even if an explicit dlog likelihood is given. 
\end{itemize}

The actual number to input is the sum of the method type above plus the
gradient handling method. Thus the default is equal to 100, for example, and the
Nelder-Mead simplex algorithm forcing numerical derivatives would be
001.\footnote{Yes, 000 reduces to 0 and 001 reduces to 1. The internals
don't care, so use whichever form makes the most sense to you as a human.}


\paragraph{The step size} This is the initial distance that the
algorithm will use to start searching for points. Basically arbitrary
because all of these algorithms are adaptive, meaning that they will
change the step size as they see fit.

\paragraph{Tolerance} The algorithm will stop when it feels
that the calculated log likelihood is changing by less than this amount,
indicating that the search is at a local maximum and the first derivative of
the log likelihood function is nearing zero.

\paragraph{Verbose} Should the search output the points it is evaluating?
This is useful for long evaluations to reassure you that nothing is
broken; for example, some of the GSL's searches can get trapped in two-step loops.
It is probably a good idea to run your search verbosely at least once.
Also, see \cinline{apop\_opts.mle\_trace\_path} below.

\paragraph{Starting point} An array of doubles indicating where you want
your search to begin. Being an array, you can use a form like the above
to initialize it (but if you are using automatic allocation like this, then it will be destroyed when you leave the
function that it was initialized in---remember this if you want to
access the starting point in the \cind{apop\_estimate} that the MLE outputs).


\subsection{Restarting} \cindex{apop\_estimate}
\label{restart} One trick for getting a better estimate is is to solve
for an MLE using a coarse scale and then restarting the procedure at
the point where the first MLE found its solution using a finer scale.
To do this, use \cind{apop\_estimate\_restart}.

The function takes three arguments: a prior estimate that you have
already calculated, a method, and a scaling factor. 

First, the function copies off the \cinline{apop\_estimation\_params}
from the \cinline{apop\_estimate} sent in, and then makes a series of
modifications.  If the new method sent in is -1, then the new MLE will
use the same method as before; otherwise specify one of the methods
listed above. The step size and the tolerance are both multiplied by the
scaling factor sent in (so if this is 1, no change is made). If the
estimated parameters for the original estimate is finite and bounded, then 
the starting point in the new \cinline{apop\_estimation\_params}
structure is set to the estimated value; if some element of the
estimated parameters are infinity or NaN, then the original starting
point is reused.

Then, the ML estimation is run again, and we now have two estimates: the
original and the new one. If the new one has not-finite or NaN values,
then it is thrown out and the original returned. If the new estimate 
has a higher log likelihood, then it is returned, and the old estimate
is deleted.  The less likely estimate (which may be the one you sent in) is freed. That is,
there is no memory leak when you run a tournament like:
\begin{lstlisting}
est = apop_estimate_restart(est, 200, 1);
est = apop_estimate_restart(est, 100, 1e-2);
est = apop_estimate_restart(est, -1, 1e-2);
\end{lstlisting}
The estimate can only get better every time you run
\cind{apop\_estimate\_restart}, so the only cost to running a lengthy
tournament is your time waiting for the estimates to converge.

\subsection{Graphing the path} All of the methods above consist of
trying a series of candidate points. 
If \index{apop\_opts!mle\_trace\_path@\cinline{mle\_trace\_path}}\cinline{apop\_opts.mle\_trace\_path}
has a name of positive
length, then every time the MLE evaluates the function, then the value
will be output to a table in the database with the given name. You can
then plot this table to get an idea of the path the estimation routine
used to arrive at its MLE.

First, set this variable and run the MLE. The begin/commit wrapper
speeds things up a touch, but this will clearly be slower than without
taking notes:
\begin{lstlisting}
    strcpy(apop_opts.mle_trace_path, "pathtable");
    apop_query("begin;");
    e   = apop_zipf.estimate(...);
    apop_query("commit;");
\end{lstlisting}
Plotting data is covered fully in Chapter \ref{gnuplot}, but for now,
the function below shows how one would plot the trace.  Notice that you will want
\cinline{splot} for 3-d variables and \cinline{plot} for 2-d. The change in width
and pointsize is to remind the eye that the lines connecting the points
only indicate the path the maximizer went along, not actual values of
the function.
\begin{lstlisting}
static void plotme(char *outfile){
FILE            *f;
apop_data       *traced_path;
    f       = fopen(outfile, "w");  //overwrites. Use "a" to append.
    fprintf(f,"splot '-' with linespoints linewidth 0.5 pointsize 2\n");
    traced_path = apop_query_to_data("select * from %s"
                                        , apop_opts.mle_trace_path);
    fclose(f);
    apop_data_print(traced_path, "\t", outfile);
}
\end{lstlisting}

Finally, call \ind{Gnuplot} from your command line:
\begin{lstlisting}
gnuplot -persist < plotme
 (or)
gnuplot plotme -
\end{lstlisting}

\begin{figure}
\scalebox{0.4}{\includegraphics{search}}
\caption{A search for an optimum using a conjugate gradient method.}
\end{figure}

\section{Hypothesis testing: Likelihood ratio tests} 
If the equations derived from maximum likelihood models listed in
Chapter \ref{gauss} do not apply to your data,
then you will need to write down the likelihood
function yourself.  Fortunately, we have a computer to do the tedious
math, unlike poor Mr. Gauss, who had no such conveniences.  Also, we
have the Cramer-Rao lower bound to tell us what the variances are.



\section{Writing your own}
The design of the \cinline{apop\_model} hopes to make it as easy as
possible for you, dear reader, to write new models. For the most
part, all you need to do is write a log likelihood function, and
\cind{apop\_maximum\_likelihood} does the rest. The steps:


\begin{itemize}
\item Write a likelihood function. Its header will look like this:
\begin{lstlisting}
double apop_new_log_likelihood(const gsl_vector *beta, void *d)
\end{lstlisting}
where \cinline{beta} will be the parameters to be maximized, and \cinline{d} is the fixed parameters---the data. In every case currently included
with Apophenia, \cinline{d} is a \cinline{gsl\_matrix,} but you do not have to conform
to that. This function will return the value of the log likelihood function at the given parameters.

\item Is this a constrained optimization? See Section
\ref{constraintwriting} on how to write a constraint function.

\item Write the \cinline{estimate} method, so users can call 
\cinline{apop\_new\_like\-li\-hood.\-est\-i\-mate(...)}. For an MLE, this will be one line,
as follows:
\begin{lstlisting}
static apop_estimate * new_estimate(apop_data * data, void *parameters){
return apop_maximum_likelihood(data, apop_new_log_likelihood, parameters);
}
\end{lstlisting}
By using the \cind{static} keyword, the function name will only be
known to this file, so you can name it what you wish.  But since you
will be putting a pointer to the function into an object below, you
can use the above-mentioned 
\cinline{apop\_new\_like\-li\-hood.\-est\-i\-mate(...)}
form to call this function. 


\item Write the object, a process which will consist of filling out
another form. In your header file, include 
\begin{lstlisting}
apop_model apop_new_likelihood = {"The Me distribution", 
            number_of_parameters, 
{       //what will apop_new_likelihood.estimate return?
        1,      //parameters 
        1,      //covariance
        1,      //confidence
        0,      //dependent var
        0,      //predicted and residual
        1,      //log_likelihood
        1       //names;
},          new_estimate,
            new_log_likelihood, 
            NULL,   //place dlog likelihood here.
            NULL,   //place constraint fn here.
            NULL    //place RNG here.
            };
\end{lstlisting}
If there are constraints, then replace the appropriate \cinline{NULL} with the right constraint function; see below.
\cinline{number\_of\_parameters} is probably a positive integer like \cinline{2}, but
it is often (the number of columns in your data set -1), in which case,
set \cinline{number\_of\_parameters} to \cinline{-1}.

\item Test. Debug. Retest.

\item Optional: write a gradient for the log likelihood function. This
typically involves calculating a derivative by hand, which is an easy
problem in high-school calculus. The function's header will look like: 
\begin{lstlisting}
void apop_new_dlog_likelihood(const gsl_vector *beta, void *d, gsl_vector *gradient)
\end{lstlisting}
where \cinline{beta} and \cinline{d} are fixed as above, and \cinline{gradient} is a \cinline{gsl\_vector} with dimension matching \cinline{beta}. 
At the end of this function, you will have to assign the appropriate derivative to every element of the gradient vector:
\begin{lstlisting}
gsl_vector_set(gradient, 0, d_a);
gsl_vector_set(gradient, 1, d_b);
\end{lstlisting}
Now add the resulting dlog likelihood function to your object, by
replacing the \cinline{NULL} labeled ``place dlog likelihood here'' with
the name of your dlog likelihood function.  
\item Send the code to Apophenia's maintainer for inclusion in future
versions.  \end{itemize}


\subsection{Setting
constraints}\index{optimization!constrained}\label{constraintwriting}

The problem to be surmounted is that the parameters of a function must not take on
certain values, either because the function is undefined for those
values or because parameters with certain values would not fit the
real-world problem.

The solution taken by Apophenia is to rewrite the function being maximized such that the
function is continuous at the constraint boundary but takes a steep
downward slope. The unconstrained maximization routines will then have a 
continuous function to search but will never find an optimum 
beyond the parameter limits.

If you give it a likelihood function with no regard to constraints plus
a constraint function, \cind{apop\_maximum\_likelihood} will combine
them to a function that fits the above description and search accordingly.

This is similar to the common penalty function methods of turning an
constrained problem into an unconstrained one, as in \cite{avriel:nonlinear},
with a few differences. Primarily, we don't know if the constraint is
because the author of the system declared an arbitrary cutoff (`we can't spend more
than \$1,000.') or if evaluating the likelihood function fails
($\ln(-1)$). 

A constraint function must do three things:
\begin{itemize}
\item It must check the constraint, and if the constraint does not bind (i.e., the parameter values are OK), then it must return zero.
\item If the constraint does bind, it must return a penalty, that indicates how far off the parameter is from meeting the constraint.
\item if the constraint does bind, it must set a return vector that the likelihood function can take as a valid input. The penalty at this returned value must be zero.
\end{itemize}

The idea is that if the constraint returns zero, the log likelihood
function will return the log likelihood as usual, and if not, it will
return the log likelihood at the constraint's return vector minus the
penalty. To give a concrete example, here is a constraint function that
will ensure that both parameters of a two-dimensional input are both
greater than zero:

\begin{lstlisting}
static double beta_zero_and_one_greater_than_x_constraint(gsl_vector *beta, void * d, gsl_vector *returned_beta){
double          limit0          = 0,
                limit1          = 0,
                tolerance       = 1e-3; // or try GSL_EPSILON_DOUBLE
double          beta0   = gsl_vector_get(beta, 0),
                beta1   = gsl_vector_get(beta, 1);
        if (beta0 > limit0 && beta1 > limit1)
                return 0;
        //else create a valid return vector and return a penalty.
        gsl_vector_set(returned_beta, 0, GSL_MAX(limit0 + tolerance, beta0)); 
        gsl_vector_set(returned_beta, 1, GSL_MAX(limit1 + tolerance, beta1));
        return GSL_MAX(limit0 + tolerance - beta0, 0) 
                        + GSL_MAX(limit1 + tolerance - beta1, 0); 
}
\end{lstlisting}

Observe how it manages all three of the above steps. First, it checks
the constraints and quickly returns zero if none of them bind. Then, if
they do bind, it sets the return vector to just inside the constrained
region. Finally, it returns the distance (on the Manhattan metric)
between the input point and the point returned. The hope is that the
evaluation system will repeatedly try points closer and closer to the
zero-penalty point, and the penalty will continuously decline as we
approach that point.

For another example, have a look at the budget constraint in the code
listing at the end of this chapter.

\section{Non-stats modeling}    \label{econ101}

The MLE functions of Apophenia are designed to maximize a function
subject to constraints---which sounds a lot like any of a variety of
other problems, especially in Economics. With little abuse of the package,
one could use it to solve non-statistical models involving maximization
subject to constraints. For example, the code listing at the end of this
chapter shows how one could numerically solve a utility maximization
problem from Econ 101. 

It requires a few digressions from the MLE nomenclature:
\begin{itemize}
\item The log likelihood is actually the utility function. Don't take logs,
since that would mess up the marginal utilities.

\item The data is the fixed input to the MLE, which means the parameters:
prices, budget info, utility parameters.

\item The betas in the MLE framework are the free parameters to be
maximized, which in this case means the goods the consumer is choosing.
\end{itemize}

Utility is $U = x_1^\alpha x_2^\beta$. 
The budget constraint dictates that $P_1 x_1 + P_2 x_2 <= B$.
                                                             
The data vector looks like this:\\
0:  price$_0$\\
1:  price$_1$\\
2:  budget \\
3:  $\alpha$  \\
4:  $\beta$   
                                                             
Most of the work is in writing down all the constraints, since the
function itself is trivial. Having written down the model, the estimation
is one function call, and calculating the marginal values is one more.
Overall, the program is overkill for a problem that can be solved via
two derivatives, but the same framework can be used for problems with no
analytic solutions (to give one example, if the consumer has a stochastic
utility function).

Because the estimation finds the slopes at the optimum, it gives us
comparative statics. Thus, even though the optimization itself may be
difficult to explain, the effects that changes in inputs will have on
the output optimum is clear.

\lstinputlisting{sources/econ101.c}
