\chapter{Databases} \label{sql}

If you've been reading sequentially, you've already got all the techniques you'll need to do statistics.
This chapter is basically a convenience, although as conveniences go, it's a great one. 

One especially nice thing about keeping your data in an SQL database is
that it'll give your data names again. Here's some valid SQL: {\tt select age, gender, year from
survey}. Why, that's proper English. It goes downhill from there in terms of properness, but at its worst,
it's still pretty easy to look at a query and know what's actually going on.

\section{Basic queries}
This section discusses some of the methods of data slicing and joining
which are easy using database queries, but which are difficult via
matrix operations.

\subsection{subsets} Pulling a subset of your data is easy in SQL. If
you need all of the data for those subjects over 175cm, you can ask for
it thusly:

\begin{verbatim}
select * 
   from data 
   where height >=175;
\end{verbatim}

Almost every query will have this form, so it is worth analyzing this
simple example closely.

\paragraph{Select}
The {\tt select} clause will specify the columns of the table which will
be output. The easiest list is {\tt *}, which means `all the columns'.

Other options:

Explicitly list the columns: {\tt select age, height, weight}

Explicitly mention the table you are pulling from: {\tt select data.age,
data.height, data.weight}. This is unnecessary here, but will become
essential when dealing with multiple tables below.

Rename the output columns: {\tt select data.age as age, height as
height_in_cm}. Notice that if you do not rename {\tt data.age} as {\tt
age}, then you will need to use {\tt data$\backslash$.age}, which is a
bit annoying.

Generate your own new columns: {\tt select height, weight, (height/weight)
as hwr}. The {\tt as hwr} subclause is again more-or-less
essential if you hope to easily refer to this column in the future. Of
course, if you are dumping the query output to a gsl\_matrix, the column
names will be lost anyway.

\paragraph{From} The {\tt from} clause specifies the tables from which
you will be pulling data. The simplest case is a single table: {\tt from
data_tab}, but you can specify as many tables as necessary: {\tt from
data_tab1, data_tab2}. 

You can alias the tables, for easier reference. The clause {\tt from
data_tab1 d1, data_tab2 d2} gives short names to both tables, which can
be used for lines like {\tt select d1.age, d2.height}. 

Aliasing is generally optional but very convenient, but one case which
will repeatedly appear below is when you are joining a table to itself.
For now, simply note the syntax: {\tt from data t1, data t2} will let
you refer to the {\tt data} table as if it were two entirely independent
tables. 

\paragraph{Where}
The {\sl where} clause is your chance to pick out only those rows which
interest you. With no {\sl where} clause, the query will return one line
for every line in your original table (and the columns returned will
match those you specified in the {\sl select} clause).

Join as many subclauses as you wish, using the Boolean operators you
know and love: {\sl where ((age > 13) or (height >= 175)) and (weight == 70)}.

\subsection{Folding queries into C code} 

\paragraph{apop\_query\_db} The simplest function is {\sl
apop_query_db}, which takes a single text argument: the query. For
example,

\begin{verbatim}
#include <apophenia/db.h>
apop_query_db(" select *        \
                  from data     \
                  where height >=175;");
\end{verbatim}
A string is easiest for you as a human to read if it is
broken up over several lines; to do this, end every line with a
backslash, until you reach the end of the string. The line above runs
the query and returns nothing. [Not very useful for a {\tt select}; but see
{\tt create table} and  {\tt insert} below.]

\paragraph{sprintf} This is a standard C function which is essential for
the production of queries. 
\begin{verbatim}
#include <string.h>>
#include <apophenia/db.h>
char q[10000];
sprintf(q, " select *        \
               from data     \
               where height >=%i;", min_height);
apop_query_db(q);
\end{verbatim}
The {\tt sprintf} function is just like {\tt printf}, except it begins
with a pre-declared string (which must be large enough to hold the
string; I suggest just declaring an obnoxiously large string as above).
It does the usual substitutions of {\tt \%i}, {\tt \%g}, et cetera, and
then dumps the result to the specified string. Then you can pass that
string to {\tt apop_query_db}. Thus, if {\tt min_height==175}, the above
two pieces of code would be identical.

\paragraph{apop\_query\_to\_matrix} This function takes both a
gsl\_matrix and a query string. It will run the query and return the
resulting string for your analysis. E.g., if {\tt q} was defined as
above, then 
\begin{verbatim}
#include <gsl/gsl_matrix.h>
gsl_matrix *tall_people;

//q was defined to be a query string above.
apop_query_to_matrix(tall_people, q);
\end{verbatim}
will run the above query, allocate {\tt tall_people} to be an
appropriately-sized matrix, and fill it with data.

\subsection{Joining}
If you specify two tables in your {\tt from} line, then, lacking any
restrictions, the database will return a line for every pair of lines.
If {\tt data1} has 15 lines, and {\tt data2} has 8 lines, then {\tt select *
from data1, data2} will return 15$\times$8 = 120 lines.  Such a product
quickly gets overwhelming: joining a thousand-row table with another
thousand-row table will produce a million rows.

Thus, the {\tt where} clause becomes essential. Its most typical use is
when one column in each table represents identical information. For
example, say that one data source gave you mean income by ZIP code,
while another gave you mean heights by ZIP code. Joining the two tables
would give you a multitude of lines which include height from a ZIP code
in Alaska and an income for a ZIP in Kansas; clearly, the only ones you
are interested in are those where the ZIP code is the same in both data
sets. Here is a query which would keep only those lines which make
sense:

\begin{verbatim}
select t1.income, t2.height
   from econ_data t1, health_data t2
   where t1.zip == t2.zip_code
\end{verbatim}

You can see that using the table.column name for the columns is now
essential. Also, notice that we did not have to include either ZIP code
in the output if it is not used later. If you do want to include a ZIP
code, then you can use either {\tt t1.zip} or {\tt t2.zip_code}; since
the two will be by definition identical, using both will be a waste of
space.


\subsection{aggregation}


\subsection{generating new variables} Frankly, SQL is not very good for
creating new data from the existing. But the few things that you can do
are very easy.

\paragraph{A time lag} Here is a simple example:
\begin{verbatim}
select year, (t1.income - t2.income) as diff
   from data t1, data t2
   where t1.year=(t2.year -1);
\end{verbatim}

The {\tt where} clause will line up the table with a copy of itself
lagged by one year, and then the {\tt diff} variable will be the
one-year change in income.



\subsection{Getting data in and out}
We now have three different ways to represent a matrix of data: as a {\tt gsl\_matrix}, as a text file,
or as a database table. This section will show you how to best shunt your data between a database and
the other two formats.

\section{An example: dummy variables}
Here's a neat trick: using SQL's {\tt case} a few dozen times, we can turn a
variable which is discrete but not ordered (such as district numbers in the
following example) into a series of dummy variables. It requires writing down a
separate {\tt case} statement for each value the variable could take, but that's
what {\tt for} loops are for.

\codefig{dummy}{A sample of using SQL to create dummy variables}

[Note to editor: I just cut and pasted figure \ref{dummy} from my hard drive. Will clean it up
later. The gist is that we first query out the list of districts; then we write a
select statement with a line {\tt case district when district\_no then 1 else 0} for
each and every district\_no. You can then run your regression on the output of the
query without any further manipulation.

Notice that the for loop goes from i=1, not i=0; this is because when including
dummy variables, you always have to exclude one value, which will be the baseline;
using i=1 means district[0] will be the baseline.]




\section{An example: the easiest t-test you'll ever run.}
Say we have a set of observations of our sample's years of education, and their annual income. We want to
know if getting that grad school education is {\it really} worth it. The null hypothesis is: (Income for
people with education less than 16 years) $\leq$ (income for people with greater than or equal to 16 years
of education).

That first data set is:
\begin{verbatim}
#include <gsl/gsl.h>
#include <gsl/gsl_matrix.h>
#include "sqlite_wrappers.h"
gsl_vector	*undereducated;
   query_to_vector(&undereducated, 
      "select income from survey \
      where education <16");
\end{verbatim}
while the second group is:
\begin{verbatim}
gsl_vector	*overeducated;
   query_to_vector(&overeducated, 
      "select income from survey \
      where education >=16");
\end{verbatim}

Here's a factoid for you: incomes are usually distributed log-normally, so we should do a t-test on the
log of income:
\begin{verbatim}
#include <gsl/gsl_sf_log.h>
int i;
for(i=0;i< overeducated->size; i++)
   gsl_vector_set(overeducated, i, 
               gsl_sf_log(gsl_vector_get(overeducated, i)));
for(i=0;i< undereducated->size; i++)
   gsl_vector_set(undereducated, i, 
               gsl_sf_log(gsl_vector_get(undereducated, i)));
\end{verbatim}

We've already written functions to find the mean and variance of a vector:

\begin{verbatim}
#include <apophenia/stats.h>
double	mean_over, mean_under, var_over, var_under;
mean_over  = apop_mean(overeducated);
mean_under = apop_mean(undereducated);
var_over   = apop_variance(overeducated);
var_under  = apop_variance(undereducated);
\end{verbatim}

The other factoid you'll need is that the difference of two normal
distributions is also normal, and the variance of the difference is
the sum of the two original variances.\footnote{Your stats textbook
will tell you that the sum of two Normals is normal: ${\cal N}(\mu_1,
\sigma_1) + {\cal N}(\mu_2, \sigma_2) \sim {\cal N}(\mu_1 + \mu_2,
\sigma_1 + \sigma_2)$. Now, subtracting a ${\cal N}(\mu_2, \sigma_2)$
is exactly equivalent to adding a ${\cal N}(-\mu_2, \sigma_2)$, so we
get the result in the text.} That is, we can write down:

\begin{verbatim}
#include <gsl/gsl_cdf.h>
double	test_me = gsl_cdf_gaussian_P(mean_over-mean_under, 
                                       var_over+var_under);
\end{verbatim}

and {\tt test\_me} is the probability that the difference between the
means is less than or equal to zero. If {\tt test\_me} turns out to be greater
than your preferred confidence interval, (e.g., 95\%), then reject the
null and go to grad school. Else, there isn't enough information to
distinguish between the two with confidence.

